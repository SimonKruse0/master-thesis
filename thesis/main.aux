\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{37803332}
\pgfsyspdfmark {pgfid2}{2051147}{36194423}
\pgfsyspdfmark {pgfid4}{2362837}{53585219}
\pgfsyspdfmark {pgfid3}{2051147}{34585514}
\@writefile{toc}{\contentsline {section}{Abstract}{iii}{Doc-Start}\protected@file@percent }
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\HyPL@Entry{4<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces GP (left) and BNN (right) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuerity makes the standard implemented GP (optimized with emperical Bayes) overreact to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuerity happens at $x = 0.5$\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GP_vs_BNN}{{1.1}{2}{GP (left) and BNN (right) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuerity makes the standard implemented GP (optimized with emperical Bayes) overreact to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuerity happens at $x = 0.5$\relax }{figure.caption.2}{}}
\newlabel{GP_vs_BNN@cref}{{[figure][1][0,1]1.1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Contribution}{2}{section.0.1.1}\protected@file@percent }
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\abx@aux@cite{DNGO}
\abx@aux@segm{0}{0}{DNGO}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related work}{3}{section.0.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the thesis}{4}{section.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Notation}{4}{section.0.1.4}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Optimization}{5}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Optimization methodology}{5}{section.0.2.1}\protected@file@percent }
\newlabel{OPT}{{2.1}{5}{Optimization methodology}{equation.0.2.1.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][5][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{algOPT}{{1}{5}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}When to use sample-efficient optimization}{6}{subsection.0.2.1.1}\protected@file@percent }
\abx@aux@cite{Adam}
\abx@aux@segm{0}{0}{Adam}
\abx@aux@cite{deterministicsurrogatemodels}
\abx@aux@segm{0}{0}{deterministicsurrogatemodels}
\@writefile{toc}{\contentsline {subsubsection}{Exploitation and exploration}{7}{subsubsection*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization). This illustrates the idea of a sample-efficient solver\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:example}{{2.1}{8}{Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization). This illustrates the idea of a sample-efficient solver\relax }{figure.caption.4}{}}
\newlabel{fig:example@cref}{{[figure][1][0,2]2.1}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsubsection}{Noisy objective functions}{8}{subsubsection*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying black objective. Bottom: The expected improvement \textit  {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text  {policy}_{BO}(\mathcal  {D}) = -79,27$\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:03-03}{{2.2}{9}{Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying black objective. Bottom: The expected improvement \textit {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text {policy}_{BO}(\mathcal {D}) = -79,27$\relax }{figure.caption.6}{}}
\newlabel{fig:03-03@cref}{{[figure][2][0,2]2.2}{[1][8][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian regression}{9}{section.0.2.2}\protected@file@percent }
\newlabel{Predictive2}{{2.2}{9}{Bayesian regression}{equation.0.2.2.2}{}}
\newlabel{Predictive2@cref}{{[equation][2][0,2]2.2}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Surrogate model}{9}{subsection.0.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inference of surrogate models}{10}{subsection.0.2.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Overview of inference methods applied on the statistical models used in this project. $E$ is the number of edges in the SPN. $n$ is the number of datapoints. $K \leq n$ is the number of mixture comonents. We will soon learn that for an SPN the number of mixture compenets is exponential larger than number of edges i.e. $E << K$. In theory MCMC methods samples from true the posterior distribution, and do not need any fitting/learning. \relax }}{10}{table.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces When then predictive distribution is Gaussian, expected improvement becomes closed form. The figure illustrates the values of expected improvement for different predictive uncertainties $\sigma _x = \sqrt  {\mathbb  {V}ar_{p(y|x,\mathcal  {D})}[y]}$ versus the average improvement $y_{\qopname  \relax m{min}}-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal  {D})}[y]$\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{EI_illustration}{{2.3}{11}{When then predictive distribution is Gaussian, expected improvement becomes closed form. The figure illustrates the values of expected improvement for different predictive uncertainties $\sigma _x = \sqrt {\mathbb {V}ar_{p(y|x,\mathcal {D})}[y]}$ versus the average improvement $y_{\min }-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal {D})}[y]$\relax }{figure.caption.8}{}}
\newlabel{EI_illustration@cref}{{[figure][3][0,2]2.3}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Acquisition function}{11}{section.0.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Expected improvement}{11}{subsection.0.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Exact expected improvement}{12}{subsubsection*.9}\protected@file@percent }
\newlabel{ExactEI}{{2.3.1}{12}{Exact expected improvement}{subsubsection*.9}{}}
\newlabel{ExactEI@cref}{{[subsection][1][0,2,3]2.3.1}{[1][11][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The same points as fig .. where the acquisition function is the expected improvement. Whereas Expected improvement is fixed, LCB does have a tunable parameter, which can trade off exploration and exploitation \relax }}{13}{figure.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Approximate expected improvement}{13}{subsubsection*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Lower confidense bound}{13}{subsection.0.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Illustration of the values of the negative lower confidence bound for different predictive uncertainties $\sigma _x = \sqrt  {\mathbb  {V}ar_{p(y|x,\mathcal  {D})}[y]}$ versus the average improvement $y_{\qopname  \relax m{min}}-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal  {D})}[y]$\relax }}{14}{figure.caption.12}\protected@file@percent }
\newlabel{nLCB_illustration}{{2.5}{14}{Illustration of the values of the negative lower confidence bound for different predictive uncertainties $\sigma _x = \sqrt {\mathbb {V}ar_{p(y|x,\mathcal {D})}[y]}$ versus the average improvement $y_{\min }-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal {D})}[y]$\relax }{figure.caption.12}{}}
\newlabel{nLCB_illustration@cref}{{[figure][5][0,2]2.5}{[1][13][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Entropy search}{14}{subsection.0.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The same points as fig .. where the acquisition function is the negative lower confidence bound. \relax }}{15}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Discriminative surrogate models}{17}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gaussian process surrogate}{17}{section.0.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: Samples from $\mathcal  {N}(\textbf  {f}|0,\kappa (x_1,\dots  , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{GP_illustration}{{3.1}{18}{Left: Samples from $\mathcal {N}(\textbf {f}|0,\kappa (x_1,\dots , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }{figure.caption.14}{}}
\newlabel{GP_illustration@cref}{{[figure][1][0,3]3.1}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Exact predictive distribution}{18}{subsection.0.3.1.1}\protected@file@percent }
\newlabel{GP_predictive}{{3.2}{18}{Exact predictive distribution}{equation.0.3.1.2}{}}
\newlabel{GP_predictive@cref}{{[equation][2][0,3]3.2}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsubsection}{Posterior function distribution}{19}{subsubsection*.15}\protected@file@percent }
\newlabel{posterior_function_new}{{3.3}{19}{Posterior function distribution}{equation.0.3.1.3}{}}
\newlabel{posterior_function_new@cref}{{[equation][3][0,3]3.3}{[1][18][]19}}
\newlabel{marginal_distribution}{{3.4}{19}{Posterior function distribution}{equation.0.3.1.4}{}}
\newlabel{marginal_distribution@cref}{{[equation][4][0,3]3.4}{[1][19][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Left: Samples from posterior function distribution for $\mathcal  {N}(\textbf  {f}_*|m(\textbf  {x}),V(\textbf  {x}))$ where $\textbf  {x} = \{x_1,\dots  , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{GP_illustration2}{{3.2}{20}{Left: Samples from posterior function distribution for $\mathcal {N}(\textbf {f}_*|m(\textbf {x}),V(\textbf {x}))$ where $\textbf {x} = \{x_1,\dots , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }{figure.caption.16}{}}
\newlabel{GP_illustration2@cref}{{[figure][2][0,3]3.2}{[1][19][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Learning - Emperical bayes inference}{20}{subsection.0.3.1.2}\protected@file@percent }
\newlabel{GP_predictive_prior}{{3.7}{20}{Learning - Emperical bayes inference}{equation.0.3.1.7}{}}
\newlabel{GP_predictive_prior@cref}{{[equation][7][0,3]3.7}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces There is large difference beween the two GPs the different lenght scales in the kernel matters a lot. Left is chosen 9 out of 10 times, when minimizing the \relax }}{21}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Performance characteristic for GP}{21}{subsection.0.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces BNN with $\qopname  \relax o{tan}^{-1}(\cdot )$ activation functions and different prior distributions. \relax }}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig:example2}{{3.5}{22}{BNN with $\tan ^{-1}(\cdot )$ activation functions and different prior distributions. \relax }{figure.caption.21}{}}
\newlabel{fig:example2@cref}{{[figure][5][0,3]3.5}{[1][22][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces GP tested on all problems.\relax }}{22}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bayesian Neural Networks}{22}{section.0.3.2}\protected@file@percent }
\newlabel{BNN}{{3.2}{22}{Bayesian Neural Networks}{section.0.3.2}{}}
\newlabel{BNN@cref}{{[section][2][0,3]3.2}{[1][22][]22}}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}No U-Turn sampling}{26}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Adaptive stochatic HMC}{26}{subsection.0.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Design of Bayesian neural network}{26}{subsection.0.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Number of units in each of the 3 layers have a large influence of the BNNs ability to be expressive\relax }}{27}{figure.caption.23}\protected@file@percent }
\newlabel{n_unit_BNN}{{3.6}{27}{Number of units in each of the 3 layers have a large influence of the BNNs ability to be expressive\relax }{figure.caption.23}{}}
\newlabel{n_unit_BNN@cref}{{[figure][6][0,3]3.6}{[1][26][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example where 100 nodes on each of three layers, lead to a much more expressive model. $\sigma ^2$ follows a informative prior $InvGamma(1000,1)$, i.e. prior mean $E[\sigma ^2] \approx \frac  {1}{1000}$, and variance $\approx \frac  {1}{1000^3}$, however since the data is distributed in such a complex way the limited expressiveness of the model, forces the model to infer make $\sigma ^2$ large, i.e. including the data in the noise\relax }}{27}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Number of samples}{28}{subsubsection*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces BOHAMIANN tested on all problems.\relax }}{28}{figure.caption.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces numpyro BNN tested on all problems.\relax }}{29}{figure.caption.27}\protected@file@percent }
\abx@aux@cite{bishop1995neural}
\abx@aux@segm{0}{0}{bishop1995neural}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\abx@aux@cite{JordanPaper}
\abx@aux@segm{0}{0}{JordanPaper}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Generative models as surrogate}{31}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Conditional distribution in a Bayesian setting}{31}{section.0.4.1}\protected@file@percent }
\newlabel{mixture_include_prior}{{4.1}{31}{Conditional distribution in a Bayesian setting}{section.0.4.1}{}}
\newlabel{mixture_include_prior@cref}{{[section][1][0,4]4.1}{[1][31][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }}{32}{figure.caption.28}\protected@file@percent }
\newlabel{pred_dist_manipulation}{{4.1}{32}{Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }{figure.caption.28}{}}
\newlabel{pred_dist_manipulation@cref}{{[figure][1][0,4]4.1}{[1][32][]32}}
\@writefile{toc}{\contentsline {subsubsection}{Mean and variance of predictive distribution v1}{33}{subsubsection*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conditional of mixture model}{33}{section.0.4.2}\protected@file@percent }
\newlabel{Conditional_mixture}{{4.2}{33}{Conditional of mixture model}{section.0.4.2}{}}
\newlabel{Conditional_mixture@cref}{{[section][2][0,4]4.2}{[1][33][]33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Kernel estimator regression}{34}{section.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Conditional of Kernel density estimator}{34}{subsection.0.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces KDE tested on all problems. It is seen that we might loose a lot of generalization properties optained from GPs and BNN, however, for the very complicated cases, KDE might have an advantage.\relax }}{35}{figure.caption.30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Gaussian mixture regression}{35}{section.0.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Conditional of Gaussian mixture model}{35}{subsection.0.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Sum product networks}{36}{section.0.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces left: the data lies perfect for the SPN. Right: The data distribution is not suited for SPN\relax }}{36}{figure.caption.31}\protected@file@percent }
\newlabel{SPN_fig}{{4.3}{36}{left: the data lies perfect for the SPN. Right: The data distribution is not suited for SPN\relax }{figure.caption.31}{}}
\newlabel{SPN_fig@cref}{{[figure][3][0,4]4.3}{[1][36][]36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The SPN trained in Figure \ref  {SPN_fig}. Number of channels are 3, giving 9 product nodes, which are weighted in the final sum node.\relax }}{37}{figure.caption.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Example of how the weight of each mixture component is turned up and down according the amount of data observed.\relax }}{37}{figure.caption.32}\protected@file@percent }
\newlabel{WeightedSPN}{{4.4}{37}{Example of how the weight of each mixture component is turned up and down according the amount of data observed.\relax }{figure.caption.32}{}}
\newlabel{WeightedSPN@cref}{{[figure][4][0,4]4.4}{[1][36][]37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}SPN as mixture regression}{37}{subsection.0.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces SPN tested on all problems.\relax }}{38}{figure.caption.34}\protected@file@percent }
\newlabel{SPN4}{{\caption@xref {SPN4}{ on input line 237}}{38}{SPN as mixture regression}{definition.1}{}}
\newlabel{SPN4@cref}{{[subsection][1][0,4,5]4.5.1}{[1][37][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}SPN as a mixture model}{39}{subsection.0.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Conditional of SPN}{39}{subsection.0.4.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{calculation of responsibility}{40}{subsubsection*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Gaussian approximation of mixture regression}{40}{section.0.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Mean and variance of conditional SPN}{40}{subsection.0.4.6.1}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Mixture model training}{41}{section.0.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Expectation-maximization for mixture models}{41}{subsection.0.4.7.1}\protected@file@percent }
\newlabel{mixture_pdf}{{4.12}{41}{Expectation-maximization for mixture models}{equation.0.4.7.12}{}}
\newlabel{mixture_pdf@cref}{{[equation][12][0,4]4.12}{[1][41][]41}}
\newlabel{EM}{{\caption@xref {EM}{ on input line 56}}{42}{Expectation-maximization for mixture models}{testexample2.4}{}}
\newlabel{EM@cref}{{[subsection][1][0,4,7]4.7.1}{[1][41][]42}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{45}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}implementation}{45}{section.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}standardized data}{45}{subsection.0.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Regression analysis}{45}{section.0.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}uncertainty quantification}{45}{subsection.0.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Regression of a probabilistic objective function, is not a smart choice using anything else than a mixture regression model. Top we see the performance of 5 different regression models fitted to an increasing amount of training data, the underlying function is tricky, as it jumps between multiple objective functions yielding a violation to most of the generative models, where most are capable of handling gaussian noise this is not gaussian noise and hence a very difficult problem. This kind of objective function could definitely be relevant in certain cases.\relax }}{46}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plots visulising the results in above figure.\relax }}{46}{figure.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}prediction quantification}{46}{subsection.0.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}regression benchmark}{46}{subsection.0.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Mixture regression}{46}{section.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Regression analysis of GP, BOHAMIANN and NumpyNN 1D}{47}{section.0.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{47}{section.0.5.5}\protected@file@percent }
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 151}}{47}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.39}{}}
\newlabel{f1_reg_2D@cref}{{[section][5][0,5]5.5}{[1][46][]47}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Mixture regression on simple functions}{47}{section.0.5.6}\protected@file@percent }
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 176}}{48}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.40}{}}
\newlabel{f1_reg_2D@cref}{{[section][5][0,5]5.5}{[1][47][]48}}
\newlabel{f23_reg_2D}{{\caption@xref {f23_reg_2D}{ on input line 204}}{49}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.41}{}}
\newlabel{f23_reg_2D@cref}{{[section][5][0,5]5.5}{[1][47][]49}}
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 229}}{50}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.42}{}}
\newlabel{f1_reg_2D@cref}{{[section][5][0,5]5.5}{[1][47][]50}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces 2D Regression plot for a few of the problems, with the correct \relax }}{50}{figure.caption.43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction\relax }}{50}{figure.caption.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Regression plot for anther problem\relax }}{50}{figure.caption.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 1: "5 separable functions"\relax }}{51}{figure.caption.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 2: "4 functions with low or moderate conditioning"\relax }}{51}{figure.caption.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 3: "5 functions with high conditioning, unimodal"\relax }}{51}{figure.caption.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 4: "5 multi-modal functions with adequate global structure"\relax }}{51}{figure.caption.49}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 5: "5 multi-modal functions with weak global structure"\relax }}{52}{figure.caption.50}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search\relax }}{52}{figure.caption.51}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Test functions f1 to f12\relax }}{52}{figure.caption.52}\protected@file@percent }
\newlabel{2DBlockcyclic}{{5.12}{52}{Test functions f1 to f12\relax }{figure.caption.52}{}}
\newlabel{2DBlockcyclic@cref}{{[figure][12][0,5]5.12}{[1][47][]52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Test functions f13-24\relax }}{53}{figure.caption.53}\protected@file@percent }
\newlabel{f13-24}{{5.13}{53}{Test functions f13-24\relax }{figure.caption.53}{}}
\newlabel{f13-24@cref}{{[figure][13][0,5]5.13}{[1][47][]53}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and further work}{55}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}further work}{55}{section.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Bibliography}{56}{chapter*.54}\protected@file@percent }
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Nature_BO_paper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{BOHAMIANN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DNGO}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ALStatisticalModels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesoptbook}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Adam}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deterministicsurrogatemodels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop1995neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{JordanPaper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2015scalable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NIPS2016_a96d3afe}{none/global//global/global}
