\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{38075306}
\pgfsyspdfmark {pgfid2}{2051147}{36738371}
\pgfsyspdfmark {pgfid4}{2362837}{54401141}
\pgfsyspdfmark {pgfid3}{2051147}{35401436}
\HyPL@Entry{2<</S/r>>}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{ii}{Doc-Start}\protected@file@percent }
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\HyPL@Entry{6<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}notation}{1}{section.0.1.1}\protected@file@percent }
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}related work}{2}{section.0.1.2}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Optimization methodology}{3}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{OPT}{{2.1}{3}{Optimization methodology}{equation.0.2.0.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][3][]3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{3}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algOPT}{{1}{3}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}When to use Bayesian optmization}{4}{section.0.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:example}{{2.1}{5}{Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }{figure.caption.2}{}}
\newlabel{fig:example@cref}{{[figure][1][0,2]2.1}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Observation model}{5}{section.0.2.2}\protected@file@percent }
\newlabel{ObsModel}{{2.2}{5}{Observation model}{section.0.2.2}{}}
\newlabel{ObsModel@cref}{{[section][2][0,2]2.2}{[1][5][]5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian Optimization}{7}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Predictive2}{{3.1}{7}{Bayesian Optimization}{equation.0.3.0.1}{}}
\newlabel{Predictive2@cref}{{[equation][1][0,3]3.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Acquisition function}{7}{section.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}surrogate model}{9}{section.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Discriminative model as surrogate model}{9}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Using a generative model as surrogate model}{9}{subsection.0.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Inclusion of prior distribution}{10}{subsection.0.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Bayesian regression models - probabilistic surrogate model}{13}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Gaussian mixture regression}{13}{section.0.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces EI improvement might in this case be stucked at the point 0, since the conditional distribution is normalized by $p(x)$ and therefore not influenced by the amount of data\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{MixtureReg_challenge}{{4.1}{14}{EI improvement might in this case be stucked at the point 0, since the conditional distribution is normalized by $p(x)$ and therefore not influenced by the amount of data\relax }{figure.caption.6}{}}
\newlabel{MixtureReg_challenge@cref}{{[figure][1][0,4]4.1}{[1][13][]14}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Mixture regression in a Bayesian setting}{14}{section.0.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Sum product networks}{15}{section.0.4.3}\protected@file@percent }
\newlabel{SPN4}{{\caption@xref {SPN4}{ on input line 79}}{15}{}{definition.1}{}}
\newlabel{SPN4@cref}{{[section][3][0,4]4.3}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces SPN\relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{SPN_fig}{{4.2}{18}{SPN\relax }{figure.caption.7}{}}
\newlabel{SPN_fig@cref}{{[figure][2][0,4]4.2}{[1][17][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Model including wifi information\relax }}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:wifi2}{{4.3}{18}{Model including wifi information\relax }{figure.caption.8}{}}
\newlabel{fig:wifi2@cref}{{[figure][3][0,4]4.3}{[1][18][]18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Inference: Prediction and learning}{19}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Exact and approximate inference}{19}{section.0.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Overview of inference methods applied on the statistical models used in this project. $E$ is the number of edges in the SPN. $n$ is the number of datapoints. $K \leq n$ is the number of mixture comonents. We will soon learn that for an SPN the number of mixture compenets is exponential larger than number of edges i.e. $E << K$. In theory MCMC methods samples from true the posterior distribution, and do not need any fitting/learning. \relax }}{19}{table.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}SPN}{19}{section.0.5.2}\protected@file@percent }
\abx@aux@cite{SPN_EM}
\abx@aux@segm{0}{0}{SPN_EM}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}SPN - prediction}{20}{subsection.0.5.2.1}\protected@file@percent }
\newlabel{SPN_1}{{\caption@xref {SPN_1}{ on input line 80}}{20}{SPN - prediction}{subsection.0.5.2.1}{}}
\newlabel{SPN_1@cref}{{[subsection][1][0,5,2]5.2.1}{[1][20][]20}}
\newlabel{SPN}{{\caption@xref {SPN}{ on input line 101}}{20}{SPN - prediction}{subsection.0.5.2.1}{}}
\newlabel{SPN@cref}{{[subsection][1][0,5,2]5.2.1}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}SPN - learning}{20}{subsection.0.5.2.2}\protected@file@percent }
\newlabel{SPN3}{{\caption@xref {SPN3}{ on input line 144}}{21}{SPN - learning}{definition.2}{}}
\newlabel{SPN3@cref}{{[subsection][2][0,5,2]5.2.2}{[1][21][]21}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Expectation-maximization for mixture models}{22}{section.0.5.3}\protected@file@percent }
\newlabel{EM}{{\caption@xref {EM}{ on input line 248}}{23}{Expectation-maximization for mixture models}{testexample2.1}{}}
\newlabel{EM@cref}{{[section][3][0,5]5.3}{[1][22][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Gaussian Mixture Regression}{24}{section.0.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}GMR - prediction}{24}{subsection.0.5.4.1}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}GMR - Leaning}{25}{subsection.0.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Gaussian Process Regression}{25}{section.0.5.5}\protected@file@percent }
\newlabel{GP_predictive}{{5.7}{25}{Gaussian Process Regression}{equation.0.5.5.7}{}}
\newlabel{GP_predictive@cref}{{[equation][7][0,5]5.7}{[1][25][]25}}
\newlabel{marginal_distribution}{{5.8}{26}{Gaussian Process Regression}{equation.0.5.5.8}{}}
\newlabel{marginal_distribution@cref}{{[equation][8][0,5]5.8}{[1][25][]26}}
\newlabel{posterior_function}{{5.11}{26}{}{equation.0.5.5.11}{}}
\newlabel{posterior_function@cref}{{[equation][11][0,5]5.11}{[1][26][]26}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Deep Network Global Optimization - ?}{27}{section.0.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Bayesian Neural Networks}{27}{section.0.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}No U-Turn sampling}{30}{subsection.0.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Adaptive stochatic HMC}{30}{subsection.0.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Appendix - SPN}{31}{section.0.5.8}\protected@file@percent }
\newlabel{SPN2}{{\caption@xref {SPN2}{ on input line 670}}{31}{Appendix - SPN}{section.0.5.8}{}}
\newlabel{SPN2@cref}{{[section][8][0,5]5.8}{[1][31][]31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{33}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}regression benchmark}{33}{subsection.0.6.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Mixture regression on simple functions}{33}{section.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Bibliography}{34}{chapter*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Title}{35}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2015scalable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NIPS2016_a96d3afe}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
