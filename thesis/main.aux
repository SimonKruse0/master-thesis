\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{38075306}
\pgfsyspdfmark {pgfid2}{2051147}{36738371}
\pgfsyspdfmark {pgfid4}{2362837}{54401141}
\pgfsyspdfmark {pgfid3}{2051147}{35401436}
\HyPL@Entry{2<</S/r>>}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\HyPL@Entry{4<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}notation}{1}{section.0.1.1}\protected@file@percent }
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}related work}{2}{section.0.1.2}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Optimization methodology}{3}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{OPT}{{2.1}{3}{Optimization methodology}{equation.0.2.0.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][3][]3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{3}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algOPT}{{1}{3}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}When to use Bayesian optmization}{4}{section.0.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:example}{{2.1}{5}{Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }{figure.caption.2}{}}
\newlabel{fig:example@cref}{{[figure][1][0,2]2.1}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Observation model}{5}{section.0.2.2}\protected@file@percent }
\newlabel{ObsModel}{{2.2}{5}{Observation model}{section.0.2.2}{}}
\newlabel{ObsModel@cref}{{[section][2][0,2]2.2}{[1][5][]5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian Optimization}{7}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Predictive2}{{3.1}{7}{Bayesian Optimization}{equation.0.3.0.1}{}}
\newlabel{Predictive2@cref}{{[equation][1][0,3]3.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Acquisition function}{7}{section.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}surrogate model}{9}{section.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Discriminative model as surrogate model}{9}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Using a generative model as surrogate model}{9}{subsection.0.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Inclusion of prior distribution}{10}{subsection.0.3.2.3}\protected@file@percent }
\newlabel{manipulated_pred_dist}{{3.2}{10}{Inclusion of prior distribution}{equation.0.3.2.2}{}}
\newlabel{manipulated_pred_dist@cref}{{[equation][2][0,3]3.2}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N$. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should collapse into the uncertain prior\relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{pred_dist_manipulation}{{3.1}{11}{Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N$. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should collapse into the uncertain prior\relax }{figure.caption.4}{}}
\newlabel{pred_dist_manipulation@cref}{{[figure][1][0,3]3.1}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsubsection}{Manipulated predictive mean}{11}{subsubsection*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Manipulated predictive variance}{11}{subsubsection*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Gaussian process surrogate}{13}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Gaussian Process Regression}{13}{section.0.4.1}\protected@file@percent }
\newlabel{GP_predictive}{{4.1}{13}{Gaussian Process Regression}{equation.0.4.1.1}{}}
\newlabel{GP_predictive@cref}{{[equation][1][0,4]4.1}{[1][13][]13}}
\newlabel{marginal_distribution}{{4.2}{13}{Gaussian Process Regression}{equation.0.4.1.2}{}}
\newlabel{marginal_distribution@cref}{{[equation][2][0,4]4.2}{[1][13][]13}}
\newlabel{posterior_function}{{4.5}{13}{}{equation.0.4.1.5}{}}
\newlabel{posterior_function@cref}{{[equation][5][0,4]4.5}{[1][13][]13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bayesian neural network surrogate}{15}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Bayesian Neural Networks}{15}{section.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}No U-Turn sampling}{17}{subsection.0.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Adaptive stochatic HMC}{17}{subsection.0.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Mixture regression surrogate}{19}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Gaussian mixture regression}{19}{section.0.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces EI improvement might in this case be stucked at the point 0, since the conditional distribution is normalized by $p(x)$ and therefore not influenced by the amount of data\relax }}{19}{figure.caption.7}\protected@file@percent }
\newlabel{MixtureReg_challenge}{{6.1}{19}{EI improvement might in this case be stucked at the point 0, since the conditional distribution is normalized by $p(x)$ and therefore not influenced by the amount of data\relax }{figure.caption.7}{}}
\newlabel{MixtureReg_challenge@cref}{{[figure][1][0,6]6.1}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Mixture regression in a Bayesian setting}{20}{section.0.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Sum product networks}{20}{section.0.6.3}\protected@file@percent }
\newlabel{SPN4}{{\caption@xref {SPN4}{ on input line 70}}{20}{}{definition.1}{}}
\newlabel{SPN4@cref}{{[section][3][0,6]6.3}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces SPN\relax }}{23}{figure.caption.8}\protected@file@percent }
\newlabel{SPN_fig}{{6.2}{23}{SPN\relax }{figure.caption.8}{}}
\newlabel{SPN_fig@cref}{{[figure][2][0,6]6.2}{[1][23][]23}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Model including wifi information\relax }}{23}{figure.caption.9}\protected@file@percent }
\newlabel{fig:wifi2}{{6.3}{23}{Model including wifi information\relax }{figure.caption.9}{}}
\newlabel{fig:wifi2@cref}{{[figure][3][0,6]6.3}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}SPN}{24}{section.0.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}SPN - prediction}{24}{subsection.0.6.4.1}\protected@file@percent }
\newlabel{SPN_1}{{\caption@xref {SPN_1}{ on input line 242}}{24}{SPN - prediction}{subsection.0.6.4.1}{}}
\newlabel{SPN_1@cref}{{[subsection][1][0,6,4]6.4.1}{[1][24][]24}}
\abx@aux@cite{SPN_EM}
\abx@aux@segm{0}{0}{SPN_EM}
\newlabel{SPN}{{\caption@xref {SPN}{ on input line 263}}{25}{SPN - prediction}{subsection.0.6.4.1}{}}
\newlabel{SPN@cref}{{[subsection][1][0,6,4]6.4.1}{[1][24][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}SPN - learning}{25}{subsection.0.6.4.2}\protected@file@percent }
\newlabel{SPN3}{{\caption@xref {SPN3}{ on input line 306}}{25}{SPN - learning}{definition.2}{}}
\newlabel{SPN3@cref}{{[subsection][2][0,6,4]6.4.2}{[1][25][]25}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Expectation-maximization for mixture models}{27}{section.0.6.5}\protected@file@percent }
\newlabel{mixture_pdf}{{6.1}{27}{Expectation-maximization for mixture models}{equation.0.6.5.1}{}}
\newlabel{mixture_pdf@cref}{{[equation][1][0,6]6.1}{[1][27][]27}}
\newlabel{EM}{{\caption@xref {EM}{ on input line 410}}{28}{Expectation-maximization for mixture models}{testexample2.4}{}}
\newlabel{EM@cref}{{[section][5][0,6]6.5}{[1][27][]28}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Gaussian Mixture Regression}{30}{section.0.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}GMR - prediction}{30}{subsection.0.6.6.1}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}GMR - Leaning}{31}{subsection.0.6.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Results}{33}{chapter.0.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.0.1}regression benchmark}{33}{subsection.0.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Mixture regression on simple functions}{33}{section.0.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction\relax }}{33}{figure.caption.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Regression plot for anther problem\relax }}{33}{figure.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Table summarizing for all problems\relax }}{33}{figure.caption.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces An example table.\relax }}{34}{table.caption.14}\protected@file@percent }
\newlabel{table:example_table}{{7.1}{34}{An example table.\relax }{table.caption.14}{}}
\newlabel{table:example_table@cref}{{[table][1][0,7]7.1}{[1][33][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search\relax }}{34}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Bibliography}{35}{chapter*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Implementation}{37}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2015scalable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NIPS2016_a96d3afe}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesoptbook}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
