\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{35837252}
\pgfsyspdfmark {pgfid2}{2051147}{34228343}
\pgfsyspdfmark {pgfid4}{2362837}{51619139}
\pgfsyspdfmark {pgfid3}{2051147}{32619434}
\HyPL@Entry{2<</S/r>>}
\@writefile{toc}{\contentsline {section}{Preface}{ii}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Abstract}{iii}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iv}{Doc-Start}\protected@file@percent }
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{TakeHumanOutOfLoop}
\abx@aux@segm{0}{0}{TakeHumanOutOfLoop}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\HyPL@Entry{8<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Gaussian process (GP) and Bayesian neural network (BNN) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuity makes the standard implemented GP (optimized with emperical Bayes) expresses large uncertainties to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuity happens at $x = 0.5$\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GP_vs_BNN}{{1.1}{2}{Gaussian process (GP) and Bayesian neural network (BNN) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuity makes the standard implemented GP (optimized with emperical Bayes) expresses large uncertainties to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuity happens at $x = 0.5$\relax }{figure.caption.2}{}}
\newlabel{GP_vs_BNN@cref}{{[figure][1][0,1]1.1}{[1][1][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Project limitation and contibution}{2}{section.0.1.1}\protected@file@percent }
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\abx@aux@cite{DNGO}
\abx@aux@segm{0}{0}{DNGO}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related work}{3}{section.0.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of this thesis}{4}{section.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Notation}{4}{section.0.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Bayesian notation}{4}{subsection.0.1.4.1}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Optimization}{5}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Optimization methodology}{5}{section.0.2.1}\protected@file@percent }
\newlabel{OPT}{{2.1}{5}{Optimization methodology}{equation.0.2.1.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][5][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{algOPT}{{1}{5}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Sample-efficient optimization}{6}{subsection.0.2.1.1}\protected@file@percent }
\abx@aux@cite{Adam}
\abx@aux@segm{0}{0}{Adam}
\abx@aux@cite{deterministicsurrogatemodels}
\abx@aux@segm{0}{0}{deterministicsurrogatemodels}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization).\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{Sample_efficient_illustration}{{2.1}{7}{Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization).\relax }{figure.caption.3}{}}
\newlabel{Sample_efficient_illustration@cref}{{[figure][1][0,2]2.1}{[1][7][]7}}
\abx@aux@cite{GlobalOptimization}
\abx@aux@segm{0}{0}{GlobalOptimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Exploitation and exploration}{8}{subsection.0.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Noisy objective functions}{8}{subsection.0.2.1.3}\protected@file@percent }
\newlabel{BayesOpt}{{2.2}{9}{Noisy objective functions}{equation.0.2.1.2}{}}
\newlabel{BayesOpt@cref}{{[equation][2][0,2]2.2}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying objective (black sin-function). Bottom: The expected improvement \textit  {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text  {policy}_{BO}(\mathcal  {D}) = 26.06$\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{BO_example}{{2.2}{9}{Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying objective (black sin-function). Bottom: The expected improvement \textit {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text {policy}_{BO}(\mathcal {D}) = 26.06$\relax }{figure.caption.4}{}}
\newlabel{BO_example@cref}{{[figure][2][0,2]2.2}{[1][8][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian regression}{9}{section.0.2.2}\protected@file@percent }
\newlabel{Predictive2}{{2.3}{9}{Bayesian regression}{equation.0.2.2.3}{}}
\newlabel{Predictive2@cref}{{[equation][3][0,2]2.3}{[1][9][]9}}
\abx@aux@cite{??}
\abx@aux@segm{0}{0}{??}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Surrogate model}{10}{subsection.0.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inference of surrogate models}{10}{subsection.0.2.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Overview of inference methods applied on the statistical models used in this project. $E$ is the number of edges in the SPN. $n$ is the number of data points. $K \leq n$ is the number of mixture components. We will soon learn that for an SPN the number of mixture components is exponentially larger than the number of edges i.e. $E << K$. In theory MCMC methods samples from true the posterior distribution, and do not need any fitting/learning. \relax }}{11}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Acquisition function}{11}{section.0.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The same regression model and points as Figure \ref  {BO_example}, but with three different acquisition functions: Expected improvement and negative lower confidence bound for two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{Different_AQ_functions}{{2.3}{11}{The same regression model and points as Figure \ref {BO_example}, but with three different acquisition functions: Expected improvement and negative lower confidence bound for two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.\relax }{figure.caption.6}{}}
\newlabel{Different_AQ_functions@cref}{{[figure][3][0,2]2.3}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Contourplot of expected improvement (EI) and lower confidence bound (LCB) for two different quantiles for different (Gaussian) predictive uncertainties $\sigma _x = \sqrt  {\mathbb  {V}ar_{p(y|x,\mathcal  {D})y]}}$ versus the average improvement $y_{\qopname  \relax m{min}}-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal  {D})}[y]$. Darker colors indicates higher values. The colored lines are the mapping $x \DOTSB \mapstochar \rightarrow (\sigma _x, y_{\qopname  \relax m{min}}-\mu _x)$ for $x = [-100, 100]$ for the Bayesian regression function in Figure \ref  {Different_AQ_functions} - and thereby explains how the acquisition functions balances exploitation and exploration. The orange dot represent the point maximizing the acquisition function\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{EI_illustration}{{2.4}{12}{Contourplot of expected improvement (EI) and lower confidence bound (LCB) for two different quantiles for different (Gaussian) predictive uncertainties $\sigma _x = \sqrt {\mathbb {V}ar_{p(y|x,\mathcal {D})y]}}$ versus the average improvement $y_{\min }-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal {D})}[y]$. Darker colors indicates higher values. The colored lines are the mapping $x \mapsto (\sigma _x, y_{\min }-\mu _x)$ for $x = [-100, 100]$ for the Bayesian regression function in Figure \ref {Different_AQ_functions} - and thereby explains how the acquisition functions balances exploitation and exploration. The orange dot represent the point maximizing the acquisition function\relax }{figure.caption.7}{}}
\newlabel{EI_illustration@cref}{{[figure][4][0,2]2.4}{[1][11][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Expected improvement}{12}{subsection.0.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Exact expected improvement}{12}{subsubsection*.8}\protected@file@percent }
\newlabel{ExactEI}{{2.3.1}{12}{Exact expected improvement}{subsubsection*.8}{}}
\newlabel{ExactEI@cref}{{[subsection][1][0,2,3]2.3.1}{[1][12][]12}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate expected improvement}{13}{subsubsection*.9}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{PredEntropy}
\abx@aux@segm{0}{0}{PredEntropy}
\abx@aux@cite{entropysearch}
\abx@aux@segm{0}{0}{entropysearch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Other acquisition functions}{14}{subsection.0.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Lower confidence bound}{14}{subsubsection*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Entropy search}{14}{subsubsection*.11}\protected@file@percent }
\newlabel{mutualinfo}{{2.5}{14}{Entropy search}{equation.0.2.3.5}{}}
\newlabel{mutualinfo@cref}{{[equation][5][0,2]2.5}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsubsection}{Probability of improvement}{14}{subsubsection*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{15}{section.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Discriminative surrogate models}{17}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{predictive_posterior_dist}{{3.1}{17}{Discriminative surrogate models}{equation.0.3.0.1}{}}
\newlabel{predictive_posterior_dist@cref}{{[equation][1][0,3]3.1}{[1][17][]17}}
\newlabel{GP_likelihood}{{3.2}{17}{Discriminative surrogate models}{equation.0.3.0.2}{}}
\newlabel{GP_likelihood@cref}{{[equation][2][0,3]3.2}{[1][17][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gaussian process surrogate}{17}{section.0.3.1}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: Samples from $\mathcal  {N}(\textbf  {f}|0,\kappa (x_1,\dots  , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution (right).\relax }}{18}{figure.caption.13}\protected@file@percent }
\newlabel{GP_illustration}{{3.1}{18}{Left: Samples from $\mathcal {N}(\textbf {f}|0,\kappa (x_1,\dots , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution (right).\relax }{figure.caption.13}{}}
\newlabel{GP_illustration@cref}{{[figure][1][0,3]3.1}{[1][18][]18}}
\newlabel{prior_gp}{{3.3}{18}{Gaussian process surrogate}{equation.0.3.1.3}{}}
\newlabel{prior_gp@cref}{{[equation][3][0,3]3.3}{[1][18][]18}}
\newlabel{conditional_GP}{{3.4}{18}{Gaussian process surrogate}{equation.0.3.1.4}{}}
\newlabel{conditional_GP@cref}{{[equation][4][0,3]3.4}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Exact predictive distribution}{18}{subsection.0.3.1.1}\protected@file@percent }
\newlabel{GP_predictive}{{3.5}{18}{Exact predictive distribution}{equation.0.3.1.5}{}}
\newlabel{GP_predictive@cref}{{[equation][5][0,3]3.5}{[1][18][]18}}
\newlabel{marginal_distribution}{{3.6}{19}{Exact predictive distribution}{equation.0.3.1.6}{}}
\newlabel{marginal_distribution@cref}{{[equation][6][0,3]3.6}{[1][18][]19}}
\newlabel{posterior_distribution}{{3.7}{19}{Exact predictive distribution}{equation.0.3.1.7}{}}
\newlabel{posterior_distribution@cref}{{[equation][7][0,3]3.7}{[1][18][]19}}
\newlabel{Gamma}{{3.8}{19}{Exact predictive distribution}{equation.0.3.1.8}{}}
\newlabel{Gamma@cref}{{[equation][8][0,3]3.8}{[1][18][]19}}
\@writefile{toc}{\contentsline {subsubsection}{Posterior function distribution}{19}{subsubsection*.14}\protected@file@percent }
\newlabel{posterior_function_new}{{3.9}{19}{Posterior function distribution}{equation.0.3.1.9}{}}
\newlabel{posterior_function_new@cref}{{[equation][9][0,3]3.9}{[1][19][]19}}
\newlabel{GP_posterior_unomalized}{{3.10}{19}{Posterior function distribution}{equation.0.3.1.10}{}}
\newlabel{GP_posterior_unomalized@cref}{{[equation][10][0,3]3.10}{[1][19][]19}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Left: Samples from posterior function distribution for $\mathcal  {N}(\textbf  {f}_*|m(\textbf  {x}),V(\textbf  {x}))$ where $\textbf  {x} = \{x_1,\dots  , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }}{20}{figure.caption.15}\protected@file@percent }
\newlabel{GP_illustration2}{{3.2}{20}{Left: Samples from posterior function distribution for $\mathcal {N}(\textbf {f}_*|m(\textbf {x}),V(\textbf {x}))$ where $\textbf {x} = \{x_1,\dots , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }{figure.caption.15}{}}
\newlabel{GP_illustration2@cref}{{[figure][2][0,3]3.2}{[1][19][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Learning - Empirical Bayes inference}{20}{subsection.0.3.1.2}\protected@file@percent }
\newlabel{GP_optim}{{3.11}{20}{Learning - Empirical Bayes inference}{equation.0.3.1.11}{}}
\newlabel{GP_optim@cref}{{[equation][11][0,3]3.11}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The same data fitted with two quite different GPs - note the different lenghtscales in the kernel and observation variance $\sigma ^2$. Left is chosen 19 out of 20 times and has and marginalized log likelihood of $-16.70$, whereas 1 out of 20 runs the solution at right is given with marginalized log likelihood of $-14.29$.\relax }}{21}{figure.caption.16}\protected@file@percent }
\newlabel{GP_two_results}{{3.3}{21}{The same data fitted with two quite different GPs - note the different lenghtscales in the kernel and observation variance $\sigma ^2$. Left is chosen 19 out of 20 times and has and marginalized log likelihood of $-16.70$, whereas 1 out of 20 runs the solution at right is given with marginalized log likelihood of $-14.29$.\relax }{figure.caption.16}{}}
\newlabel{GP_two_results@cref}{{[figure][3][0,3]3.3}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bayesian neural network regression}{22}{section.0.3.2}\protected@file@percent }
\newlabel{BNN}{{3.2}{22}{Bayesian neural network regression}{section.0.3.2}{}}
\newlabel{BNN@cref}{{[section][2][0,3]3.2}{[1][22][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Left: 20 black predictive \textit  {prior} samples from a 3 layers BNN with 50 $\qopname  \relax o{tanh}$ nodes, with standard Gaussian prior distributions. Right: 20 predictive \textit  {posterior} samples. The blue areas are $\pm $ 2 predictive standard deviations from the red predictive mean.\relax }}{22}{figure.caption.17}\protected@file@percent }
\newlabel{BNN_prior_posterior}{{3.4}{22}{Left: 20 black predictive \textit {prior} samples from a 3 layers BNN with 50 $\tanh $ nodes, with standard Gaussian prior distributions. Right: 20 predictive \textit {posterior} samples. The blue areas are $\pm $ 2 predictive standard deviations from the red predictive mean.\relax }{figure.caption.17}{}}
\newlabel{BNN_prior_posterior@cref}{{[figure][4][0,3]3.4}{[1][22][]22}}
\abx@aux@cite{ML_Bayesian_Pespective}
\abx@aux@segm{0}{0}{ML_Bayesian_Pespective}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Graphical representation of a Bayesian neural network compared to a neural network. The weights are assigned a probability density. Note that we often prior assume no correlation and a standard normal distribution, but the posterior (after observing data) might contain correlations between the weights.\relax }}{23}{figure.caption.18}\protected@file@percent }
\newlabel{BNN_illustration2}{{3.5}{23}{Graphical representation of a Bayesian neural network compared to a neural network. The weights are assigned a probability density. Note that we often prior assume no correlation and a standard normal distribution, but the posterior (after observing data) might contain correlations between the weights.\relax }{figure.caption.18}{}}
\newlabel{BNN_illustration2@cref}{{[figure][5][0,3]3.5}{[1][22][]23}}
\abx@aux@cite{MCMC}
\abx@aux@segm{0}{0}{MCMC}
\abx@aux@cite{ML_Bayesian_Pespective}
\abx@aux@segm{0}{0}{ML_Bayesian_Pespective}
\abx@aux@cite{neal}
\abx@aux@segm{0}{0}{neal}
\abx@aux@cite{NUTS}
\abx@aux@segm{0}{0}{NUTS}
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}MCMC used in thesis}{26}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Design and properties of Bayesian neural network}{26}{subsection.0.3.2.2}\protected@file@percent }
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Prior samples from 3 layer BNN. The number of units in each of the three layers have a large influence on the BNNs ability to be expressive. Note that the inference becomes more computationally demanding with BNN size.\relax }}{27}{figure.caption.19}\protected@file@percent }
\newlabel{n_unit_BNN}{{3.6}{27}{Prior samples from 3 layer BNN. The number of units in each of the three layers have a large influence on the BNNs ability to be expressive. Note that the inference becomes more computationally demanding with BNN size.\relax }{figure.caption.19}{}}
\newlabel{n_unit_BNN@cref}{{[figure][6][0,3]3.6}{[1][27][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example where 100 nodes on each of three layers, lead to a much more expressive model. $\sigma ^2$ follows a informative prior $InvGamma(1000,1)$, i.e. prior mean $E[\sigma ^2] \approx \frac  {1}{1000}$, and variance $\approx \frac  {1}{1000^3}$, however since the data is distributed in such a complex way the limited expressiveness of the model, forces the model to infer make $\sigma ^2$ large, i.e. including the data in the noise\relax }}{27}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Summary}{27}{section.0.3.3}\protected@file@percent }
\abx@aux@cite{bishop1995neural}
\abx@aux@segm{0}{0}{bishop1995neural}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\abx@aux@cite{JordanPaper}
\abx@aux@segm{0}{0}{JordanPaper}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Generative models as surrogate}{29}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Conditional distribution in a Bayesian setting}{29}{section.0.4.1}\protected@file@percent }
\newlabel{mixture_include_prior}{{4.1}{29}{Conditional distribution in a Bayesian setting}{section.0.4.1}{}}
\newlabel{mixture_include_prior@cref}{{[section][1][0,4]4.1}{[1][29][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }}{30}{figure.caption.21}\protected@file@percent }
\newlabel{pred_dist_manipulation}{{4.1}{30}{Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }{figure.caption.21}{}}
\newlabel{pred_dist_manipulation@cref}{{[figure][1][0,4]4.1}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Mean and variance of predictive distribution}{30}{subsection.0.4.1.1}\protected@file@percent }
\newlabel{mean_variance_pred_mixture}{{4.1.1}{30}{Mean and variance of predictive distribution}{subsection.0.4.1.1}{}}
\newlabel{mean_variance_pred_mixture@cref}{{[subsection][1][0,4,1]4.1.1}{[1][30][]30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conditional of mixture model}{31}{section.0.4.2}\protected@file@percent }
\newlabel{Conditional_mixture}{{4.2}{31}{Conditional of mixture model}{section.0.4.2}{}}
\newlabel{Conditional_mixture@cref}{{[section][2][0,4]4.2}{[1][31][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Kernel density estimator regression}{32}{section.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Conditional of Kernel density estimator}{32}{subsection.0.4.3.1}\protected@file@percent }
\abx@aux@cite{GMR}
\abx@aux@segm{0}{0}{GMR}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Gaussian mixture regression}{33}{section.0.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Conditional of Gaussian mixture model}{33}{subsection.0.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Sum product networks}{33}{section.0.4.5}\protected@file@percent }
\abx@aux@cite{RAT_SPN}
\abx@aux@segm{0}{0}{RAT_SPN}
\newlabel{SPN_1}{{\caption@xref {SPN_1}{ on input line 200}}{34}{Sum product networks}{testexample.10}{}}
\newlabel{SPN_1@cref}{{[section][5][0,4]4.5}{[1][34][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of random constructed sum-product network (RAT-SPN) for the joint distribution $p(x_1, \dots  , x_5, y)$. The product nodes, $\otimes $, always combine 2 nodes from different scopes, while sum nodes, $\oplus $, sum all of the product nodes for similar scopes. Note the drawing is not complete; This illustarates a RAT-SPN with 3 channels, so every cluster of product nodes has size 9 and every cluster of sum nodes (except the root) has size 3.\relax }}{35}{figure.caption.22}\protected@file@percent }
\newlabel{SPN_graph_illu}{{4.2}{35}{Illustration of random constructed sum-product network (RAT-SPN) for the joint distribution $p(x_1, \dots , x_5, y)$. The product nodes, $\otimes $, always combine 2 nodes from different scopes, while sum nodes, $\oplus $, sum all of the product nodes for similar scopes. Note the drawing is not complete; This illustarates a RAT-SPN with 3 channels, so every cluster of product nodes has size 9 and every cluster of sum nodes (except the root) has size 3.\relax }{figure.caption.22}{}}
\newlabel{SPN_graph_illu@cref}{{[figure][2][0,4]4.2}{[1][34][]35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Graphical representation of the SPN used in Figure \ref  {SPN_fig}. The 9 green arrows are weighted and sums to 1\relax }}{35}{figure.caption.23}\protected@file@percent }
\newlabel{SPN_fig2}{{4.3}{35}{Graphical representation of the SPN used in Figure \ref {SPN_fig}. The 9 green arrows are weighted and sums to 1\relax }{figure.caption.23}{}}
\newlabel{SPN_fig2@cref}{{[figure][3][0,4]4.3}{[1][35][]35}}
\abx@aux@cite{??}
\abx@aux@segm{0}{0}{??}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces SPN on the joint probalility $p(x,y)$ with 3 leaf distributions in each scope (shown on the axes), trained on 3 different data sets. Left: The data lies perfect for the SPN. Middle: Numbers in the graph represent how the weight of each mixture component is weighted. Right: The data is distributed badly for the SPN.\relax }}{36}{figure.caption.24}\protected@file@percent }
\newlabel{SPN_fig}{{4.4}{36}{SPN on the joint probalility $p(x,y)$ with 3 leaf distributions in each scope (shown on the axes), trained on 3 different data sets. Left: The data lies perfect for the SPN. Middle: Numbers in the graph represent how the weight of each mixture component is weighted. Right: The data is distributed badly for the SPN.\relax }{figure.caption.24}{}}
\newlabel{SPN_fig@cref}{{[figure][4][0,4]4.4}{[1][35][]36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}SPN as a mixture model}{36}{subsection.0.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Conditional of SPN}{37}{subsection.0.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{calculation of responsibility}{37}{subsubsection*.25}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Mixture model training}{38}{section.0.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Expectation-maximization for mixture models}{38}{subsection.0.4.6.1}\protected@file@percent }
\newlabel{mixture_pdf}{{4.10}{38}{Expectation-maximization for mixture models}{equation.0.4.6.10}{}}
\newlabel{mixture_pdf@cref}{{[equation][10][0,4]4.10}{[1][38][]38}}
\newlabel{EM}{{\caption@xref {EM}{ on input line 56}}{39}{Expectation-maximization for mixture models}{testexample2.5}{}}
\newlabel{EM@cref}{{[subsection][1][0,4,6]4.6.1}{[1][38][]39}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}EM for Gaussian mixture}{40}{subsection.0.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary}{41}{section.0.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{43}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Bayesian Optimization (BO)}{43}{section.0.5.1}\protected@file@percent }
\newlabel{BO_implementation}{{5.1}{43}{Bayesian Optimization (BO)}{section.0.5.1}{}}
\newlabel{BO_implementation@cref}{{[section][1][0,5]5.1}{[1][43][]43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Surrogates}{43}{section.0.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Discriminative surrogates}{43}{subsection.0.5.2.1}\protected@file@percent }
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\abx@aux@cite{GMR}
\abx@aux@segm{0}{0}{GMR}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Overview of chosen discriminative surrogate models \relax }}{44}{table.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generative surrogates}{44}{subsubsection*.27}\protected@file@percent }
\abx@aux@cite{GMR}
\abx@aux@segm{0}{0}{GMR}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Overview of chosen generative surrogate models \relax }}{45}{table.caption.28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Standardized data}{45}{section.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experimental results}{46}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Regression analysis methodology}{46}{section.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Uncertainty quantification}{46}{subsection.0.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Predictive power}{47}{subsection.0.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Benchmark surrogate model}{47}{subsection.0.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Model comparison on test problems (regression)}{47}{section.0.6.2}\protected@file@percent }
\newlabel{TEST_problems}{{\caption@xref {TEST_problems}{ on input line 311}}{47}{Model comparison on test problems (regression)}{figure.caption.29}{}}
\newlabel{TEST_problems@cref}{{[section][2][0,6]6.2}{[1][47][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Two different performance measures of all surrogate models as a function of number of training data points with 10000 testpoints. Left: Predictive power, i.e. mean relative error between predictive mean and the test points (small is better). Right: Uncertainty quantification, measured with exponential mean predictive log density (large is better). All lines reprecent the average across 10 random seeds.\relax }}{48}{figure.caption.30}\protected@file@percent }
\newlabel{Test1_reg_plot}{{6.1}{48}{Two different performance measures of all surrogate models as a function of number of training data points with 10000 testpoints. Left: Predictive power, i.e. mean relative error between predictive mean and the test points (small is better). Right: Uncertainty quantification, measured with exponential mean predictive log density (large is better). All lines reprecent the average across 10 random seeds.\relax }{figure.caption.30}{}}
\newlabel{Test1_reg_plot@cref}{{[figure][1][0,6]6.1}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test1 (black line). This is one of the 90 regression experiments that conduct the results in Figure \ref  {Test1_reg_plot}. The blue lines encapsulates the 95\% credible region. The red line is the predictive mean. The green $\alpha (x) \in [0,1]$ denote the prior weighting for the generative models.\relax }}{49}{figure.caption.31}\protected@file@percent }
\newlabel{Test1_reg_visual_1}{{6.2}{49}{Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test1 (black line). This is one of the 90 regression experiments that conduct the results in Figure \ref {Test1_reg_plot}. The blue lines encapsulates the 95\% credible region. The red line is the predictive mean. The green $\alpha (x) \in [0,1]$ denote the prior weighting for the generative models.\relax }{figure.caption.31}{}}
\newlabel{Test1_reg_visual_1@cref}{{[figure][2][0,6]6.2}{[1][48][]49}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref  {Test1_reg_plot}.\relax }}{49}{figure.caption.32}\protected@file@percent }
\newlabel{Test2_reg_plot}{{6.3}{49}{Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref {Test1_reg_plot}.\relax }{figure.caption.32}{}}
\newlabel{Test2_reg_plot@cref}{{[figure][3][0,6]6.3}{[1][49][]49}}
\abx@aux@cite{DeepKernelLearning}
\abx@aux@segm{0}{0}{DeepKernelLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test2 (black line). This is one of 90 experiments that conduct the results in Figure \ref  {Test2_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }}{50}{figure.caption.33}\protected@file@percent }
\newlabel{Test2_reg_visual}{{6.4}{50}{Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test2 (black line). This is one of 90 experiments that conduct the results in Figure \ref {Test2_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }{figure.caption.33}{}}
\newlabel{Test2_reg_visual@cref}{{[figure][4][0,6]6.4}{[1][50][]50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Test4: Anisotropic problem}{50}{subsection.0.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref  {Test1_reg_plot}.\relax }}{51}{figure.caption.34}\protected@file@percent }
\newlabel{Test3_reg_plot}{{6.5}{51}{Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref {Test1_reg_plot}.\relax }{figure.caption.34}{}}
\newlabel{Test3_reg_plot@cref}{{[figure][5][0,6]6.5}{[1][50][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test3 (black line). This is one of 90 experiments that conduct the results in Figure \ref  {Test3_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }}{51}{figure.caption.35}\protected@file@percent }
\newlabel{Test3_reg_visual}{{6.6}{51}{Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test3 (black line). This is one of 90 experiments that conduct the results in Figure \ref {Test3_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }{figure.caption.35}{}}
\newlabel{Test3_reg_visual@cref}{{[figure][6][0,6]6.6}{[1][50][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref  {Test1_reg_plot}.\relax }}{51}{figure.caption.36}\protected@file@percent }
\newlabel{Test4_reg_plot}{{6.7}{51}{Left: Predictive power (small is better). Right: Uncertainty quantification (large is better). Average across 10 random seeds. More details in Figure \ref {Test1_reg_plot}.\relax }{figure.caption.36}{}}
\newlabel{Test4_reg_plot@cref}{{[figure][7][0,6]6.7}{[1][50][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test4 (black line). This is one of 90 experiments that conduct the results in Figure \ref  {Test4_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }}{52}{figure.caption.37}\protected@file@percent }
\newlabel{Test4_reg_visual}{{6.8}{52}{Plots visualizing all surrogate-models fit of 36 data points (black dots) from Test4 (black line). This is one of 90 experiments that conduct the results in Figure \ref {Test4_reg_plot}. The blue lines define the 95\% credible region. Predictive mean (red). $\alpha (x) \in [0,1]$ is prior weighting (green).\relax }{figure.caption.37}{}}
\newlabel{Test4_reg_visual@cref}{{[figure][8][0,6]6.8}{[1][52][]52}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Model comparison on test problems (Bayesian Optimization)}{53}{section.0.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces All test function used in Bayesian Optimization. We redefine Test3 and Test4.\relax }}{53}{figure.caption.38}\protected@file@percent }
\newlabel{TEST_problems2}{{6.9}{53}{All test function used in Bayesian Optimization. We redefine Test3 and Test4.\relax }{figure.caption.38}{}}
\newlabel{TEST_problems2@cref}{{[figure][9][0,6]6.9}{[1][53][]53}}
\abx@aux@cite{hansen2009real}
\abx@aux@segm{0}{0}{hansen2009real}
\abx@aux@cite{COCO}
\abx@aux@segm{0}{0}{COCO}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Bayesian optimization of the Test functions average accross 20 different samples of initial dataset of size 5.\relax }}{54}{figure.caption.39}\protected@file@percent }
\newlabel{Test_bayesOpt}{{6.10}{54}{Bayesian optimization of the Test functions average accross 20 different samples of initial dataset of size 5.\relax }{figure.caption.39}{}}
\newlabel{Test_bayesOpt@cref}{{[figure][10][0,6]6.10}{[1][53][]54}}
\abx@aux@cite{hansen2009real}
\abx@aux@segm{0}{0}{hansen2009real}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Test on BBOB in higher dimensions}{55}{section.0.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces COCO Test functions f3, f9,f15, and f21 in 2 dimensions\relax }}{55}{figure.caption.40}\protected@file@percent }
\newlabel{COCO_tests}{{6.11}{55}{COCO Test functions f3, f9,f15, and f21 in 2 dimensions\relax }{figure.caption.40}{}}
\newlabel{COCO_tests@cref}{{[figure][11][0,6]6.11}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Regression performance on BBOB}{55}{subsection.0.6.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Regression performance of 4 selected out of 12 problems of BBOB (f3,f9,f15,f21 in dimensions 2,3,5). All plots are in Appendix \ref  {BBOB_regression_all}. Top: Predictive power. Bottom: Uncertainty quantification. More details in Figure \ref  {Test1_reg_plot}. Average across 10 different samples of seeded samples from the problems.\relax }}{56}{figure.caption.41}\protected@file@percent }
\newlabel{BBOB_regression}{{6.12}{56}{Regression performance of 4 selected out of 12 problems of BBOB (f3,f9,f15,f21 in dimensions 2,3,5). All plots are in Appendix \ref {BBOB_regression_all}. Top: Predictive power. Bottom: Uncertainty quantification. More details in Figure \ref {Test1_reg_plot}. Average across 10 different samples of seeded samples from the problems.\relax }{figure.caption.41}{}}
\newlabel{BBOB_regression@cref}{{[figure][12][0,6]6.12}{[1][55][]56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Bayesian optimization performance on BBOB}{56}{subsection.0.6.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Bayesian optimization of BBOB in 2 dimensions, average accross 20 different samples of initial dataset of size 5)\relax }}{56}{figure.caption.42}\protected@file@percent }
\newlabel{BBOB_bayesOpt}{{6.13}{56}{Bayesian optimization of BBOB in 2 dimensions, average accross 20 different samples of initial dataset of size 5)\relax }{figure.caption.42}{}}
\newlabel{BBOB_bayesOpt@cref}{{[figure][13][0,6]6.13}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Bayesian optimization of BBOB in 3 dimensions average accross 20 different samples of initial dataset of size 5)\relax }}{57}{figure.caption.43}\protected@file@percent }
\newlabel{BBOB_bayesOpt}{{6.14}{57}{Bayesian optimization of BBOB in 3 dimensions average accross 20 different samples of initial dataset of size 5)\relax }{figure.caption.43}{}}
\newlabel{BBOB_bayesOpt@cref}{{[figure][14][0,6]6.14}{[1][56][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Bayesian optimization of BBOB in 5 dimensions average accross 20 different samples of initial dataset of size 5.\relax }}{57}{figure.caption.44}\protected@file@percent }
\newlabel{BBOB_bayesOpt5}{{6.15}{57}{Bayesian optimization of BBOB in 5 dimensions average accross 20 different samples of initial dataset of size 5.\relax }{figure.caption.44}{}}
\newlabel{BBOB_bayesOpt5@cref}{{[figure][15][0,6]6.15}{[1][57][]57}}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion}{59}{chapter.0.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Surrogate models}{60}{section.0.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion and further work}{61}{chapter.0.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}further work}{61}{section.0.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Bibliography}{62}{chapter*.45}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Gaussian mixture rules}{65}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{GaussianConditional}{{A.1}{65}{Gaussian mixture rules}{equation.A.0.1}{}}
\newlabel{GaussianConditional@cref}{{[equation][1][2147483647,0,1]A.1}{[1][65][]65}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional results}{66}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}All BayesOpt results}{66}{section.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Bayesian optimization of the 1D tests with the 20 seeded optimization paths.\relax }}{66}{figure.caption.46}\protected@file@percent }
\newlabel{BayesOpt_all}{{B.1}{66}{Bayesian optimization of the 1D tests with the 20 seeded optimization paths.\relax }{figure.caption.46}{}}
\newlabel{BayesOpt_all@cref}{{[figure][1][2147483647,0,2]B.1}{[1][66][]66}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}All BBOB f3,f9,f15,f21 regression results}{66}{section.B.2}\protected@file@percent }
\newlabel{BBOB_regression_all}{{B.2}{66}{All BBOB f3,f9,f15,f21 regression results}{section.B.2}{}}
\newlabel{BBOB_regression_all@cref}{{[subappendix][2][2147483647,0,2]B.2}{[1][66][]66}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Regression performance of all surrogate-models on f3 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }}{66}{figure.caption.47}\protected@file@percent }
\newlabel{BBOB_regression_all0}{{B.2}{66}{Regression performance of all surrogate-models on f3 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }{figure.caption.47}{}}
\newlabel{BBOB_regression_all0@cref}{{[figure][2][2147483647,0,2]B.2}{[1][66][]66}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Regression performance of all surrogate-models on f9 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }}{67}{figure.caption.48}\protected@file@percent }
\newlabel{BBOB_regression_all1}{{B.3}{67}{Regression performance of all surrogate-models on f9 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }{figure.caption.48}{}}
\newlabel{BBOB_regression_all1@cref}{{[figure][3][2147483647,0,2]B.3}{[1][66][]67}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Regression performance of all surrogate-models on f15 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }}{67}{figure.caption.49}\protected@file@percent }
\newlabel{BBOB_regression_all2}{{B.4}{67}{Regression performance of all surrogate-models on f15 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }{figure.caption.49}{}}
\newlabel{BBOB_regression_all2@cref}{{[figure][4][2147483647,0,2]B.4}{[1][67][]67}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Regression performance of all surrogate-models on f21 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }}{68}{figure.caption.50}\protected@file@percent }
\newlabel{BBOB_regression_all3}{{B.5}{68}{Regression performance of all surrogate-models on f21 (BBOB function) in dimensions 2,3,5. Average accross 10 random seeded data samples.\relax }{figure.caption.50}{}}
\newlabel{BBOB_regression_all3@cref}{{[figure][5][2147483647,0,2]B.5}{[1][67][]68}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{boyd2004convex}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{TakeHumanOutOfLoop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Nature_BO_paper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{BOHAMIANN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DNGO}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ALStatisticalModels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesoptbook}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Adam}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deterministicsurrogatemodels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{GlobalOptimization}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{PredEntropy}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{entropysearch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ML_Bayesian_Pespective}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MCMC}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NUTS}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop1995neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{JordanPaper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{GMR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{RAT_SPN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DeepKernelLearning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hansen2009real}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{COCO}{none/global//global/global}
