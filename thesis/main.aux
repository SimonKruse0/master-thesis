\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{37803332}
\pgfsyspdfmark {pgfid2}{2051147}{36194423}
\pgfsyspdfmark {pgfid4}{2362837}{53585219}
\pgfsyspdfmark {pgfid3}{2051147}{34585514}
\@writefile{toc}{\contentsline {section}{Abstract}{iii}{Doc-Start}\protected@file@percent }
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{TakeHumanOutOfLoop}
\abx@aux@segm{0}{0}{TakeHumanOutOfLoop}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\HyPL@Entry{6<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Gaussian process (GP) and Bayesian neural network (BNN) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuerity makes the standard implemented GP (optimized with emperical Bayes) overreact to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuerity happens at $x = 0.5$\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GP_vs_BNN}{{1.1}{2}{Gaussian process (GP) and Bayesian neural network (BNN) fitted to 18 data points. The objective function is the dashed black line. This examplifies how a discontinuerity makes the standard implemented GP (optimized with emperical Bayes) overreact to all other areas in the domain $[0,1]$, while the Bayesian NN only express uncertainty where the discontinuerity happens at $x = 0.5$\relax }{figure.caption.2}{}}
\newlabel{GP_vs_BNN@cref}{{[figure][1][0,1]1.1}{[1][1][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Project limitation}{2}{section.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{2}{section.0.1.2}\protected@file@percent }
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\abx@aux@cite{DNGO}
\abx@aux@segm{0}{0}{DNGO}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{Nature_BO_paper}
\abx@aux@segm{0}{0}{Nature_BO_paper}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Related work}{3}{section.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Structure of the thesis}{3}{section.0.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Notation}{4}{section.0.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Bayesian notation}{4}{subsection.0.1.5.1}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Optimization}{5}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Optimization methodology}{5}{section.0.2.1}\protected@file@percent }
\newlabel{OPT}{{2.1}{5}{Optimization methodology}{equation.0.2.1.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][5][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{algOPT}{{1}{5}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Sample-efficient optimization}{6}{subsection.0.2.1.1}\protected@file@percent }
\abx@aux@cite{Adam}
\abx@aux@segm{0}{0}{Adam}
\abx@aux@cite{deterministicsurrogatemodels}
\abx@aux@segm{0}{0}{deterministicsurrogatemodels}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization).\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{Sample_efficient_illustration}{{2.1}{7}{Example of an optimization task tuning a parameterised regression model with parameters $\lambda $ and $\sigma $, on a test set, i.e. minimization of prediction error. We see the first 23 evaluations out of 100 in grid search vs 23 evaluations using a sample-efficient solver (Bayesian optimization).\relax }{figure.caption.3}{}}
\newlabel{Sample_efficient_illustration@cref}{{[figure][1][0,2]2.1}{[1][7][]7}}
\abx@aux@cite{GlobalOptimization}
\abx@aux@segm{0}{0}{GlobalOptimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Exploitation and exploration}{8}{subsection.0.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Noisy objective functions}{8}{subsection.0.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying objective (black sin-function). Bottom: The expected improvement \textit  {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text  {policy}_{BO}(\mathcal  {D}) = 26.06$\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{BO_example}{{2.2}{9}{Top: Bayesian regression model (Gaussian Process) is fitted to the observed data, which are sampled from the underlying objective (black sin-function). Bottom: The expected improvement \textit {acquisition function} is maximied at the orange arrow, i.e. the location of the next sample. $\text {policy}_{BO}(\mathcal {D}) = 26.06$\relax }{figure.caption.4}{}}
\newlabel{BO_example@cref}{{[figure][2][0,2]2.2}{[1][8][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian regression}{9}{section.0.2.2}\protected@file@percent }
\newlabel{Predictive2}{{2.2}{9}{Bayesian regression}{equation.0.2.2.2}{}}
\newlabel{Predictive2@cref}{{[equation][2][0,2]2.2}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Surrogate model}{9}{subsection.0.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inference of surrogate models}{10}{subsection.0.2.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Overview of inference methods applied on the statistical models used in this project. $E$ is the number of edges in the SPN. $n$ is the number of data points. $K \leq n$ is the number of mixture components. We will soon learn that for an SPN the number of mixture components is exponentially larger than the number of edges i.e. $E << K$. In theory MCMC methods samples from true the posterior distribution, and do not need any fitting/learning. \relax }}{10}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Acquisition function}{11}{section.0.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The same regression model and points as Figure \ref  {BO_example}, but with three different acquisition functions: Expected improvement and negative lower confidence bound for two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{Different_AQ_functions}{{2.3}{11}{The same regression model and points as Figure \ref {BO_example}, but with three different acquisition functions: Expected improvement and negative lower confidence bound for two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.\relax }{figure.caption.6}{}}
\newlabel{Different_AQ_functions@cref}{{[figure][3][0,2]2.3}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Contourplot of expected improvement and lower confidence bound (for two different quantiles) for different (Gaussian) predictive uncertainties $\sigma _x = \sqrt  {\mathbb  {V}ar_{p(y|x,\mathcal  {D})y]}}$ versus the average improvement $y_{\qopname  \relax m{min}}-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal  {D})}[y]$ (high values are dark). The colored lines are the mapping $x \DOTSB \mapstochar \rightarrow (\sigma _x, y_{\qopname  \relax m{min}}-\mu _x)$ for $x = [-100, 100]$ for the Bayesian regression function in Figure \ref  {Different_AQ_functions} - and thereby explains how the acquisition functions balances exploitation and exploration. The orange dot represent the point maximizing the acquisition function\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{EI_illustration}{{2.4}{11}{Contourplot of expected improvement and lower confidence bound (for two different quantiles) for different (Gaussian) predictive uncertainties $\sigma _x = \sqrt {\mathbb {V}ar_{p(y|x,\mathcal {D})y]}}$ versus the average improvement $y_{\min }-\mu _x$, where $\mu _x = E_{p(y|x,\mathcal {D})}[y]$ (high values are dark). The colored lines are the mapping $x \mapsto (\sigma _x, y_{\min }-\mu _x)$ for $x = [-100, 100]$ for the Bayesian regression function in Figure \ref {Different_AQ_functions} - and thereby explains how the acquisition functions balances exploitation and exploration. The orange dot represent the point maximizing the acquisition function\relax }{figure.caption.7}{}}
\newlabel{EI_illustration@cref}{{[figure][4][0,2]2.4}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Expected improvement}{12}{subsection.0.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Exact expected improvement}{12}{subsubsection*.8}\protected@file@percent }
\newlabel{ExactEI}{{2.3.1}{12}{Exact expected improvement}{subsubsection*.8}{}}
\newlabel{ExactEI@cref}{{[subsection][1][0,2,3]2.3.1}{[1][12][]12}}
\@writefile{toc}{\contentsline {subsubsection}{Approximate expected improvement}{13}{subsubsection*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Lower confidense bound}{13}{subsection.0.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Entropy search}{13}{subsection.0.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}probability of improvement}{13}{subsection.0.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Discriminative surrogate models}{15}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{predictive_posterior_dist}{{3.1}{15}{Discriminative surrogate models}{equation.0.3.0.1}{}}
\newlabel{predictive_posterior_dist@cref}{{[equation][1][0,3]3.1}{[1][15][]15}}
\newlabel{GP_likelihood}{{3.2}{15}{Discriminative surrogate models}{equation.0.3.0.2}{}}
\newlabel{GP_likelihood@cref}{{[equation][2][0,3]3.2}{[1][15][]15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gaussian process surrogate}{15}{section.0.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: Samples from $\mathcal  {N}(\textbf  {f}|0,\kappa (x_1,\dots  , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution (right).\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{GP_illustration}{{3.1}{16}{Left: Samples from $\mathcal {N}(\textbf {f}|0,\kappa (x_1,\dots , x_{11}))$ where $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution (right).\relax }{figure.caption.10}{}}
\newlabel{GP_illustration@cref}{{[figure][1][0,3]3.1}{[1][16][]16}}
\newlabel{prior_gp}{{3.3}{16}{Gaussian process surrogate}{equation.0.3.1.3}{}}
\newlabel{prior_gp@cref}{{[equation][3][0,3]3.3}{[1][16][]16}}
\newlabel{conditional_GP}{{3.4}{16}{Gaussian process surrogate}{equation.0.3.1.4}{}}
\newlabel{conditional_GP@cref}{{[equation][4][0,3]3.4}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Exact predictive distribution}{16}{subsection.0.3.1.1}\protected@file@percent }
\newlabel{GP_predictive}{{3.5}{16}{Exact predictive distribution}{equation.0.3.1.5}{}}
\newlabel{GP_predictive@cref}{{[equation][5][0,3]3.5}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsubsection}{Posterior function distribution}{17}{subsubsection*.11}\protected@file@percent }
\newlabel{posterior_function_new}{{3.6}{17}{Posterior function distribution}{equation.0.3.1.6}{}}
\newlabel{posterior_function_new@cref}{{[equation][6][0,3]3.6}{[1][17][]17}}
\newlabel{GP_posterior_unomalized}{{3.7}{17}{Posterior function distribution}{equation.0.3.1.7}{}}
\newlabel{GP_posterior_unomalized@cref}{{[equation][7][0,3]3.7}{[1][17][]17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Left: Samples from posterior function distribution for $\mathcal  {N}(\textbf  {f}_*|m(\textbf  {x}),V(\textbf  {x}))$ where $\textbf  {x} = \{x_1,\dots  , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf  {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{GP_illustration2}{{3.2}{18}{Left: Samples from posterior function distribution for $\mathcal {N}(\textbf {f}_*|m(\textbf {x}),V(\textbf {x}))$ where $\textbf {x} = \{x_1,\dots , x_{11}\}$ and $x_i= 0.5(i-1)$. Illustration that a samples from a Gaussian process is just samples from the multivariate normal distribution. We could potentially choose $\textbf {x}$ to be all of the real line, which will give us the GP - an infinitely large multivariate normal distribution.\relax }{figure.caption.12}{}}
\newlabel{GP_illustration2@cref}{{[figure][2][0,3]3.2}{[1][17][]18}}
\newlabel{marginal_distribution}{{3.8}{18}{Posterior function distribution}{equation.0.3.1.8}{}}
\newlabel{marginal_distribution@cref}{{[equation][8][0,3]3.8}{[1][17][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Learning - Empirical Bayes inference}{18}{subsection.0.3.1.2}\protected@file@percent }
\newlabel{GP_predictive_prior}{{3.11}{18}{Learning - Empirical Bayes inference}{equation.0.3.1.11}{}}
\newlabel{GP_predictive_prior@cref}{{[equation][11][0,3]3.11}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces There is large difference beween the two GPs the different lenght scales in the kernel matters a lot. Left is chosen 9 out of 10 times, when minimizing the \relax }}{19}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bayesian Neural Networks}{20}{section.0.3.2}\protected@file@percent }
\newlabel{BNN}{{3.2}{20}{Bayesian Neural Networks}{section.0.3.2}{}}
\newlabel{BNN@cref}{{[section][2][0,3]3.2}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Left: 20 black predictive \textit  {prior} samples from a 3 layers BNN with 50 $\qopname  \relax o{tanh}$ nodes, with standard Gaussian prior distributions. Right: 20 predictive \textit  {posterior} samples. The blue areas are $\pm $ 2 predictive standard deviations from the red predictive mean.\relax }}{20}{figure.caption.14}\protected@file@percent }
\newlabel{BNN_prior_posterior}{{3.4}{20}{Left: 20 black predictive \textit {prior} samples from a 3 layers BNN with 50 $\tanh $ nodes, with standard Gaussian prior distributions. Right: 20 predictive \textit {posterior} samples. The blue areas are $\pm $ 2 predictive standard deviations from the red predictive mean.\relax }{figure.caption.14}{}}
\newlabel{BNN_prior_posterior@cref}{{[figure][4][0,3]3.4}{[1][20][]20}}
\abx@aux@cite{??}
\abx@aux@segm{0}{0}{??}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Graphical representation of a Bayesian neural network compared to a neural network. The weights are assigned a probability density. Note that we often prior assume no correlation and a standard normal distribution, but the posterior (after observing data) might contain correlations between the weights (from CYDA)\relax }}{21}{figure.caption.15}\protected@file@percent }
\newlabel{BNN_illustration2}{{3.5}{21}{Graphical representation of a Bayesian neural network compared to a neural network. The weights are assigned a probability density. Note that we often prior assume no correlation and a standard normal distribution, but the posterior (after observing data) might contain correlations between the weights (from CYDA)\relax }{figure.caption.15}{}}
\newlabel{BNN_illustration2@cref}{{[figure][5][0,3]3.5}{[1][20][]21}}
\abx@aux@cite{??}
\abx@aux@segm{0}{0}{??}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{BOHAMIANN}
\abx@aux@segm{0}{0}{BOHAMIANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}MCMC used in thesis}{24}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NUTS}{24}{subsubsection*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adaptive stochatic HMC}{24}{subsubsection*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Design and properties of Bayesian neural network}{24}{subsection.0.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Number of units in each of the 3 layers have a large influence of the BNNs ability to be expressive <lav om>\relax }}{25}{figure.caption.18}\protected@file@percent }
\newlabel{n_unit_BNN}{{3.6}{25}{Number of units in each of the 3 layers have a large influence of the BNNs ability to be expressive <lav om>\relax }{figure.caption.18}{}}
\newlabel{n_unit_BNN@cref}{{[figure][6][0,3]3.6}{[1][25][]25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example where 100 nodes on each of three layers, lead to a much more expressive model. $\sigma ^2$ follows a informative prior $InvGamma(1000,1)$, i.e. prior mean $E[\sigma ^2] \approx \frac  {1}{1000}$, and variance $\approx \frac  {1}{1000^3}$, however since the data is distributed in such a complex way the limited expressiveness of the model, forces the model to infer make $\sigma ^2$ large, i.e. including the data in the noise\relax }}{25}{figure.caption.19}\protected@file@percent }
\abx@aux@cite{bishop1995neural}
\abx@aux@segm{0}{0}{bishop1995neural}
\abx@aux@cite{ALStatisticalModels}
\abx@aux@segm{0}{0}{ALStatisticalModels}
\abx@aux@cite{JordanPaper}
\abx@aux@segm{0}{0}{JordanPaper}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Generative models as surrogate}{27}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Conditional distribution in a Bayesian setting}{27}{section.0.4.1}\protected@file@percent }
\newlabel{mixture_include_prior}{{4.1}{27}{Conditional distribution in a Bayesian setting}{section.0.4.1}{}}
\newlabel{mixture_include_prior@cref}{{[section][1][0,4]4.1}{[1][27][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{pred_dist_manipulation}{{4.1}{29}{Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N\cdot \Delta $. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should transform into the uncertain prior\relax }{figure.caption.20}{}}
\newlabel{pred_dist_manipulation@cref}{{[figure][1][0,4]4.1}{[1][28][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Mean and variance of predictive distribution v1}{29}{subsection.0.4.1.1}\protected@file@percent }
\newlabel{mean_variance_pred_mixture}{{4.1.1}{29}{Mean and variance of predictive distribution v1}{subsection.0.4.1.1}{}}
\newlabel{mean_variance_pred_mixture@cref}{{[subsection][1][0,4,1]4.1.1}{[1][29][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conditional of mixture model}{29}{section.0.4.2}\protected@file@percent }
\newlabel{Conditional_mixture}{{4.2}{29}{Conditional of mixture model}{section.0.4.2}{}}
\newlabel{Conditional_mixture@cref}{{[section][2][0,4]4.2}{[1][29][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Kernel estimator regression}{31}{section.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Conditional of Kernel density estimator}{31}{subsection.0.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Gaussian mixture regression}{31}{section.0.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Conditional of Gaussian mixture model}{32}{subsection.0.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Sum product networks}{32}{section.0.4.5}\protected@file@percent }
\newlabel{SPN_1}{{\caption@xref {SPN_1}{ on input line 183}}{32}{Sum product networks}{section.0.4.5}{}}
\newlabel{SPN_1@cref}{{[section][5][0,4]4.5}{[1][32][]32}}
\abx@aux@cite{RAT_SPN}
\abx@aux@segm{0}{0}{RAT_SPN}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of random constructed sum-product network (RAT-SPN) for the joint distribution $p(x_1, \dots  , x_5, y)$. The product nodes always combine 2 nodes from different scopes, while sum nodes sum all of the product nodes for similar scopes. Note the drawing is not complete; This illustarates a RAT-SPN with 3 channels, so every cluster of product nodes has size 9 and every cluster of sum nodes (except the root) has size 3.\relax }}{33}{figure.caption.21}\protected@file@percent }
\newlabel{SPN_graph_illu}{{4.2}{33}{Illustration of random constructed sum-product network (RAT-SPN) for the joint distribution $p(x_1, \dots , x_5, y)$. The product nodes always combine 2 nodes from different scopes, while sum nodes sum all of the product nodes for similar scopes. Note the drawing is not complete; This illustarates a RAT-SPN with 3 channels, so every cluster of product nodes has size 9 and every cluster of sum nodes (except the root) has size 3.\relax }{figure.caption.21}{}}
\newlabel{SPN_graph_illu@cref}{{[figure][2][0,4]4.2}{[1][33][]33}}
\abx@aux@cite{??}
\abx@aux@segm{0}{0}{??}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Graphical representation of the SPN used in Figure \ref  {SPN_fig}. The 9 green arrows are weighted and sums to 1\relax }}{34}{figure.caption.22}\protected@file@percent }
\newlabel{SPN_fig2}{{4.3}{34}{Graphical representation of the SPN used in Figure \ref {SPN_fig}. The 9 green arrows are weighted and sums to 1\relax }{figure.caption.22}{}}
\newlabel{SPN_fig2@cref}{{[figure][3][0,4]4.3}{[1][33][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces SPN on the joint probalility $p(x,y)$ with 3 leaf distributions in each scope (shown on the axes), trained on 3 different data sets. Left: The data lies perfect for the SPN. Middle: Numbers in the graph represent how the weight of each mixture component is weighted. Right: The data is distributed badly for the SPN.\relax }}{34}{figure.caption.23}\protected@file@percent }
\newlabel{SPN_fig}{{4.4}{34}{SPN on the joint probalility $p(x,y)$ with 3 leaf distributions in each scope (shown on the axes), trained on 3 different data sets. Left: The data lies perfect for the SPN. Middle: Numbers in the graph represent how the weight of each mixture component is weighted. Right: The data is distributed badly for the SPN.\relax }{figure.caption.23}{}}
\newlabel{SPN_fig@cref}{{[figure][4][0,4]4.4}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}SPN as a mixture model}{34}{subsection.0.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Conditional of SPN}{35}{subsection.0.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{calculation of responsibility}{35}{subsubsection*.24}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Mixture model training}{36}{section.0.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Expectation-maximization for mixture models}{36}{subsection.0.4.6.1}\protected@file@percent }
\newlabel{mixture_pdf}{{4.10}{36}{Expectation-maximization for mixture models}{equation.0.4.6.10}{}}
\newlabel{mixture_pdf@cref}{{[equation][10][0,4]4.10}{[1][36][]36}}
\newlabel{EM}{{\caption@xref {EM}{ on input line 56}}{37}{Expectation-maximization for mixture models}{testexample2.4}{}}
\newlabel{EM@cref}{{[subsection][1][0,4,6]4.6.1}{[1][36][]37}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{41}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}implementation}{41}{section.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}standardized data}{41}{subsection.0.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Model compartment on test problems}{41}{section.0.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Overview of chosen models \relax }}{42}{table.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Mean predictive performance MAE (left) and mean predictiv probalility (right) as functions of number of data points. All curves reprecent the average across 10 runs for each model. The Gaussian process has the bedst predictive power, however it scores very low in the \relax }}{43}{figure.caption.26}\protected@file@percent }
\newlabel{Test1_reg_plot}{{5.1}{43}{Mean predictive performance MAE (left) and mean predictiv probalility (right) as functions of number of data points. All curves reprecent the average across 10 runs for each model. The Gaussian process has the bedst predictive power, however it scores very low in the \relax }{figure.caption.26}{}}
\newlabel{Test1_reg_plot@cref}{{[figure][1][0,5]5.1}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plots visulising the results in figure at $n_{data} = 23$\relax }}{43}{figure.caption.27}\protected@file@percent }
\newlabel{Test1_reg_visual_1}{{5.2}{43}{Plots visulising the results in figure at $n_{data} = 23$\relax }{figure.caption.27}{}}
\newlabel{Test1_reg_visual_1@cref}{{[figure][2][0,5]5.2}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Plots visulising the results in figure at $n_{data} = 133$\relax }}{44}{figure.caption.28}\protected@file@percent }
\newlabel{Test1_reg_visual_2}{{5.3}{44}{Plots visulising the results in figure at $n_{data} = 133$\relax }{figure.caption.28}{}}
\newlabel{Test1_reg_visual_2@cref}{{[figure][3][0,5]5.3}{[1][43][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Plots visulising the results in above figure.\relax }}{44}{figure.caption.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Plots visulising the results in above figure.\relax }}{45}{figure.caption.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Test3: multi-modal problem}{45}{subsection.0.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Plots visulising the results in above figure.\relax }}{46}{figure.caption.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Plots visulising the results in above figure.\relax }}{47}{figure.caption.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Test4: anisotropic problem}{47}{subsection.0.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Plots visulising the results in above figure.\relax }}{48}{figure.caption.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Plots visulising the results in above figure.\relax }}{49}{figure.caption.34}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Regression analysis}{49}{section.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}uncertainty quantification}{49}{subsection.0.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Regression of a probabilistic objective function, is not a smart choice using anything else than a mixture regression model. Top we see the performance of 5 different regression models fitted to an increasing amount of training data, the underlying function is tricky, as it jumps between multiple objective functions yielding a violation to most of the generative models, where most are capable of handling gaussian noise this is not gaussian noise and hence a very difficult problem. This kind of objective function could definitely be relevant in certain cases.\relax }}{50}{figure.caption.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Plots visulising the results in above figure.\relax }}{50}{figure.caption.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}prediction quantification}{50}{subsection.0.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}regression benchmark}{50}{subsection.0.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Mixture regression}{50}{section.0.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Regression analysis of GP, BOHAMIANN and NumpyNN 1D}{51}{section.0.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{51}{section.0.5.6}\protected@file@percent }
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 532}}{51}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.37}{}}
\newlabel{f1_reg_2D@cref}{{[section][6][0,5]5.6}{[1][51][]51}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Mixture regression on simple functions}{51}{section.0.5.7}\protected@file@percent }
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 557}}{52}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.38}{}}
\newlabel{f1_reg_2D@cref}{{[section][6][0,5]5.6}{[1][51][]52}}
\newlabel{f23_reg_2D}{{\caption@xref {f23_reg_2D}{ on input line 585}}{53}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.39}{}}
\newlabel{f23_reg_2D@cref}{{[section][6][0,5]5.6}{[1][51][]53}}
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 610}}{54}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.40}{}}
\newlabel{f1_reg_2D@cref}{{[section][6][0,5]5.6}{[1][51][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces 2D Regression plot for a few of the problems, with the correct \relax }}{54}{figure.caption.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction\relax }}{54}{figure.caption.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Regression plot for anther problem\relax }}{54}{figure.caption.43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 1: "5 separable functions"\relax }}{55}{figure.caption.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 2: "4 functions with low or moderate conditioning"\relax }}{55}{figure.caption.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 3: "5 functions with high conditioning, unimodal"\relax }}{55}{figure.caption.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 4: "5 multi-modal functions with adequate global structure"\relax }}{55}{figure.caption.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 5: "5 multi-modal functions with weak global structure"\relax }}{56}{figure.caption.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search\relax }}{56}{figure.caption.49}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Test functions f1 to f12\relax }}{56}{figure.caption.50}\protected@file@percent }
\newlabel{2DBlockcyclic}{{5.21}{56}{Test functions f1 to f12\relax }{figure.caption.50}{}}
\newlabel{2DBlockcyclic@cref}{{[figure][21][0,5]5.21}{[1][51][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Test functions f13-24\relax }}{57}{figure.caption.51}\protected@file@percent }
\newlabel{f13-24}{{5.22}{57}{Test functions f13-24\relax }{figure.caption.51}{}}
\newlabel{f13-24@cref}{{[figure][22][0,5]5.22}{[1][51][]57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and further work}{59}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}further work}{59}{section.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Bibliography}{60}{chapter*.52}\protected@file@percent }
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{boyd2004convex}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{TakeHumanOutOfLoop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Nature_BO_paper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{BOHAMIANN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DNGO}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ALStatisticalModels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesoptbook}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Adam}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deterministicsurrogatemodels}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{GlobalOptimization}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop1995neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{JordanPaper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{RAT_SPN}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2015scalable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NIPS2016_a96d3afe}{none/global//global/global}
