\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{2051147}{38075306}
\pgfsyspdfmark {pgfid2}{2051147}{36738371}
\pgfsyspdfmark {pgfid4}{2362837}{54401141}
\pgfsyspdfmark {pgfid3}{2051147}{35401436}
\HyPL@Entry{2<</S/r>>}
\HyPL@Entry{4<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GP_vs_BNN}{{\caption@xref {fig:GP_vs_BNN}{ on input line 59}}{2}{Introduction}{figure.caption.2}{}}
\newlabel{fig:GP_vs_BNN@cref}{{[chapter][1][0]1}{[1][1][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Example where a discontinuerity makes the GP overreact to all other uncertainties. Left is a GP, right is a Bayesian neural network\relax }}{2}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Contribution}{2}{section.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related work}{2}{section.0.1.2}\protected@file@percent }
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the thesis}{3}{section.0.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}notation}{3}{section.0.1.4}\protected@file@percent }
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\abx@aux@cite{bayesoptbook}
\abx@aux@segm{0}{0}{bayesoptbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Optimization}{5}{chapter.0.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Optimization methodology}{5}{section.0.2.1}\protected@file@percent }
\newlabel{OPT}{{2.1}{5}{Optimization methodology}{equation.0.2.1.1}{}}
\newlabel{OPT@cref}{{[equation][1][0,2]2.1}{[1][5][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sequencial Optimization \cite {bayesoptbook} \relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{algOPT}{{1}{5}{Sequencial Optimization \cite {bayesoptbook} \relax }{algorithm.1}{}}
\newlabel{algOPT@cref}{{[algorithm][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}When to use Bayesian optmization}{6}{subsection.0.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:example}{{2.1}{7}{Hyper parameter tuning of a model $M(\lambda , \sigma )$, 23 evaluation in grid search vs 23 evaluations using Bayesian optimization\relax }{figure.caption.3}{}}
\newlabel{fig:example@cref}{{[figure][1][0,2]2.1}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Observation model}{7}{subsection.0.2.1.2}\protected@file@percent }
\newlabel{ObsModel}{{2.1.2}{7}{Observation model}{subsection.0.2.1.2}{}}
\newlabel{ObsModel@cref}{{[subsection][2][0,2,1]2.1.2}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bayesian Optimization}{8}{section.0.2.2}\protected@file@percent }
\newlabel{Predictive2}{{2.2}{8}{Bayesian Optimization}{equation.0.2.2.2}{}}
\newlabel{Predictive2@cref}{{[equation][2][0,2]2.2}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Acquisition function}{8}{section.0.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Expected improvement}{8}{subsection.0.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Entropy search}{10}{subsection.0.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}surrogate model}{11}{section.0.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Discriminative model as surrogate model}{11}{subsection.0.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Using a generative model as surrogate model}{11}{subsection.0.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Inclusion of prior distribution}{12}{subsection.0.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Inclusion of prior distribution}{12}{subsection.0.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Inclusion of prior distribution}{12}{subsection.0.2.4.5}\protected@file@percent }
\newlabel{manipulated_pred_dist}{{2.3}{12}{Inclusion of prior distribution}{equation.0.2.4.3}{}}
\newlabel{manipulated_pred_dist@cref}{{[equation][3][0,2]2.3}{[1][12][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N$. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should collapse into the uncertain prior\relax }}{13}{figure.caption.5}\protected@file@percent }
\newlabel{pred_dist_manipulation}{{2.2}{13}{Left: Illustration of how the preditive distribution is manipulated according the the scaling function $S(x) := p(x)\cdot N$. Right: Illustration of why it makes sense to manipulate the predictive distribution $p(y|x)$, if there is a small amount of input data at a region, then the predictive distribution should collapse into the uncertain prior\relax }{figure.caption.5}{}}
\newlabel{pred_dist_manipulation@cref}{{[figure][2][0,2]2.2}{[1][12][]13}}
\@writefile{toc}{\contentsline {subsubsection}{Manipulated predictive mean}{13}{subsubsection*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Manipulated predictive variance}{13}{subsubsection*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Inference of surrogate models}{14}{section.0.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Overview of inference methods applied on the statistical models used in this project. $E$ is the number of edges in the SPN. $n$ is the number of datapoints. $K \leq n$ is the number of mixture comonents. We will soon learn that for an SPN the number of mixture compenets is exponential larger than number of edges i.e. $E << K$. In theory MCMC methods samples from true the posterior distribution, and do not need any fitting/learning. \relax }}{14}{table.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Discriminative surrogate models}{15}{chapter.0.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gaussian process surrogate}{15}{section.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Exact predictive distribution}{15}{subsection.0.3.1.1}\protected@file@percent }
\newlabel{GP_predictive}{{3.1}{15}{Exact predictive distribution}{equation.0.3.1.1}{}}
\newlabel{GP_predictive@cref}{{[equation][1][0,3]3.1}{[1][15][]15}}
\newlabel{marginal_distribution}{{3.2}{15}{Exact predictive distribution}{equation.0.3.1.2}{}}
\newlabel{marginal_distribution@cref}{{[equation][2][0,3]3.2}{[1][15][]15}}
\newlabel{posterior_function}{{3.5}{15}{}{equation.0.3.1.5}{}}
\newlabel{posterior_function@cref}{{[equation][5][0,3]3.5}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Realisations> Illustartaion of GP is just samples from a multivariate normal distribution. In the limit the we have a infinite-variate normal distribution, which we call a GP.\relax }}{16}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bayesian Neural Networks}{17}{section.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}No U-Turn sampling}{20}{subsection.0.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Adaptive stochatic HMC}{21}{subsection.0.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Prior samples\relax }}{21}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Posterior samples after observing \relax }}{22}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Regression analysis}{22}{section.0.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Generative model surrogates}{23}{chapter.0.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Sum product networks}{23}{section.0.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}SPN is a mixture model}{23}{subsection.0.4.1.1}\protected@file@percent }
\newlabel{SPN4}{{\caption@xref {SPN4}{ on input line 76}}{23}{SPN is a mixture model}{definition.1}{}}
\newlabel{SPN4@cref}{{[subsection][1][0,4,1]4.1.1}{[1][23][]23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Conditional of SPN}{24}{subsection.0.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}calculation of responsibility, $\gamma (x)$}{25}{subsection.0.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Mean and variance of conditional SPN}{25}{subsection.0.4.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces left: the data lies perfect for the SPN. Right: The data distribution is not suited for SPN\relax }}{26}{figure.caption.12}\protected@file@percent }
\newlabel{SPN_fig}{{4.1}{26}{left: the data lies perfect for the SPN. Right: The data distribution is not suited for SPN\relax }{figure.caption.12}{}}
\newlabel{SPN_fig@cref}{{[figure][1][0,4]4.1}{[1][26][]26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Example of how the weight of each mixture component is turned up and down according the amount of data observed.\relax }}{26}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}SPN - prediction}{27}{subsection.0.4.1.5}\protected@file@percent }
\newlabel{SPN_1}{{\caption@xref {SPN_1}{ on input line 243}}{27}{SPN - prediction}{subsection.0.4.1.5}{}}
\newlabel{SPN_1@cref}{{[subsection][5][0,4,1]4.1.5}{[1][27][]27}}
\abx@aux@cite{SPN_EM}
\abx@aux@segm{0}{0}{SPN_EM}
\newlabel{SPN}{{\caption@xref {SPN}{ on input line 264}}{28}{SPN - prediction}{subsection.0.4.1.5}{}}
\newlabel{SPN@cref}{{[subsection][5][0,4,1]4.1.5}{[1][27][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}SPN - learning}{28}{subsection.0.4.1.6}\protected@file@percent }
\newlabel{SPN3}{{\caption@xref {SPN3}{ on input line 307}}{28}{SPN - learning}{definition.2}{}}
\newlabel{SPN3@cref}{{[subsection][6][0,4,1]4.1.6}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.7}Expectation-maximization for mixture models}{29}{subsection.0.4.1.7}\protected@file@percent }
\newlabel{mixture_pdf}{{4.1}{29}{Expectation-maximization for mixture models}{equation.0.4.1.1}{}}
\newlabel{mixture_pdf@cref}{{[equation][1][0,4]4.1}{[1][29][]29}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\newlabel{EM}{{\caption@xref {EM}{ on input line 410}}{30}{Expectation-maximization for mixture models}{testexample2.4}{}}
\newlabel{EM@cref}{{[subsection][7][0,4,1]4.1.7}{[1][30][]30}}
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Kernel estimator regression}{32}{section.0.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Gaussian mixture regression}{33}{section.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Gaussian Mixture Regression}{33}{subsection.0.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}GMR - prediction}{33}{subsection.0.4.3.2}\protected@file@percent }
\abx@aux@cite{bishop}
\abx@aux@segm{0}{0}{bishop}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}GMR - Leaning}{34}{subsection.0.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regression analysis}{34}{section.0.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{35}{chapter.0.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Regression analysis}{35}{section.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}uncertainty quantification}{35}{subsection.0.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}prediction quantification}{35}{subsection.0.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}regression benchmark}{35}{subsection.0.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Regression analysis of GP, BOHAMIANN and NumpyNN 1D}{35}{section.0.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{35}{section.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Mixture regression on simple functions}{35}{section.0.5.4}\protected@file@percent }
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 67}}{36}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.15}{}}
\newlabel{f1_reg_2D@cref}{{[section][3][0,5]5.3}{[1][35][]36}}
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 92}}{37}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.16}{}}
\newlabel{f1_reg_2D@cref}{{[section][3][0,5]5.3}{[1][35][]37}}
\newlabel{f23_reg_2D}{{\caption@xref {f23_reg_2D}{ on input line 120}}{38}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.17}{}}
\newlabel{f23_reg_2D@cref}{{[section][3][0,5]5.3}{[1][35][]38}}
\newlabel{f1_reg_2D}{{\caption@xref {f1_reg_2D}{ on input line 145}}{39}{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{figure.caption.18}{}}
\newlabel{f1_reg_2D@cref}{{[section][3][0,5]5.3}{[1][35][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces 2D Regression plot for a few of the problems, with the correct \relax }}{39}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction\relax }}{39}{figure.caption.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Regression plot for anther problem\relax }}{39}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 1: "5 separable functions"\relax }}{40}{figure.caption.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 2: "4 functions with low or moderate conditioning"\relax }}{40}{figure.caption.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 3: "5 functions with high conditioning, unimodal"\relax }}{40}{figure.caption.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 4: "5 multi-modal functions with adequate global structure"\relax }}{40}{figure.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates and using expected improvement with a budget of $40$ samples for group 5: "5 multi-modal functions with weak global structure"\relax }}{41}{figure.caption.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search\relax }}{41}{figure.caption.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Test functions f1 to f12\relax }}{41}{figure.caption.28}\protected@file@percent }
\newlabel{2DBlockcyclic}{{5.10}{41}{Test functions f1 to f12\relax }{figure.caption.28}{}}
\newlabel{2DBlockcyclic@cref}{{[figure][10][0,5]5.10}{[1][35][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Test functions f13-24\relax }}{42}{figure.caption.29}\protected@file@percent }
\newlabel{f13-24}{{5.11}{42}{Test functions f13-24\relax }{figure.caption.29}{}}
\newlabel{f13-24@cref}{{[figure][11][0,5]5.11}{[1][35][]42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and further work}{43}{chapter.0.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{44}{chapter*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Implementation}{45}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@cite{PhDthesis}
\abx@aux@segm{0}{0}{PhDthesis}
\abx@aux@cite{snoek2015scalable}
\abx@aux@segm{0}{0}{snoek2015scalable}
\abx@aux@cite{NIPS2016_a96d3afe}
\abx@aux@segm{0}{0}{NIPS2016_a96d3afe}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{PhDthesis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesoptbook}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snoek2015scalable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{NIPS2016_a96d3afe}{none/global//global/global}
