\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\contentsline {section}{Abstract}{iii}{Doc-Start}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.0.1}%
\contentsline {section}{\numberline {1.1}Contribution}{2}{section.0.1.1}%
\contentsline {section}{\numberline {1.2}Related work}{3}{section.0.1.2}%
\contentsline {section}{\numberline {1.3}Structure of the thesis}{4}{section.0.1.3}%
\contentsline {section}{\numberline {1.4}Notation}{4}{section.0.1.4}%
\contentsline {chapter}{\numberline {2}Bayesian Optimization}{5}{chapter.0.2}%
\contentsline {section}{\numberline {2.1}Optimization methodology}{5}{section.0.2.1}%
\contentsline {subsection}{\numberline {2.1.1}When to use sample-efficient optimization}{6}{subsection.0.2.1.1}%
\contentsline {subsubsection}{Noisy objective functions}{7}{subsubsection*.4}%
\contentsline {section}{\numberline {2.2}Bayesian regression}{9}{section.0.2.2}%
\contentsline {subsection}{\numberline {2.2.1}surrogate model}{9}{subsection.0.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Inference of surrogate models}{9}{subsection.0.2.2.2}%
\contentsline {section}{\numberline {2.3}Acquisition function}{10}{section.0.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Expected improvement}{10}{subsection.0.2.3.1}%
\contentsline {subsubsection}{Exact expected improvement}{11}{subsubsection*.8}%
\contentsline {subsubsection}{Approximate expected improvement}{12}{subsubsection*.10}%
\contentsline {subsection}{\numberline {2.3.2}Lower confidense bound}{13}{subsection.0.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Entropy search}{13}{subsection.0.2.3.3}%
\contentsline {chapter}{\numberline {3}Discriminative surrogate models}{15}{chapter.0.3}%
\contentsline {section}{\numberline {3.1}Gaussian process surrogate}{15}{section.0.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Exact predictive distribution}{16}{subsection.0.3.1.1}%
\contentsline {subsubsection}{Posterior function distribution}{17}{subsubsection*.14}%
\contentsline {subsection}{\numberline {3.1.2}Learning - Emperical bayes inference}{18}{subsection.0.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Performance characteristic for GP}{18}{subsection.0.3.1.3}%
\contentsline {section}{\numberline {3.2}Bayesian Neural Networks}{20}{section.0.3.2}%
\contentsline {subsection}{\numberline {3.2.1}No U-Turn sampling}{23}{subsection.0.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Adaptive stochatic HMC}{23}{subsection.0.3.2.2}%
\contentsline {chapter}{\numberline {4}Generative models as surrogate}{25}{chapter.0.4}%
\contentsline {section}{\numberline {4.1}Conditional distribution in a Bayesian setting}{25}{section.0.4.1}%
\contentsline {subsubsection}{Mean and variance of predictive distribution v1}{27}{subsubsection*.24}%
\contentsline {section}{\numberline {4.2}Conditional of mixture model}{27}{section.0.4.2}%
\contentsline {section}{\numberline {4.3}Kernel estimator regression}{28}{section.0.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Conditional of Kernel density estimator}{28}{subsection.0.4.3.1}%
\contentsline {section}{\numberline {4.4}Gaussian mixture regression}{29}{section.0.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Conditional of Gaussian mixture model}{29}{subsection.0.4.4.1}%
\contentsline {section}{\numberline {4.5}Sum product networks}{30}{section.0.4.5}%
\contentsline {subsection}{\numberline {4.5.1}SPN as mixture regression}{31}{subsection.0.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}SPN as a mixture model}{33}{subsection.0.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3}Conditional of SPN}{33}{subsection.0.4.5.3}%
\contentsline {subsubsection}{calculation of responsibility}{34}{subsubsection*.30}%
\contentsline {section}{\numberline {4.6}Gaussian approximation of mixture regression}{34}{section.0.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Mean and variance of conditional SPN}{34}{subsection.0.4.6.1}%
\contentsline {section}{\numberline {4.7}Mixture model training}{35}{section.0.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Expectation-maximization for mixture models}{35}{subsection.0.4.7.1}%
\contentsline {chapter}{\numberline {5}Results}{39}{chapter.0.5}%
\contentsline {section}{\numberline {5.1}implementation}{39}{section.0.5.1}%
\contentsline {subsection}{\numberline {5.1.1}standardized data}{39}{subsection.0.5.1.1}%
\contentsline {section}{\numberline {5.2}Regression analysis}{39}{section.0.5.2}%
\contentsline {subsection}{\numberline {5.2.1}uncertainty quantification}{39}{subsection.0.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}prediction quantification}{40}{subsection.0.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}regression benchmark}{40}{subsection.0.5.2.3}%
\contentsline {section}{\numberline {5.3}Mixture regression}{40}{section.0.5.3}%
\contentsline {section}{\numberline {5.4}Regression analysis of GP, BOHAMIANN and NumpyNN 1D}{40}{section.0.5.4}%
\contentsline {section}{\numberline {5.5}Regression analysis of GP, BOHAMIANN and NumpyNN 2D}{40}{section.0.5.5}%
\contentsline {section}{\numberline {5.6}Mixture regression on simple functions}{40}{section.0.5.6}%
\contentsline {chapter}{\numberline {6}Conclusion and further work}{49}{chapter.0.6}%
\contentsline {section}{\numberline {6.1}further work}{49}{section.0.6.1}%
\contentsline {chapter}{Bibliography}{50}{chapter*.49}%
