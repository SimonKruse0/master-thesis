\chapter{Bayesian Optimization}
This chapter will introduce Bayesian optimization. We start with a general introduction to the
concept of optimization (mainly based on \cite{bayesoptbook}), which culminates with the
introduction of the idea of Bayesian optimization (BO). Next, we dive into the first BO component:
The Bayesian regression methodology. Finally, the concept of an acquisition function is introduced,
with a focus on expected improvement and a brief description of the other types. 

\input{Chapters/01_optimization.tex}

% <Cope with inacuracies> i.e. allows for stochastic objective function. 
% <Uncertainty measure with prediction based on simple and clear prior 
% assumptions about the characteristic about the objective function. >
% <Provides an adequate termination condition for the opt. process>. 

% <kilde 151. Bayes Opt is assumed superior to other global optimization technics 
% with limited budget>

\section{Bayesian regression}
Whereas traditional regression workflow is the following: Given data, choose the best fitting model
parameters, make predictions using those parameters. The Bayesian framework allows us to skip the
dependency of a single set of parameters and instead use \textit{all possible} parameters by treating the set
of parameters as a random quantity, $\theta \sim p(\theta|\mathcal{D})$, where some values/realizations of $\theta$ are more
probable than others given data. In Bayesian regression we are interest is the predictive posterior distribution,  
\begin{align}\label{Predictive2}
    p(y|x, \mathcal{D}) &= \int p(y,\theta|x, \mathcal{D}) d\theta\\
    &= \int p(y|x,\theta)p(\theta|\mathcal{D}) d\theta,
\end{align}
where the posterior $p(\theta|\mathcal{D})$ gives weighting to the proposed regression model
$p(y|x,\theta)$. Note that the second equation is true because of the probability chain rule and
that $y$ is fully described by the parametric model $p(y|x,\theta)$ and the parameters $\theta$ are
fully described by the posterior distribution $p(\theta|\mathcal{D})$.
\begin{testexample2}[Bayesian methodology]
    In Bayesian modelling, $p(\theta|\mathcal{D})$ is the posterior distribution, and it is linked
     to the likelihood $p(\mathcal{D}|\theta)$ and prior $p(\theta)$ via Bayes rule,
    $$p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})} \propto
    p(\mathcal{D}|\theta)p(\theta),$$ where the evidence $p(\mathcal{D})$ act as a propotionality
    constant since $p(\theta|\mathcal{D})$ is only a function of $\theta$. In the case of
    regression, we always condition on $\textbf{x}$, 
    $$p(\theta| \mathcal{D}) = \frac{p(\textbf{y}|\textbf{x}, \theta)p(\theta|
    \textbf{x})}{p(\textbf{y}|\textbf{x})} \propto p(\textbf{y}|\textbf{x}, \theta)p(\theta| \textbf{x})$$
    The modeling task to is to specify a likelihood $p(\textbf{y}|\textbf{x},\theta)$, which encodes how likely the model $\theta$
    explain the data, and a prior $p(\theta|\textbf{x})$, which encodes our prior belief about the model. 
\end{testexample2}

\subsection{Surrogate model}
%We are now ready to define the (cheap) surrogate model $p(y|x,\mathcal{D})$ in \eqref{BayesOpt}. 
A surrogate model in a Bayesian optimization setting (i.e. $p(y|x,\mathcal{D})$ in \eqref{BayesOpt})
is a Bayesian regression model. The most used surrogate model is a Gaussian Process \cite{??}. But there have
been investigations on other surrogates, such as Bayesian neural networks and Bayesian regression
trees. These are all discriminative models, and another approach we focus on in this project is to
model $y$ and $x$ jointly in a so-called generative model, $p(x,y)$. A generative model can be used
implicitly as a surrogate from the conditional distribution of $y$ given $x$, $p(y|x)$.

In this thesis, the Bayesian regression models investigated as Bayesian optimization surrogates are
the following:
\begin{itemize}[noitemsep]
    \item Gaussian process (GP)
    \item Bayesian neural network (BNN)
    \item Kernel density regression (KDE)
    \item Gaussian mixture regression (GMR)
    \item Sum-product networks (SPN)
\end{itemize}

We now introduce the concept of inference, which is necessary for using the probabilistic surrogate models
in Bayesian Optimization. 

\subsection{Inference of surrogate models}
Inference is the process of computing answers to queries about a probabilistic model after observing
data. In Bayesian regression, the query is the predictive distribution, $p(y|x,\mathcal{D})$, as we
are interested in the distribution of $y$ given $x$ and data, $\mathcal{D}$. This
often indirectly create the posterior query, $p(\theta|\mathcal{D})$, the probability of model
parameters $\theta$ given data $\mathcal{D}$. Lastly, it is also inference when we train a Gaussian
mixture model or SPN using the expectation-maximization algorithm (EM) since we are iteratively
answering the query $\mathbb{E}_{p(z|\theta^{(k)})}[z|\theta]$.

%\subsection{Exact and approximate inference}
We distinguish between two different ways of inference: Exact and approximate inference. It is
\textit{exact inference} when a probabilistic query is calculated exactly. It is possible to
calculate exact inference on the predictive distribution for the Gaussian mixture model, Sum product
network, and Gaussian processes. Models which allow for exact inference have a powerful advantage
over the models with approximate inference since we can guarantee the answers to the queries are
correct; however, they are usually also less expressive (unable to explain complicated models). 

When it is not possible to answer a probabilistic query exact, we can approximate the answer using
\textit{approximate inference}. When dealing with complicated and expressive statistical models,
exact inference is often intractable, and we need to use approximate inference. Approximate
inference is a broad category of methods, which includes variational inference and Markov chain
Monte Carlo (MCMC). The two Bayesian Neural networks, we deal with in this project, Bohamiann and
Numpyro BNN are similar regression models but are inferred using two different MCMC methods. As
revealed later (see in section \ref{BNN}) approximate inference might be inexact and wrong. 

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l}
    %\rowcolor[HTML]{C0C0C0} 
    \textbf{Model}       & \textbf{Predictive inference}    &   \textbf{Learning} \\ \hline
    GP                          & Exact $O(n^3)$            & Empirical Bayes\\
    Numpyro BNN                 & No U-Turn Sampler         & \\
    Bohamiann BNN               & Adaptive stochatic HMC    & \\
    Kernel density regression   & Exact $O(n)$              & \\
    Gaussian mixture regression & Exact $O(K)$              & EM  \\
    SPN                         & Exact $O(E)$              &  EM $O(E)$\\
    \end{tabular}
    \caption{Overview of inference methods applied on the statistical models used in this project.
            $E$ is the number of edges in the SPN. $n$ is the number of data points. $K \leq n$ is
            the number of mixture components. We will soon learn that for an SPN the number of
            mixture components is exponentially larger than the number of edges i.e. $E << K$. In theory
            MCMC methods samples from true the posterior distribution, and do not need any
            fitting/learning. }
\end{table}

%\newpage
\section{Acquisition function}
Given a correct predictive distribution $p(y|x\mathcal{D})$ the next step in Bayesian optimization
is to select the next location $x \in \mathcal{X}$ to sample from. The next location is chosen
according to a so-called acquisition function (AQ function), which balances out the well-known
concept of exploitation and exploration. It is exploitation if the next chosen location is found
according to its average improvement. It is exploration if the next point is chosen in a region of
high uncertainty and thereby helps lower the overall uncertainty. First, we will look at the
acquisition function used in the thesis: Expected improvement. Secondly, we shortly present other
different types of acquisition functions. In Figure \ref{Different_AQ_functions} we see three
different acquisition functions. The expected improvement (EI) is known for being biased towards
exploitation. In contrast, the negative lower confidence bound (LCB) has a parameter $\beta$, which
can be tuned to make it focus more on exploration. 
\begin{figure}[H]
    \centering
    \includegraphics[trim=1cm 0cm 1cm 1cm,clip,width=\textwidth]{Pictures/illustration_AQs.pdf}
    \caption{The same regression model and points as Figure \ref{BO_example}, but with three
    different acquisition functions: Expected improvement and negative lower confidence bound for
    two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.}\label{Different_AQ_functions}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/expected_improvement_illustration.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/neg_lower_confidence_illustration_1.pdf}
      \end{minipage}
     \hfill
     \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/neg_lower_confidence_illustration_3.pdf}
      \end{minipage}
    \caption{Contourplot of expected improvement (EI) and lower confidence bound (LCB) for two different
    quantiles for different (Gaussian) predictive uncertainties $\sigma_x =
    \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})y]}}$ versus the average improvement $y_{\min}-\mu_x$,
    where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$. Darker colors indicates higher values. The colored lines are the
    mapping $x \mapsto (\sigma_x, y_{\min}-\mu_x)$ for $x = [-100, 100]$ for the Bayesian regression
    function in Figure \ref{Different_AQ_functions} - and thereby explains how the acquisition
    functions balances exploitation and exploration. The orange dot represent the point maximizing
    the acquisition function}
    \label{EI_illustration}
\end{figure}


\subsection{Expected improvement}
A popular choice of acquisition function is expected improvement, 
$$EI(x) = \mathbb{E}_{p(y|x,\mathcal{D})}[\max(0, y_{\min}-y)]$$ where we only consider the values
$y$, which improves the current best value in the expectation of the predictive distribution,
$p(y|x,\mathcal{D})$. Therefore, a $x$ which yield a bad predictive mean value
$\mathbb{E}_{p(y|x,\mathcal{D})}[y]> y_{\min}$ might still be maximizing the expectated improvement,
if the predictive uncertainty is very large at $x$. Figure \ref{EI_illustration} illustrates that a
large uncertainty in the predictive distribution (represented as the predictive variance) can lead
to relative large values even for non-improving mean predictions.


\begin{note2}[Why defining expected improvment with max]    
    Note that $\max(0,\cdot)$ is important since the Bayesian optimization othervise reduces to
    a simple non-probabilistic surrogate-based optimization method,
    \begin{align*}
        \mathbb{E}_{p(y|x,\mathcal{D})}[y_{\min}-y] = y_{\min} - \mathbb{E}_{p(y|x,\mathcal{D})}[y]
    \end{align*}
    i.e. maximizing the above is equivalent to maximizing the predictive mean, and thereby we loose
    all the valuable information about the predictive uncertainties from the Bayesian regression model. 
\end{note2}

\subsubsection{Exact expected improvement} \label{ExactEI} 
In the following derivation we assume the
predictive distribution can be approxiamted by a normal distribution dependent on the point of
interest $x$ and the data $\mathcal{D}$ (note for the GP it is in fact not an approximation), 
$$p(y|x,\mathcal{D}) \approx \mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$$ where we
will change to a less complicated notation $\mathcal{N}(y|\mu_x,
\sigma^2_x):=\mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$. This is completely fine
since $x$ is fixed (and $\mathcal{D}$ is fixed) when evaluating the expected improvement in a point
$x$. %  $\sigma_x := \sigma^2(x,\mathcal{D})$ and $\mu_x := \mu(x,\mathcal{D})$
%as evaluated functions, i.e. numbers. 
Furthermore, the density of
a standard normal distribution is denoted $\phi(\cdot):=\mathcal{N}(\cdot | 0,1)$, and the cumlative
density function (CDF) of a standard normal distribution is denoted, $\Phi(\cdot) :=
\int_{-\infty}^{\cdot} \phi(\epsilon)d\epsilon$. We will now see that the normal approximation
of the predictive distribution yields closed form solution to the expected improvement function, 

\begin{align*}
    \mathbb{E}_{\mathcal{N}(y|\mu_x, \sigma_x^2)}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) \mathcal{N}(y|\mu_x, \sigma_x^2) dy\\
    &= \int_{-\infty}^{y_{\min}} (y_{\min}-y) \frac{1}{\sigma_x}\phi\left(\frac{y-\mu_x}{\sigma_x}\right) dy\\
    &= \int_{-\infty}^{\frac{y_{\min}-\mu_x}{\sigma_x}} (y_{\min}-\mu_x-\sigma_x\epsilon) \frac{1}{\sigma_x}\phi\left(\epsilon\right) \sigma_x d\epsilon\\
    &= \int_{-\infty}^u \sigma_x \cdot (u-\epsilon) \phi(\epsilon) d\epsilon\\
    &=  \sigma_x \cdot \left( u\cdot \int_{-\infty}^u \phi(\epsilon) d\epsilon +\int_{-\infty}^u (-\epsilon)  \phi(\epsilon) d\epsilon \right) \\
    &= \sigma_x [u\Phi(u)+ \phi(u)]
\end{align*}

where $u:=\frac{y_{\min}-\mu_x}{\sigma_x}$. 

\begin{note2}[Derivation details]
    To understand the identity $\phi(u) = \int_{-\infty}^u
    (-\epsilon)  \phi(\epsilon) d\epsilon$ used in the last equality, we first see that the antiderivative
is $\phi(\epsilon) = \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-\epsilon^2}{2}\right)$,
\begin{align*}
    \frac{d}{d \epsilon} \phi(\epsilon) =  \frac{1}{\sqrt{2\pi}}\frac{d}{d \epsilon}  \exp\left(\frac{-\epsilon^2}{2}\right) 
    =  \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-\epsilon^2}{2}\right)(-\epsilon)
    = -\epsilon \phi(\epsilon)
\end{align*}
and evaluating the rieman integral is equivalent to evaluate the antiderivative in its boundaries, giving the 
solution, 
$$\int_{-\infty}^u
(-\epsilon)  \phi(\epsilon) d\epsilon = \left[\phi(\epsilon)\right]_{-\infty}^u = \phi(u)-0 = \phi(u)$$ 
\end{note2}

We can also explicily write the expected improvement as, 
$$EI(x) = (y_{\min}-\mu_x)\Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)+ \sigma_x
\phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)$$ where the first part can be interpretted as
exploitation (favouring points with a large average improvement $I(x) := (y_{\min}-\mu_x)$) and the second
part can be seen as exploitation (favouring points with high uncertainty.). This can also be seen
in Figure \eqref{EI_illustration}, where it is clear that the expected improvement is growing for
increasing average improvement $I(x)$ and also for increasing prediction uncertainty $\sigma_x$.

% taking the derivative with
% respect to $I(x) := (y_{\min}-\mu_x)$ and $\sigma_x$, we see that expected improvement is is
% increasing if the improvement grows or if the variance $\sigma_x$ grows, i.e
% $$\frac{\partial EI(x)}{\partial I(x)} = \Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) > 0, \hspace*{0.5cm} 
% \frac{\partial EI(x)}{\partial \sigma_x} = \phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) >0$$ 
% <obs mistake in the book!!!?>

\subsubsection{Approximate expected improvement}
If the predictive distribution is non-Gaussian, it is either possible to approximate it as a Gaussian
(By using the mean and variance of the predictive distribution to define the Gaussian approximation)
or calculate the expected improvement approximately as follows, 
\begin{align}
    E_{p(y|x,\mathcal{D})}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) p(y|x,\mathcal{D}) dy\\
    &\approx \frac{1}{K} \sum_{k=1}^K  \max(0,y_{\min}-y^{(k)}) \label{aEI}
\end{align}
where $y^{(k)}$ are samples from the predictive distribution.

% In the case of a parametric model
% like a Bayesian NN, then we already got posterior samples from posterior $\theta^{(k)} \sim p(\theta | \mathcal{D})$, giving, 
% $$p(y|x,\mathcal{D}) \approx \frac{1}{K_2} \sum_{k=1}^{K_2} p(y|x,\theta^{(k)})$$ where
% $p(y|x,\mathcal{D} = \mathcal{N}(y|NN_w,\sigma))$. So essentially we should sample from a sampled
% distribution, but instead the PhD thesis \cite{PhDthesis}, just sample the mean and set $K_2 = 1$... 


% \begin{align*}
%     \mathbb{E}_{y_*|\textbf{x}_*,D_n}[\max(0,y_{\min}-y_*)] &= ??\\
%     \mathbb{E}[\min(0,y_{\min}-y_*)|\textbf{x}_*,D_n] &= \int_{-\infty}^\infty \min(0,y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &= \int_{-\infty}^{y_{\min}} (y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &\approx \frac{1}{N} \sum_{\theta \in \Omega } [y_{\min}-f_\theta(x)]
% \end{align*}

% where $\Omega = \{\theta|f_{\theta}(x)< y_{\min}\}$

%\section{uncertainties}
%Alatoric vs epistemic uncertainties 
\subsection{Other acquisition functions}
Expected improvement is just one choice of acquisition function, we now shortly present three different acquisition functions, 
Lower confidence bound, entropy search (mutual information acquisition) and probability of improvement. As mentioned, we
only use expected improvement in the experiments of this thesis.

\subsubsection{Lower confidence bound}
Lower confidence bound acquisition function \cite[145]{bayesoptbook}\footnote{
\cite[145]{bayesoptbook} deals with an maximization problem, and an upper confidence bound
acquisition function is presented, however, this formulation is equivalent.} is parameterised by a
confidence parameter $\pi \in [0,1]$ which defines the preditive $(1-\pi)$-quantile at $x$,
 $$q_{1-\pi}(x) = \inf \{y^*|\mathbb{P}(y\leq y^* | x, \mathcal{D}) \geq (1-\pi) \},$$
i.e. the prediction $y \sim p(y|x, \mathcal{D})$ will only be be less than the lower bound $q_{1-\pi}(x) $
with a tunable probability of $1-\pi$. 
The acquisition function is simply defined as the negative lower quantile
$$LCB_{\pi}(x) = -q_{1-\pi}(x).$$ It is negative since we want to find the next location which
maximizes the acquisition function. Choosing a confidence level close to 1, i.e. $\pi \approx 1$ yields
exploration, as seen in figure \ref{Different_AQ_functions}. For a Gaussian predictive distribution
this the lower confidence bound is simply given as, 
$$LCB(x) = - (\mu_x - \beta \sigma_x)$$
where $\beta = \Phi^{-1}(\pi)$. 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{Pictures/neg_lower_confidence_illustration.pdf}
%     \caption{<Soon similar plot as Figure \ref{EI_illustration}> Illustration of the values of the
%     negative lower confidence bound for different predictive uncertainties $\sigma_x =
%     \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})}[y]}$ versus the average improvement $y_{\min}-\mu_x$,
%     where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$}
%     \label{nLCB_illustration}
% \end{figure}

\subsubsection{Entropy search}
Optimization policies known as policy search utilize information theory to select the next point
which will provide the most information (i.e useful knowledge) about the objective function. 
More specifically their acquisition function is \textit{mutual information} \cite[135-140]{bayesoptbook}, 
\begin{equation}\label{mutualinfo}
    I(x,y) = \int \int p(x,y) \log \frac{p(x,y)}{p(y)p(x)} dy dx,
\end{equation}
which is a measurement of dependency between the random variables. If $x$ and $y$ are independent, then
$p(x,y) = p(x)p(y)$, i.e. the fraction in \eqref{mutualinfo} becomes 1 and thereby $I(x,y) = 0$. 
More details found in \cite{PredEntropy} and \cite{entropysearch}. 

% uncertainty about the objective function over a larger volume. Expected
% improvement (gure 7.3) and probability of improvement (gure 7.9),
% on the other hand, are computed only from inspection of the marginal
% predictive distribution ð‘(ð‘¦ | ð‘¥, D). As a result, they cannot dierentiate
% observation locations based on their global impact on our belief.


% The reasoning underlying entropy search policies is somewhat different from and more general than the other acquisition functions we
% have considered thus far, all of which ultimately focus on maximizing
% the posterior mean function. Although this is a pragmatic concern, it is
% information-theoretic decision making as a intimately linked to optimization. Information-theoretic experimental
% model of the scientic method design is instead motivated by an abstract pursuit of knowledge, and may
% be interpreted as a mathematical formulation of the scientic method.

%One-step lookahead with (either) information gain yields an acquisi function known as mutual
%information



\subsubsection{Probability of improvement}
Probability of improvment acquisitionfunction is defined as follows, 
$$PI(x) = \mathbb{P}{p(y|x,\mathcal{D})}(\max(0,y_{\min}-y)>0) =
\mathbb{P}{p(y|x,\mathcal{D})}(y<y_{\min})$$ i.e. the probability of the prediction is an actually
improvement. It does not take the magnitude of the improvement into consideration (as Expected
improvement). It rather just if there is an improvement or not. In the case of a Gaussian predictive
probability it is given on closed form
$$PI(x) = \PhiÂ·\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)$$
where $p(y|x\mathcal{D}) =\mathcal{N}(y|\mu_x,\sigma_x^2) =\mu_x + \sigma_x\cdot\mathcal{N}(y|0,1)$

\section{Summary}
In this chapter, we introduced Bayesian optimization and found its relevance among the large number
of optimization algorithms: It is a leading methodology for sample-efficient optimization. Then we
dived into the details of a surrogate model in a BO setting. This was nothing else than a cheaply
evaluated Bayesian regression model. Finally, we introduced the second component of BO, i.e. an
acquisition function. We derived the need closed-form of expected improvement, and shortly presented
alternative acquisition fucntion. In the following chapter, we will introduce discriminative
surrogate models. 


% We then discuss the knowledge
% gradient (Section 4.2), entropy search, and predictive entropy search (Section 4.3) acquisition functions.
% These alternate acquisition functions are most useful in exotic problems where an assumption made by
% expected improvement that the primary benefit of sampling occurs through an improvement at the point
% sampled is no longer true.


% The entropy search (ES) (Hennig and Schuler, 2012) acquisition function values the information we have
% about the location of the global maximum according to its differential entropy

% ES seeks the point to evaluate what causes the largest decrease in differential entropy

% (Recall from, e.g., Cover and Thomas (2012),
% that the differential entropy of a continuous probability distribution p(x) is R
% p(x) log(p(x)) dx, and that
% smaller differential entropy indicates less uncertainty.)

% Predictive entropy search (PES) (HernÂ´andezLobato et al., 2014) seeks the same point but uses a reformulation of the entropy reduction objective
% based on mutual information. Exact calculations of PES and ES would give equivalent acquisition functions, but exact calculation is not typically possible, and so the difference in computational techniques
% used to approximate the PES and ES acquisition functions creates practical differences in the sampling
% decisions that result from the two approaches. We first discuss ES and then PES.

% Let x  be the global optimum of f. The posterior distribution on f at time n induces a probability
% distribution for x
% . Indeed, if domain A were finite, we could represent f over its domain by a
% vector (f(x): x âˆˆ A), and x
% would correspond to the largest element in this vector. The distribution of
% this vector under the time-n posterior distribution would be multivariate normal, and this multivariate
% normal distribution would imply the distribution of x

% . When A is continuous, the same ideas apply,
% where x

% is a random variable whose distribution is implied by the Gaussian process posterior on f




% With kriging, we can develop search methods that put some emphasis on sampling where the standard
% error is high. In this way, we obtain the desired feature of â€˜paying attention to parts of the space
% that have been relatively unexplored.â€™