\chapter{Bayesian Optimization}
This chapter will introduce Bayesian optimization. We start with a general introduction to 
the concept of optimization. 


\input{Chapters/01_optimization.tex}

% <Cope with inacuracies> i.e. allows for stochastic objective function. 
% <Uncertainty measure with prediction based on simple and clear prior 
% assumptions about the characteristic about the objective function. >
% <Provides an adequate termination condition for the opt. process>. 

% <kilde 151. Bayes Opt is assumed superior to other global optimization technics 
% with limited budget>

\section{Bayesian regression}

Whereas traditional regression workflow is the following: From data, fit model parameters, make
predictions using the parameters. The Bayesian framework allows us to skip the dependency of a
single set of parameters and instead use all possible parameters by treating the set of parameters
as a random quantity, $\Theta$, where some values/realizations of $\Theta$ are more probable than
others. What is of interest is the predictive posterior distribution,  
\begin{align}\label{Predictive2}
    p(y|x, \mathcal{D}) &= \int p(y,\theta|x, \mathcal{D}) d\theta\\
    &= \int p(y|x,\theta)p(\theta|\mathcal{D}) d\theta
\end{align}
where the second equation is true because of the probability chain rule and that $Y$ is fully
described by the parametric model $p(y|x,\theta)$ and the parameters $\Theta$ is fully described by
the posterior distribution $p(\theta|\mathcal{D})$.

\subsection{surrogate model}
A surrogate model in a Bayesian optimization setting is just a Bayesian regression model. The most
used surrogate model is a Gaussian Process. But there have been investigations on other types of
surrogates, such as Bayesian neural networks and Bayesian regression trees. These are all
discriminative models, and another approach, which we focus on in this project, is to model $y$ and
$x$ jointly in a so-called generative model, $p(x,y)$. A generative model can be used implicitly
as a surrogate from the conditional distribution of $y$ given $x$, $p(y|x)$.

In this thesis, the Bayesian regression models investigated as Bayesian optimization surrogates are
the following:
\begin{itemize}[noitemsep]
    \item Gaussian Process (discriminative)
    \item Bayesian Neural network (discriminative)
    \item Kernel density regression (generative)
    \item Gaussian mixture regression (generative)
    \item SPN (generative)
\end{itemize}

We now introduce the concept of inference, which is necessary for using the probabilistic surrogate models
in Bayesian Optimization. 

\subsection{Inference of surrogate models}
Inference is the process of computing answers to queries about a probabilistic model after observing
data. In Bayesian regression, the query is the predictive distribution, $p(y|x,\mathcal{D})$, as we
are interested in the distribution of $y$ given $x$ and already observed data, $\mathcal{D}$. This
often indirectly create the posterior query, $p(\theta|\mathcal{D})$, the probability of model
parameters $\theta$ given data $\mathcal{D}$. Lastly, it is also inference when we train a Gaussian
mixture model or SPN using the expectation-maximization algorithm (EM) since we are iteratively
answering the query $E_{p(z|\theta^{(k)})}[z|\theta]$.

%\subsection{Exact and approximate inference}
We distinguish between two different ways of inference, exact and approximate inference. It is
\textit{exact inference} when a probabilistic query is calculated exact. It is possible to
calculate exact inference on the predictive distribution for the Gaussian mixture model, Sum product
network, and Gaussian processes. Models which allow for exact inference have a powerful advantage
over the models with approximate inference since we can guarantee the answers to the queries are
correct; however, they are usually also less expressive.

\todo{Note: Explain what expressive mean!}

When it is not possible to answer a probabilistic query exact, we can approximate the answer using
\textit{approximate inference}. When dealing with complicated and expressive statistical models,
exact inference is often intractable, and we need to use approximate inference. Approximate
inference is a broad category of methods, which includes variational inference, Laplace
approximation, and Markov chain Monte Carlo (MCMC). The two Bayesian Neural networks we deal with in
this project Bohamiann and Numpyro BNN are similar regression models but are inferred using two
different versions of the MCMC method, Hamiltonian Monte Carlo. As revealed later (see in section
\ref{BNN}), approximate inference might indeed be flawed and inexact. 

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l}
    %\rowcolor[HTML]{C0C0C0} 
    \textbf{Model}       & \textbf{Predictive inference}    &   \textbf{Learning} \\ \hline
    GP                          & Exact $O(n^3)$            & Emperical Bayes\\
    Numpyro BNN                 & No U-Turn Sampler         & \\
    Bohamiann BNN               & Adaptive stochatic HMC    & \\
    Kernel density regression   & Exact $O(n)$              & \\
    Gaussian mixture regression & Exact $O(K)$              & EM  \\
    SPN                         & Exact $O(E)$              &  EM $O(E)$\\
    \end{tabular}
    \caption{Overview of inference methods applied on the statistical models 
            used in this project. $E$ is the number of edges in the SPN. $n$ is the number of datapoints. 
            $K \leq n$ is the number of mixture comonents. We will soon learn that for an
            SPN the number of mixture compenets is exponential larger than number of edges
            i.e. $E << K$. In theory MCMC methods samples 
            from true the posterior distribution, and do not need any fitting/learning. 
            }
\end{table}


\section{Acquisition function}
Given a well fitted predictive distribution $p(y|x\mathcal{D})$ \eqref{Predictive2}. The next sample
point is chosen
according to a so-called acquisition function, which balances out the well-known concept of
exploitation and exploration. Where exploitation will be choosing the next location, $x \in
\mathcal{X}$ according to its average improvement, and exploration will be choosing the next point
in a region of high uncertainty and thereby help lower the overall uncertainty. We will first look
at the acquisition function used in the thesis: Expected improvement. But also shortly
go over different types of acquisition functions. 

\subsection{Expected improvement}
A popular choice of acquisition function is expected improvement, 
$$EI(x) = \mathbb{E}_{p(y|x,\mathcal{D})}[\max(0, y_{\min}-y)]$$ where we only consider the values
$y$, which improves the current best value in the expectation of the predictive distribution,
$p(y|x,\mathcal{D})$. Therefore $x$ which yield a bad predictive mean value
$\mathbb{E}_{p(y|x,\mathcal{D})}[y]> y_{\min}$ might still be maximizing the expectated improvement,
if its uncertainty is very large. Figure \ref{EI_illustration} illustrates that a large uncertainty
in the predictive distribution could lead to relative large values even for non-improving mean
predictions.

\begin{figure}%[H]
    \centering
    \includegraphics[width=\textwidth]{Pictures/expected_improvement_illustration.pdf}
    \caption{When then predictive distribution is Gaussian, expected improvement becomes closed
    form. The figure illustrates the values of expected improvement for different predictive
    uncertainties $\sigma_x = \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})}[y]}$ versus the average
    improvement $y_{\min}-\mu_x$, where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$}
    \label{EI_illustration}
\end{figure}


\begin{note}
    \textbf{Why defining expected improvment with max}    
    Note that $\max(0,\cdot)$ is important since the Bayesian optimization othervise reduces to
    a simple non-probabilistic surrogate-based optimization method,
    \begin{align*}
        \mathbb{E}_{p(y|x,\mathcal{D})}[y_{\min}-y] = y_{\min} - \mathbb{E}_{p(y|x,\mathcal{D})}[y]
    \end{align*}
    i.e. maximizing the above is equivalent to maximizing the predictive mean, and thereby we loose
    all the valuable information about the predictive uncertainties from the Bayesian regression model. 
\end{note}

\subsubsection{Exact expected improvement} \label{ExactEI} 
In the following derivation we assume the
predictive distribution can be approxiamted by a normal distribution dependent on the point of
interest $x$ and the data $\mathcal{D}$ (note for the GP it is in fact not an approximation), 
$$p(y|x,\mathcal{D}) \approx \mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$$ where we
will change to a less clumpsy notation $\mathcal{N}(y|\mu_x,
\sigma^2_x):=\mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$. This is completely fine
since we $x$ is fixed (and $\mathcal{D}$ is fixed) when evaluating the expected improvement in a point
$x$. %  $\sigma_x := \sigma^2(x,\mathcal{D})$ and $\mu_x := \mu(x,\mathcal{D})$
%as evaluated functions, i.e. numbers. 
Furthermore, the density of
a standard normal distribution is denoted $\phi(\cdot):=\mathcal{N}(\cdot | 0,1)$, and the cumlative
density function (CDF) of a standard normal distribution is denoted, $\Phi(\cdot) :=
\int_{-\infty}^{\cdot} \phi(\epsilon)d\epsilon$. We will now see that the normal approximation
of the predictive distribution yiels closed form solution to the expected improvement function, 

\begin{align*}
    E_{p(y|x,\mathcal{D})}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) p(y|x,\mathcal{D}) dy\\
    &\approx \int \max(0,y_{\min}-y) \mathcal{N}(y|\mu_x, \sigma_x^2) dy\\
    &= \int_{-\infty}^{y_{\min}} (y_{\min}-y) \frac{1}{\sigma_x}\phi\left(\frac{y-\mu_x}{\sigma_x}\right) dy\\
    &= \int_{-\infty}^{\frac{y_{\min}-\mu_x}{\sigma_x}} (y_{\min}-\mu_x-\sigma_x\epsilon) \frac{1}{\sigma_x}\phi\left(\epsilon\right) \sigma_x d\epsilon\\
    &= \int_{-\infty}^u \sigma_x \cdot (u-\epsilon) \phi(\epsilon) d\epsilon\\
    &=  \sigma_x \cdot \left( u\cdot \int_{-\infty}^u \phi(\epsilon) d\epsilon +\int_{-\infty}^u (-\epsilon)  \phi(\epsilon) d\epsilon \right) \\
    &= \sigma_x [u\Phi(u)+ \phi(u)]
\end{align*}

where $u:=\frac{y_{\min}-\mu_x}{\sigma_x}$. 

\begin{note}
    \textbf{Derivation details}
    To understand the identity $\phi(u) = \int_{-\infty}^u
    (-\epsilon)  \phi(\epsilon) d\epsilon$ used in the last equality, we first see that the antiderivative
is $\phi(\epsilon) = \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-\epsilon^2}{2}\right)$,
\begin{align*}
    \frac{d}{d \epsilon} \phi(\epsilon) =  \frac{1}{\sqrt{2\pi}}\frac{d}{d \epsilon}  \exp\left(\frac{-\epsilon^2}{2}\right) 
    =  \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-\epsilon^2}{2}\right)(-\epsilon)
    = -\epsilon \phi(\epsilon)
\end{align*}
and evaluating the rieman integral is equivalent to evaluate the antiderivative in its boundaries, giving the 
solution, 
$$\int_{-\infty}^u
(-\epsilon)  \phi(\epsilon) d\epsilon = \left[\phi(\epsilon)\right]_{-\infty}^u = \phi(u)-0 = \phi(u)$$ 
\end{note}

We can also explicily write the expected improvement as, 
$$EI(x) = (y_{\min}-\mu_x)\Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)+ \sigma_x
\phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)$$ where the first part can be interpretted as
exploitation (favouring points with a large average improvement $I(x) := (y_{\min}-\mu_x)$) and the second
part can be seen a exploitation (favouring points with high uncertainty.). This can also be seen
in Figure \eqref{EI_illustration}, where it is clear that the expected improvement is growing for
growing average improvement $I(x)$ and also for growing prediction uncertainty $\sigma_x$.


% taking the derivative with
% respect to $I(x) := (y_{\min}-\mu_x)$ and $\sigma_x$, we see that expected improvement is is
% increasing if the improvement grows or if the variance $\sigma_x$ grows, i.e
% $$\frac{\partial EI(x)}{\partial I(x)} = \Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) > 0, \hspace*{0.5cm} 
% \frac{\partial EI(x)}{\partial \sigma_x} = \phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) >0$$ 
% <obs mistake in the book!!!?>
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Pictures/4_methods_EI_000.pdf}
    \caption{The same points as fig .. where the acquisition function is the
    expected improvement. }
\end{figure}

\subsubsection{Approximate expected improvement}
If the predictive distribution is non-Gaussian, it is either possible to approximate it as a Gaussian
(By using the mean and variance of the predictive distribution to define the Gaussian approximation)
or calculate the expected improvement approximately as follows, 
\begin{align*}
    E_{p(y|x,\mathcal{D})}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) p(y|x,\mathcal{D}) dy\\
    &\approx \frac{1}{K} \sum_{k=1}^K  \max(0,y_{\min}-y^{(k)})
\end{align*}
where $y^{(k)}$ are samples from the predictive distribution.

% In the case of a parametric model
% like a Bayesian NN, then we already got posterior samples from posterior $\theta^{(k)} \sim p(\theta | \mathcal{D})$, giving, 
% $$p(y|x,\mathcal{D}) \approx \frac{1}{K_2} \sum_{k=1}^{K_2} p(y|x,\theta^{(k)})$$ where
% $p(y|x,\mathcal{D} = \mathcal{N}(y|NN_w,\sigma))$. So essentially we should sample from a sampled
% distribution, but instead the PhD thesis \cite{PhDthesis}, just sample the mean and set $K_2 = 1$... 


% \begin{align*}
%     \mathbb{E}_{y_*|\textbf{x}_*,D_n}[\max(0,y_{\min}-y_*)] &= ??\\
%     \mathbb{E}[\min(0,y_{\min}-y_*)|\textbf{x}_*,D_n] &= \int_{-\infty}^\infty \min(0,y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &= \int_{-\infty}^{y_{\min}} (y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &\approx \frac{1}{N} \sum_{\theta \in \Omega } [y_{\min}-f_\theta(x)]
% \end{align*}

% where $\Omega = \{\theta|f_{\theta}(x)< y_{\min}\}$

%\section{uncertainties}
%Alatoric vs epistemic uncertainties 

\subsection{Lower confidense bound}
Lower confidence bound is defined by a confidence paramter $\pi \in [0,1]$
and then the lower quantile $\int_{\infty}^x p(y|x,\mathcal{D})dy = (1-\pi)$
is chosen as the acquisition. For a Gaussian predictive distribution this is 
simply 
$$nLCB(x) = - (\mu_x - \beta \sigma_x)$$
where $\beta = \Phi^{-1}(1-\pi)$.  
\begin{figure}%[H]
    \centering
    \includegraphics[width=\textwidth]{Pictures/neg_lower_confidence_illustration.pdf}
    \caption{Illustration of the values of the negative lower confidence bound for different predictive
    uncertainties $\sigma_x = \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})}[y]}$ versus the average
    improvement $y_{\min}-\mu_x$, where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$}
    \label{nLCB_illustration}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Pictures/4_methods_LCB_000.pdf}
    \caption{The same points as fig .. where the acquisition function is the
    negative lower confidence bound. }
\end{figure}


\subsection{Entropy search}
% We then discuss the knowledge
% gradient (Section 4.2), entropy search, and predictive entropy search (Section 4.3) acquisition functions.
% These alternate acquisition functions are most useful in exotic problems where an assumption made by
% expected improvement that the primary benefit of sampling occurs through an improvement at the point
% sampled is no longer true.


% The entropy search (ES) (Hennig and Schuler, 2012) acquisition function values the information we have
% about the location of the global maximum according to its differential entropy

% ES seeks the point to evaluate what causes the largest decrease in differential entropy

% (Recall from, e.g., Cover and Thomas (2012),
% that the differential entropy of a continuous probability distribution p(x) is R
% p(x) log(p(x)) dx, and that
% smaller differential entropy indicates less uncertainty.)

% Predictive entropy search (PES) (Hern´andezLobato et al., 2014) seeks the same point but uses a reformulation of the entropy reduction objective
% based on mutual information. Exact calculations of PES and ES would give equivalent acquisition functions, but exact calculation is not typically possible, and so the difference in computational techniques
% used to approximate the PES and ES acquisition functions creates practical differences in the sampling
% decisions that result from the two approaches. We first discuss ES and then PES.

% Let x  be the global optimum of f. The posterior distribution on f at time n induces a probability
% distribution for x
% . Indeed, if domain A were finite, we could represent f over its domain by a
% vector (f(x): x ∈ A), and x
% would correspond to the largest element in this vector. The distribution of
% this vector under the time-n posterior distribution would be multivariate normal, and this multivariate
% normal distribution would imply the distribution of x

% . When A is continuous, the same ideas apply,
% where x

% is a random variable whose distribution is implied by the Gaussian process posterior on f



