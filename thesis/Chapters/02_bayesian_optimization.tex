\chapter{Bayesian Optimization}
This chapter will introduce Bayesian optimization. We start with a general introduction to the
concept of optimization (mainly based on \cite{bayesoptbook}), which culminates with the
introduction of the idea of Bayesian optimization (BO). Next, we dive into the first BO component:
The Bayesian regression methodology. Finally, the concept of an acquisition function is introduced,
with a focus on expected improvement and a brief description of the other types. 

\input{Chapters/01_optimization.tex}

% <Cope with inacuracies> i.e. allows for stochastic objective function. 
% <Uncertainty measure with prediction based on simple and clear prior 
% assumptions about the characteristic about the objective function. >
% <Provides an adequate termination condition for the opt. process>. 

% <kilde 151. Bayes Opt is assumed superior to other global optimization technics 
% with limited budget>

\section{Bayesian regression}
% \begin{testexample2}[Bayesian methodology]
%     In Bayesian statistics $p(\theta|\mathcal{D})$ is the posterior distribution, and it is 
% of $\theta$ is linked to the likelihood and prior via Bayes rule,
% $$p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})}$$
% \end{testexample2}
Whereas traditional regression workflow is the following: Given data, choose the best fitting model
parameters, make predictions using those parameters. The Bayesian framework allows us to skip the
dependency of a single set of parameters and instead use \textit{all possible} parameters by treating the set
of parameters as a random quantity, $\theta \sim p(\theta|\mathcal{D})$, where some values/realizations of $\theta$ are more
probable than others given data. In Bayesian regression we are interest is the predictive posterior distribution,  
\begin{align}\label{Predictive2}
    p(y|x, \mathcal{D}) &= \int p(y,\theta|x, \mathcal{D}) d\theta\\
    &= \int p(y|x,\theta)p(\theta|\mathcal{D}) d\theta,
\end{align}
where the posterior $p(\theta|\mathcal{D})$ gives weighting to the proposed regression model
$p(y|x,\theta)$. Note that the second equation is true because of the probability chain rule and
that $y$ is fully described by the parametric model $p(y|x,\theta)$ and the parameters $\theta$ are
fully described by the posterior distribution $p(\theta|\mathcal{D})$.


\subsection{Surrogate model}
A surrogate model in a Bayesian optimization setting is a Bayesian regression model. The most
used surrogate model is a Gaussian Process. But there have been investigations on other 
surrogates, such as Bayesian neural networks and Bayesian regression trees. These are all
discriminative models, and another approach we focus on in this project is to model $y$ and
$x$ jointly in a so-called generative model, $p(x,y)$. A generative model can be used implicitly
as a surrogate from the conditional distribution of $y$ given $x$, $p(y|x)$.

In this thesis, the Bayesian regression models investigated as Bayesian optimization surrogates are
the following:
\begin{itemize}[noitemsep]
    \item Gaussian process (GP)
    \item Bayesian neural network (BNN)
    \item Kernel density regression (KDE)
    \item Gaussian mixture regression (GMR)
    \item Sum-product networks (SPN)
\end{itemize}

We now introduce the concept of inference, which is necessary for using the probabilistic surrogate models
in Bayesian Optimization. 

\subsection{Inference of surrogate models}
Inference is the process of computing answers to queries about a probabilistic model after observing
data. In Bayesian regression, the query is the predictive distribution, $p(y|x,\mathcal{D})$, as we
are interested in the distribution of $y$ given $x$ and already observed data, $\mathcal{D}$. This
often indirectly create the posterior query, $p(\theta|\mathcal{D})$, the probability of model
parameters $\theta$ given data $\mathcal{D}$. Lastly, it is also inference when we train a Gaussian
mixture model or SPN using the expectation-maximization algorithm (EM) since we are iteratively
answering the query $E_{p(z|\theta^{(k)})}[z|\theta]$.

%\subsection{Exact and approximate inference}
We distinguish between two different ways of inference: Exact and approximate inference. It is
\textit{exact inference} when a probabilistic query is calculated exact. It is possible to
calculate exact inference on the predictive distribution for the Gaussian mixture model, Sum product
network, and Gaussian processes. Models which allow for exact inference have a powerful advantage
over the models with approximate inference since we can guarantee the answers to the queries are
correct; however, they are usually also less expressive (unable to explain complicated models). 

When it is not possible to answer a probabilistic query exact, we can approximate the answer using
\textit{approximate inference}. When dealing with complicated and expressive statistical models,
exact inference is often intractable, and we need to use approximate inference. Approximate
inference is a broad category of methods, which includes variational inference and Markov chain
Monte Carlo (MCMC). The two Bayesian Neural networks, we deal with in this project, Bohamiann and
Numpyro BNN are similar regression models but are inferred using two different MCMC methods,
Hamiltonian Monte Carlo - giving different results. As revealed later (see in section \ref{BNN})
approximate inference might be flawed and inexact. 

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|l}
    %\rowcolor[HTML]{C0C0C0} 
    \textbf{Model}       & \textbf{Predictive inference}    &   \textbf{Learning} \\ \hline
    GP                          & Exact $O(n^3)$            & Emperical Bayes\\
    Numpyro BNN                 & No U-Turn Sampler         & \\
    Bohamiann BNN               & Adaptive stochatic HMC    & \\
    Kernel density regression   & Exact $O(n)$              & \\
    Gaussian mixture regression & Exact $O(K)$              & EM  \\
    SPN                         & Exact $O(E)$              &  EM $O(E)$\\
    \end{tabular}
    \caption{Overview of inference methods applied on the statistical models used in this project.
            $E$ is the number of edges in the SPN. $n$ is the number of data points. $K \leq n$ is
            the number of mixture components. We will soon learn that for an SPN the number of
            mixture components is exponentially larger than the number of edges i.e. $E << K$. In theory
            MCMC methods samples from true the posterior distribution, and do not need any
            fitting/learning. }
\end{table}

\newpage
\section{Acquisition function}
Given a correct predictive distribution $p(y|x\mathcal{D})$ the next step in Bayesian optimization
is to select the next location $x \in \mathcal{X}$ to sample from. The next location is chosen
according to a so-called acquisition function (AQ function), which balances out the well-known
concept of exploitation and exploration. It is exploitation if the next chosen location according to
its average improvement. It is exploration if the next point is chosen in a region of high
uncertainty and thereby helps lower the overall uncertainty. First, we will look at the acquisition
function used in the thesis: Expected improvement. Secondly, we shortly present other different
types of acquisition functions. In Figure \ref{Different_AQ_functions} we see three different
acquisition functions. The expected improvement (EI) is known for being biased towards exploitation.
In contrast, the negative lower confidence bound (LCB) has a parameter $\beta$, which can be tuned
to make it focus more on exploration. 
\begin{figure}[H]
    \centering
    \includegraphics[trim=1cm 0cm 1cm 1cm,clip,width=\textwidth]{Pictures/illustration_AQs.pdf}
    \caption{The same regression model and points as Figure \ref{BO_example}, but with three
    different acquisition functions: Expected improvement and negative lower confidence bound for
    two different lower quantiles $0.841$ and $0.999$. The latter yields more exploration.}\label{Different_AQ_functions}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/expected_improvement_illustration.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/neg_lower_confidence_illustration_1.pdf}
      \end{minipage}
     \hfill
     \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=0.2cm 0.2cm 0cm 0.1cm,clip,width=\textwidth]{Pictures/neg_lower_confidence_illustration_3.pdf}
      \end{minipage}
    \caption{Contourplot of expected improvement and lower confidence bound (for two different
    quantiles) for different (Gaussian) predictive uncertainties $\sigma_x =
    \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})y]}}$ versus the average improvement $y_{\min}-\mu_x$,
    where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$ (high values are dark). The colored lines are the
    mapping $x \mapsto (\sigma_x, y_{\min}-\mu_x)$ for $x = [-100, 100]$ for the Bayesian regression
    function in Figure \ref{Different_AQ_functions} - and thereby explains how the acquisition
    functions balances exploitation and exploration. The orange dot represent the point maximizing
    the acquisition function}
    \label{EI_illustration}
\end{figure}


\subsection{Expected improvement}
A popular choice of acquisition function is expected improvement, 
$$EI(x) = \mathbb{E}_{p(y|x,\mathcal{D})}[\max(0, y_{\min}-y)]$$ where we only consider the values
$y$, which improves the current best value in the expectation of the predictive distribution,
$p(y|x,\mathcal{D})$. Therefore, a $x$ which yield a bad predictive mean value
$\mathbb{E}_{p(y|x,\mathcal{D})}[y]> y_{\min}$ might still be maximizing the expectated improvement,
if the predictive uncertainty is very large at $x$. Figure \ref{EI_illustration} illustrates that a
large uncertainty in the predictive distribution (represented as the predictive variance) can lead
to relative large values even for non-improving mean predictions.


\begin{note2}[Why defining expected improvment with max]    
    Note that $\max(0,\cdot)$ is important since the Bayesian optimization othervise reduces to
    a simple non-probabilistic surrogate-based optimization method,
    \begin{align*}
        \mathbb{E}_{p(y|x,\mathcal{D})}[y_{\min}-y] = y_{\min} - \mathbb{E}_{p(y|x,\mathcal{D})}[y]
    \end{align*}
    i.e. maximizing the above is equivalent to maximizing the predictive mean, and thereby we loose
    all the valuable information about the predictive uncertainties from the Bayesian regression model. 
\end{note2}

\subsubsection{Exact expected improvement} \label{ExactEI} 
In the following derivation we assume the
predictive distribution can be approxiamted by a normal distribution dependent on the point of
interest $x$ and the data $\mathcal{D}$ (note for the GP it is in fact not an approximation), 
$$p(y|x,\mathcal{D}) \approx \mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$$ where we
will change to a less clumpsy notation $\mathcal{N}(y|\mu_x,
\sigma^2_x):=\mathcal{N}(y|\mu(x,\mathcal{D}), \sigma^2(x,\mathcal{D}))$. This is completely fine
since we $x$ is fixed (and $\mathcal{D}$ is fixed) when evaluating the expected improvement in a point
$x$. %  $\sigma_x := \sigma^2(x,\mathcal{D})$ and $\mu_x := \mu(x,\mathcal{D})$
%as evaluated functions, i.e. numbers. 
Furthermore, the density of
a standard normal distribution is denoted $\phi(\cdot):=\mathcal{N}(\cdot | 0,1)$, and the cumlative
density function (CDF) of a standard normal distribution is denoted, $\Phi(\cdot) :=
\int_{-\infty}^{\cdot} \phi(\epsilon)d\epsilon$. We will now see that the normal approximation
of the predictive distribution yiels closed form solution to the expected improvement function, 

\begin{align*}
    E_{\mathcal{N}(y|\mu_x, \sigma_x^2)}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) \mathcal{N}(y|\mu_x, \sigma_x^2) dy\\
    &= \int_{-\infty}^{y_{\min}} (y_{\min}-y) \frac{1}{\sigma_x}\phi\left(\frac{y-\mu_x}{\sigma_x}\right) dy\\
    &= \int_{-\infty}^{\frac{y_{\min}-\mu_x}{\sigma_x}} (y_{\min}-\mu_x-\sigma_x\epsilon) \frac{1}{\sigma_x}\phi\left(\epsilon\right) \sigma_x d\epsilon\\
    &= \int_{-\infty}^u \sigma_x \cdot (u-\epsilon) \phi(\epsilon) d\epsilon\\
    &=  \sigma_x \cdot \left( u\cdot \int_{-\infty}^u \phi(\epsilon) d\epsilon +\int_{-\infty}^u (-\epsilon)  \phi(\epsilon) d\epsilon \right) \\
    &= \sigma_x [u\Phi(u)+ \phi(u)]
\end{align*}

where $u:=\frac{y_{\min}-\mu_x}{\sigma_x}$. 

\begin{note2}[Derivation details]
    To understand the identity $\phi(u) = \int_{-\infty}^u
    (-\epsilon)  \phi(\epsilon) d\epsilon$ used in the last equality, we first see that the antiderivative
is $\phi(\epsilon) = \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-\epsilon^2}{2}\right)$,
\begin{align*}
    \frac{d}{d \epsilon} \phi(\epsilon) =  \frac{1}{\sqrt{2\pi}}\frac{d}{d \epsilon}  \exp\left(\frac{-\epsilon^2}{2}\right) 
    =  \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-\epsilon^2}{2}\right)(-\epsilon)
    = -\epsilon \phi(\epsilon)
\end{align*}
and evaluating the rieman integral is equivalent to evaluate the antiderivative in its boundaries, giving the 
solution, 
$$\int_{-\infty}^u
(-\epsilon)  \phi(\epsilon) d\epsilon = \left[\phi(\epsilon)\right]_{-\infty}^u = \phi(u)-0 = \phi(u)$$ 
\end{note2}

We can also explicily write the expected improvement as, 
$$EI(x) = (y_{\min}-\mu_x)\Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)+ \sigma_x
\phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)$$ where the first part can be interpretted as
exploitation (favouring points with a large average improvement $I(x) := (y_{\min}-\mu_x)$) and the second
part can be seen a exploitation (favouring points with high uncertainty.). This can also be seen
in Figure \eqref{EI_illustration}, where it is clear that the expected improvement is growing for
growing average improvement $I(x)$ and also for growing prediction uncertainty $\sigma_x$.


% taking the derivative with
% respect to $I(x) := (y_{\min}-\mu_x)$ and $\sigma_x$, we see that expected improvement is is
% increasing if the improvement grows or if the variance $\sigma_x$ grows, i.e
% $$\frac{\partial EI(x)}{\partial I(x)} = \Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) > 0, \hspace*{0.5cm} 
% \frac{\partial EI(x)}{\partial \sigma_x} = \phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) >0$$ 
% <obs mistake in the book!!!?>

\subsubsection{Approximate expected improvement}
If the predictive distribution is non-Gaussian, it is either possible to approximate it as a Gaussian
(By using the mean and variance of the predictive distribution to define the Gaussian approximation)
or calculate the expected improvement approximately as follows, 
\begin{align*}
    E_{p(y|x,\mathcal{D})}[\max(0,y_{\min}-y)] &= \int \max(0,y_{\min}-y) p(y|x,\mathcal{D}) dy\\
    &\approx \frac{1}{K} \sum_{k=1}^K  \max(0,y_{\min}-y^{(k)})
\end{align*}
where $y^{(k)}$ are samples from the predictive distribution.

% In the case of a parametric model
% like a Bayesian NN, then we already got posterior samples from posterior $\theta^{(k)} \sim p(\theta | \mathcal{D})$, giving, 
% $$p(y|x,\mathcal{D}) \approx \frac{1}{K_2} \sum_{k=1}^{K_2} p(y|x,\theta^{(k)})$$ where
% $p(y|x,\mathcal{D} = \mathcal{N}(y|NN_w,\sigma))$. So essentially we should sample from a sampled
% distribution, but instead the PhD thesis \cite{PhDthesis}, just sample the mean and set $K_2 = 1$... 


% \begin{align*}
%     \mathbb{E}_{y_*|\textbf{x}_*,D_n}[\max(0,y_{\min}-y_*)] &= ??\\
%     \mathbb{E}[\min(0,y_{\min}-y_*)|\textbf{x}_*,D_n] &= \int_{-\infty}^\infty \min(0,y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &= \int_{-\infty}^{y_{\min}} (y_{\min}-y_*) p(y_*|\textbf{x}_*,D_n) dy_*\\
%     &\approx \frac{1}{N} \sum_{\theta \in \Omega } [y_{\min}-f_\theta(x)]
% \end{align*}

% where $\Omega = \{\theta|f_{\theta}(x)< y_{\min}\}$

%\section{uncertainties}
%Alatoric vs epistemic uncertainties 

\subsection{Lower confidense bound}
Lower confidence bound is defined by a confidence paramter $\pi \in [0,1]$
and then the lower quantile $\int_{\infty}^x p(y|x,\mathcal{D})dy = (1-\pi)$
is chosen as the acquisition. For a Gaussian predictive distribution this is 
simply  <more..>
$$nLCB(x) = - (\mu_x - \beta \sigma_x)$$
where $\beta = \Phi^{-1}(1-\pi)$.  
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{Pictures/neg_lower_confidence_illustration.pdf}
%     \caption{<Soon similar plot as Figure \ref{EI_illustration}> Illustration of the values of the
%     negative lower confidence bound for different predictive uncertainties $\sigma_x =
%     \sqrt{\mathbb{V}ar_{p(y|x,\mathcal{D})}[y]}$ versus the average improvement $y_{\min}-\mu_x$,
%     where $\mu_x = E_{p(y|x,\mathcal{D})}[y]$}
%     \label{nLCB_illustration}
% \end{figure}

\subsection{Entropy search}

<copy> An information theoretic approach
which accounts for the overall information gain on the optimizer ob-
tained from a new evaluation has also been presented. This is known
as informational approach to global optimization (IAGO), and uses en-
tropy as a measure for information (Villemonteix, Vazquez, and Walter,
2009).

\subsection{probability of improvement}


% We then discuss the knowledge
% gradient (Section 4.2), entropy search, and predictive entropy search (Section 4.3) acquisition functions.
% These alternate acquisition functions are most useful in exotic problems where an assumption made by
% expected improvement that the primary benefit of sampling occurs through an improvement at the point
% sampled is no longer true.


% The entropy search (ES) (Hennig and Schuler, 2012) acquisition function values the information we have
% about the location of the global maximum according to its differential entropy

% ES seeks the point to evaluate what causes the largest decrease in differential entropy

% (Recall from, e.g., Cover and Thomas (2012),
% that the differential entropy of a continuous probability distribution p(x) is R
% p(x) log(p(x)) dx, and that
% smaller differential entropy indicates less uncertainty.)

% Predictive entropy search (PES) (Hern´andezLobato et al., 2014) seeks the same point but uses a reformulation of the entropy reduction objective
% based on mutual information. Exact calculations of PES and ES would give equivalent acquisition functions, but exact calculation is not typically possible, and so the difference in computational techniques
% used to approximate the PES and ES acquisition functions creates practical differences in the sampling
% decisions that result from the two approaches. We first discuss ES and then PES.

% Let x  be the global optimum of f. The posterior distribution on f at time n induces a probability
% distribution for x
% . Indeed, if domain A were finite, we could represent f over its domain by a
% vector (f(x): x ∈ A), and x
% would correspond to the largest element in this vector. The distribution of
% this vector under the time-n posterior distribution would be multivariate normal, and this multivariate
% normal distribution would imply the distribution of x

% . When A is continuous, the same ideas apply,
% where x

% is a random variable whose distribution is implied by the Gaussian process posterior on f




% With kriging, we can develop search methods that put some emphasis on sampling where the standard
% error is high. In this way, we obtain the desired feature of ‘paying attention to parts of the space
% that have been relatively unexplored.’