\chapter{Generative models as surrogate}
Generative models are statistical models of the joint distribution $p(x,y)$ We need, however, a
discriminative model for regression, i.e. a model of the conditional distribution of $y$ given $x$,
i.e. $p(y|x)$. All generative models we deal with in this thesis allow for exact inference of the
conditional distribution. So given a well-fitted generative model, one could immediately think they
would be feasible to use as surrogate models. However, in this project, we only look at Gaussian
mixture models as generative models - and they have a problem for $x$-values where the probability
of the observed input data, the marginal $p(x)$, is low. Recall the conditional distribution is 
$$p(y|x) = \frac{p(x,y)}{p(x)}$$ and can be interpreted as a slice of the joint distribution
$p(x,y)$ for a fixed value of $x$, but normalized with $p(x) = \int p(x,y) dy$. So even if there is
a very small probability of the data, the conditional probability $p(y|x)$ gets artificially certain
in the case of Gaussian mixtures. We, therefore, need to introduce a prior distribution for $y$,
which will take over in areas for no data, i.e. small $p(x)$. This is discussed in section \ref{mixture_include_prior}.
%\ref{...}

Using generative models as regression models is not used much in the literature. Using the
conditional of a Gaussian mixture model (or kernel estimator) for regression has been discussed
briefly in \cite{bishop1995neural} and using kernel estimator \cite{ALStatisticalModels} and
\cite{JordanPaper} for active learning. According to these sources, the good reasons for using the
mixtures for regression are that they can be used to represent any relations between the variables,
e.g., $p(y|x)$ or $p(x|y)$. They are both applicable in supervised and unsupervised machine learning.
And they are good at dealing with incomplete data, i.e. missing values in the data set <change
this>. We hypothesize that it will allow for an expressive surrogate model, which competently
can deal with complex BO tasks, as they do not assume continuity. In this thesis we will first look
at the most simple approach to a generative model, i.e. putting an equally weighted Gaussian mixture
component on each data point. This is also referred to as a kernel estimator (some might know this 
from kde-plots/estimating a distribution from data), but with a twist of including a prior
distribution to it. Next, we look at the more intelligent models, Gaussian mixture models, which
hopefully can capture some correlations between the variables. And finally, we look at the more
complicated sum-product networks, which introduce a generalization element and have a flavor of a
neural network. To summarize, the mixture regression models are:

\begin{itemize}[noitemsep]
     \item Kernel estimator regression,
     \item Gaussian mixture regression,
     \item Sum-product networks.
 \end{itemize}

\textbf{Note:}

Initially, we wanted to investigate sum-product networks, untill we realised that they are just
mixture models. Therefor it made sense to develop regression theory for the more simple Gaussian
mixture models. However, it results weren't always relyable, and
when the most easy way was just to place a gaussian distribution around
all data point i.e. the Kernel density estimator. 


 \section{Conditional distribution in a Bayesian setting}\label{mixture_include_prior}
 \input{Chapters/05_a_prior_inclusion.tex}


%  \begin{figure}[H]
%     \centering
%     {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_10.pdf} }%
%     \qquad
%    {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_100.pdf} }%
%     \caption{Example of Naive GMR with prior. When data is observed $\alpha := \frac{S(x)}{S(x)+1}$ gets close
%     to 1 and the likehood is dominant.}
% \end{figure}


\section{Conditional of mixture model}\label{Conditional_mixture}
To exploit a generative model as a surrogate model in Bayesian optimization, we need to calculate
the condtional distribution. Fortunately, all generative models used in this thesis are mixture
models, which simplifies the upcomming deveriations. We define a general mixture model as, 
$$p(x,y) = \sum_z \lambda_z p_z(x,y)$$ where $p_z(x,y)$ are mixture components, i.e. simpler
generative models with same support, $(x,y) \in \mathcal{X}\times \mathbb{R}$. The goal is to define
the conditional distribution exact for all the mixture models. As we will soon see, this is again a
mixture model, 
$$p(y|x) = \sum_z \gamma_z(x) p_z(y|x).$$ with $\sum_z \gamma_z(x) = 1$ and $\gamma_z(x) \in [0,1]$.
First, we calcalculate the marginal distribution $p(x)$ of the mixture, 
%assuming that it is possible to marginalize $p_z(x) = \int p_z(x,y) dy$
\begin{align*}
    p(x) &= \int p(x,y) dy =\sum_{z} \lambda_z \int p_z(x,y) dy =\sum_{z} \lambda_z p_z(x)
\end{align*}

Next, we can calculate the conditional in terms of the conditional of the individual mixture
components, 
\begin{align}
    p(y|x) &= \frac{p(y,x)}{p(x)}\\
    &= \sum_{z} \frac{\lambda_z}{p_z(x)} p_z(x,y)\\
    &=  \sum_{z}  \frac{\lambda_z p_z(x)}{p(x)}p_z(y|x)\\
    &=  \sum_{z}  \underbrace{ \frac{\lambda_z p_z(x)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}}_{\gamma_z(x)} p_z(y|x)
\end{align}

So we see that the conditional of a mixture model is again a mixture model. 
We also see that $\sum_z \gamma_z(x) = 1$ and hence we can interpret the above as the following, 
$$p(y|x) = p_z(y|x)  \hspace*{1cm} z \sim Cat(\gamma_1(x), \dots, \gamma_Z(x)) $$ And we name
$p(z|x) = \gamma_z(x) \in [0,1]$ the \textit{responsibility} of mixture component $z$ at a given
location $x \in \mathcal{X}$, (The probability that $y$ to belong to component $z$ at a given
location $x$). For implementation we notice that the denominator in $\gamma_z(x)$ can be reused for
all components. 

We will present all the models and show how their conditional distributions are calculated concretely.


\section{Kernel estimator regression}
Maybe the most simple mixture model one could think about is to put a small variance Gaussian mixture
component around all data points and weight all the components equally. So for $n$ datapoints,
$\{(x_i,y_i)\}_{i=1}^n$, the generative model is given as, 

$$p(x,y) = \frac{1}{N} \sum_{i=1}^n \mathcal{N}\left(\begin{bmatrix}x\\y\end{bmatrix} |
\begin{bmatrix}x_i\\y_i\end{bmatrix}, \sigma^2 I \right) = \frac{1}{N} \sum_{i=1}^n 
\mathcal{N}(x|x_i, \sigma^2 I)\mathcal{N}(y|y_i, \sigma^2) $$
where $\sigma^2$ is refered as the bandwidth, when the literature refers to the above as a kernel estimator. 
Small $\sigma^2$ yields a complex model and large $\sigma^2$ yields a simple model. Therefore chosing $\sigma^2$
just rigth is crucial for a good model. 


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/KDE/Test1_KDE_n_10_seed_42.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/KDE/Test2_KDE_n_40_seed_42.jpg}
     \end{minipage}
    
     \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/KDE/Test3_KDE_n_40_seed_42.jpg}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/KDE/Test4_KDE_n_40_seed_42.jpg}
      \end{minipage}
      \caption{KDE tested on all problems. It is seen that we might loose a lot of generalization properties optained from
      GPs and BNN, however, for the very complicated cases, KDE might have an advantage.}
\end{figure}

\subsection{Conditional of Kernel density estimator}
Since the kernel density estimator is just a Gaussian mixture model, with no correlation between
any of the variables, yeilding i.e. $p_z(y|x) = p_z(y)$, therefor the conditional distribution is
given as, 
\begin{align}
    p(y|x) &= \sum_{z=1}^n \gamma_z(x) \mathcal{N}(y|\mu_{y}^{(z)},\Sigma_{yy}^{(z)} )\\
    \gamma_z(x) &= \frac{\lambda_z \mathcal{N}(x|\mu_{x}^{(z)},\Sigma_{xx}^{(z)})}{\sum_{z^*}
\lambda_{z^*} \mathcal{N}(x|\mu_{x}^{(z^*)},\Sigma_{xx}^{(z^*)})}
\end{align}
Giving a the computational complexity of $O(n)$, since we reused the denominator of 
$\gamma_z(x)$ for all components $z$. 

\section{Gaussian mixture regression}
Extending the kernel estimator regression with covariance between the variables, only $K \leq N$ components 
and different weighting on each component, we arrive at a Gaussian mixture model. This conditional of this
gives the Gaussian mixture regression model. 

We can model our data, as a generative model $p(x,y)$, 
$$p(x,y)= \sum_{z=1}^K \lambda_z \mathcal{N}(x,y|\mu^{(z)},\Sigma^{(z)}), \hspace{1cm}
\mu^{(z)}=\begin{bmatrix} \mu^{(z)}_x \\ \mu^{(z)}_y \end{bmatrix},\hspace{0.1cm} \Sigma^{(z)} =
\begin{bmatrix} \Sigma^{(z)}_{xx} & \Sigma^{(z)}_{xy}\\ \Sigma^{(z)}_{yx}& \Sigma^{(z)}_{yy}
\end{bmatrix}$$ where $\sum_{z=1}^K \lambda_z = 1$. The parameters $\left(\lambda_z,
\mu^{(z)}, \Sigma^{(z)} \right)_{z=1}^K$ need to be trained, which is done using the EM
algorithm. We will now show how the conditional is calculated exactly. 

\subsection{Conditional of Gaussian mixture model}
Since the components are multivariate Gaussian distributions, we use <REF> and can define the
conditional of a multivariate Gaussian as
\begin{align}
    p_z(y|x) &= \mathcal{N}(y|\mu^{(z)}_{y|x},\Sigma^{(z)}_{y|x} )\\
    \mu^{(z)}_{y|x} :&= \mu^{(z)}_y+\Sigma^{(z)}_{yx}(\Sigma^{(z)}_{xx})^{-1}(x-\mu^{(z)}_x)\\
    \Sigma^{(z)}_{y|x} :&= \Sigma^{(z)}_{yy}-\Sigma^{(z)}_{yx}(\Sigma^{(z)}_{xx})^{-1}\Sigma^{(z)}_{xy} 
\end{align}
Now, conditional is defined straight forward from Section \eqref{Conditional_mixture},
\begin{align}
    p(y|x) &= \sum_{z=1}^K \gamma_z(x) \mathcal{N}(y|\mu_{y|x}^{(z)},\Sigma_{y|x}^{(z)} ) \\
    \gamma_z(x) :&=\frac{\lambda_z \mathcal{N}(x|\mu_{x}^{(z)},\Sigma_{xx}^{(z)})}{\sum_{z^*=1}^K \lambda_{z^*}
\mathcal{N}(x|\mu_{x}^{(z^*)},\Sigma_{xx}^{(z^*)})}
\end{align}
note here the computational complexity is $O(K\cdot dim(x)^3)$ since the matrix inversion
of the covariance matrix $(\Sigma^{(z)}_{xx})^{-1}$ is the dominating cost and it happens
for all the $K$ components. 

\section{Sum product networks}
%<What is SPN>
A sum-product network is a mixture model, which allows for exponentially many mixture components, 
but only with linearly many parameters. Figure \ref{SPN_fig} illustrates how
simple distributions from two different scopes $x$ and $y$ can be multiplied
together and defined as many mixtures as the product of the numbers of 
distributions in each scope. In the SPN implementation used in this thesis, 
the number of distribution in each scope is called the number of channels.  

\begin{figure}[H]%
    \centering
    {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration1.pdf} }%
    \qquad
   {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration2.pdf} }%
    \caption{left: the data lies perfect for the SPN. Right: The data distribution
    is not suited for SPN}%
    \label{SPN_fig}%
\end{figure}

Figure \ref{WeightedSPN} shows that even though we are getting some
mixture components for free, then they can be turned off or given a small weight
if no data is present there. 

\begin{figure}[H]
    \centering
    {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration3.pdf} }%
    \qquad
   {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration4.pdf} }%
    \caption{Example of how the weight of each mixture component is turned up and
    down according the amount of data observed.}
    \label{WeightedSPN}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/SPN_graph2.pdf}
    \caption{The SPN trained in Figure \ref{SPN_fig}. Number of channels are
    3, giving 9 product nodes, which are weighted in the final sum node.}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/SPN/Test1_SPN_n_10_seed_42.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/SPN/Test2_SPN_n_40_seed_42.jpg}
     \end{minipage}
    
     \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/SPN/Test3_SPN_n_40_seed_42.jpg}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1cm 0.7cm 1cm 1cm,clip,width=\textwidth]{Figures/reg_illustrations/SPN/Test4_SPN_n_40_seed_42.jpg}
      \end{minipage}
      \caption{SPN tested on all problems.}
\end{figure}

\subsection{SPN as mixture regression}
We will to a large extend just see SPN as a large mixture model. This is a valid observation. 

%from [@desana]:
\begin{definition} 
    A sub-network $\bar S_z$ of $S$ is an SPN, which includes the root $S$ and then includes nodes
    according to the following recursive scheme: 
\end{definition}
\begin{algorithm}[H]
    \caption*{Collection of sub-network $S_z$ of $S$}\label{SPN4}
    \begin{algorithmic}
    %\State \textbf{Global:}  $S_z$ 
    \Function{Process}{node i, $S_z$}
    \If{$i \in \mathcal{L}eaf(S)$}
        \State  $\textbf{return: }$ 
    \EndIf
    %\For{$i \in I_{o}$}
    \If{$i\in \mathcal{S}um(S)$}
       %\State $S_z =S_z \cup \{j \in ch(i)\}$ \Comment{include one child of node $i$}
        \State $S_z =S_z.add(j \in ch(i))$ \Comment{include one child of node $i$}
        \State $\textbf{return: } \text{Process}(j, S_z)$
    \EndIf
    \If{$i\in \mathcal{P}rod(S)$}
        \State $S_z =S_z \cup \{j | j \in ch(i)\}$ \Comment{include all childen of node $i$}
        \For{$j \in ch(i)$}
            \State $\textbf{return: } \text{Process}(j,S_z)$
        \EndFor
    \EndIf
    \State $\textbf{return: } S_z$
    \EndFunction
    \State $S_z =  \text{Process(root,Ã˜)}$
    \end{algorithmic}
\end{algorithm}
So we see that at each sum node the number of different sub-networks multiplies with the number of children for that
sum node. And thereby, the total number of sub-networks is
 $$Z = \prod_{i\in \mathcal{S}um(S)}|ch(i)|$$ 
 i.e. an exponentially large amount of sub-networks. This is the amount of
 mixture components implicitly defined in an SPN. 
 Denote the set of edges in the sub-network $\mathcal{E}(S_z)$.
Now the we define a mixture coeficient, $\lambda_z$ and component for each $S_z$ as 
$$\lambda_z := \prod_{(i,j)\in \mathcal{E}(S_z)} w_{i,j}, \hspace{1cm}
p_z(x,y|\theta) := \prod_{i \in \mathcal{L}(S_z)} p_i(x,y)$$
where $p_i(x,y)$ is the leaf distribution at leaf node $i$ paramitised with $\theta$. 
It can now be proven that the SPN can be interpreted as the following mixture model, 
$$p(x,y|w,\theta) = \sum_{z=1}^Z \lambda_z(w)p_z(x,y|\theta)$$
i.e. by the weighted sum of all $Z$ sub-networks. For convinience
we define each sum component as $p(z,x,y|w,\theta) := \lambda_z(w)p_z(x,y|\theta)$.
Evaluation of $p(x,y|w,\theta)$ will never be done as the sum over $Z$ components, 
instead there is a proposition. 

\begin{proposition}
    Consider a SPN, S, a sum node $q \in \mathcal{S}um(S)$ and a child $i \in ch(q)$,
    then the following relation holds, 
    $$\sum_{z:(q,i)\in \mathcal{E}(S_z)} \lambda_z(w) p_z(x,y|\theta) = w_{i,q}
    \frac{\partial S}{\partial v(q)} v(i)$$
\end{proposition}


SPNs can also be interpreted as a deep neural network [@vergari]. Here, imagine the
weights of the sum nodes are parameters, leaf distributions are input neurons, root node is output and
all other nodes correspond to hidden neurons

\subsection{SPN as a mixture model}
<source> we can interpret an SPN as the mixture model, 
$$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$. And where
the mixture components are given as 
\begin{align*}
    p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
            &=: p_{z}(x) p_{z}(y) 
\end{align*}
where $\phi_l$ is the density of the $l$'th leafs tractable distribution. The last equation
is splitting the products up in the two marginals $p_z(x)$ and $p_z(y)$ since they are uncorrelated. 

The responsibility 
$$p(z|x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} }$$
is calculated easily using autograd. 

\subsection{Conditional of SPN}
\begin{align*}
    p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
            &=: p_{z_x}(x) p_{z_y}(y) 
\end{align*}
where $\phi_l$ is the density of the $l$'th leafs tractable distribution. Recall that we can interpret an SPN
as the mixture model, 
$$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$.

$$p(y|x)=\sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)$$
With $\gamma(x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)}$ 

\subsubsection{calculation of responsibility}
The responsibility of a datapoint to belong to one mixture component, is given by
$$\gamma_z(x) = \frac{\lambda_z p_z(x)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}$$
We can prove that the responsibility is equal to the gradient of the log likehood, 
$$L:= \sum_n \log \sum_z \lambda_z \exp \psi_z(x_n)$$
where we define $\psi_z(x_n) = \log p_z(x_n)$. Take the gradient 
$$\frac{\partial L}{\partial \psi_{z}(x_{n})} = \frac{\lambda_z p_z(x_n)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}$$
Note that the gradient easily can be found using automatic differentiation. 

\begin{figure}
    \includegraphics[width=0.5\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_10.pdf}
\end{figure}

% \subsection{Conditional of SPN}

% We will soon see how it is possible to write the conditional distribution as the mixture, 
% $$p(y|x) = \sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)$$
% where $ \Sigma(S)$ is the set of all sub-networks in the SPN, $S$ - \todo{IT IS EXPONENTIALLY LARGE}.  
% And where $p_{z_y}(y)$ is defined through $p_z(x,y)$, 
% \begin{align*}
%     p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
%             &=: p_{z_x}(x) p_{z_y}(y) 
% \end{align*}
% where $\phi_l$ is the density of the $l$'th leafs tractable distribution. Recall that we can interpret an SPN
% as the mixture model, 
% $$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
% where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$. First we calculate the marginal density,
% $p(x)$, 
% \begin{align*}
%     p(x) &= \int p(x,y)dy\\
%     &= \int \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)dy\\
%     %&= \sum_{z \in \Sigma(S)} \lambda_z  \int p_z(x,y)dy\\
%     &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)\int p_{z_y}(y)dy \\
%     &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)
% \end{align*}
% Now we are ready to calculate the conditional density, 
% \begin{align*}
%     p(y|x) &=  \frac{p(x,y)}{p(x)}\\
%             &= \frac{\sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)}{p(x)}\\
%             &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{p(x)} p_{z_y}(y)\\
%             &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)} p_{z_y}(y)\\
%             &=\sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)
% \end{align*}
% So we defined $\gamma(x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)}$ 
% and this is very convinient, as we will see soon is 
% equavalent to the derivative of the log-likehood
% of the SPN, which is easily obtained by automatic differentiation. 











\section{Gaussian approximation of mixture regression}



\subsection{Mean and variance of conditional SPN}

The mean of the conditional is just
\begin{align*}
    E_{p(y|x)}[y] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y p_{z_y}(y) dy \\
    &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} E_{\phi_l}[y]
\end{align*}

and the variance is found using the second moment, 
\begin{align*}
    E_{p(y|x)}[y^2] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y^2 p_{z_y}(y) dy \\
    &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} (Var_{\phi_l}[y]+E_{\phi_l}[y]^2)
\end{align*}

\section{Mixture model training}
The following section presents the expectation-maximization algorithm, which is used to 
train the Gaussian mixture model and the SPN. 

\input{Chapters/05_b_EM.tex}