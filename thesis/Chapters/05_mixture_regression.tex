\chapter{Generative model surrogates}
Generative models are statistical models of the joint distribution $p(x,y)$ We need, however, a
discriminative model for regression, i.e. a model of the conditional distribution of $y$ given $x$,
i.e. $p(y|x)$. All generative models we deal with in this thesis allow for exact inference of the
conditional distribution. So given a well-fitted generative model, one could immediately think they
would be feasible to use as surrogate models. However, in this project, we only look at Gaussian
mixture models as generative models - and they have a problem for $x$-values where the probability
of the observed input data, the marginal $p(x)$, is low. Recall the conditional distribution is 
$$p(y|x) = \frac{p(x,y)}{p(x)}$$ and can be interpreted as a slice of the joint distribution
$p(x,y)$ for a fixed value of $x$, but normalized with $p(x) = \int p(x,y) dy$. So even if there is
a very small probability of the data, the conditional probability $p(y|x)$ gets artificially certain
in the case of Gaussian mixtures. We, therefore, need to introduce a prior distribution for $y$,
which will take over in areas for no data, i.e. small $p(x)$. This is discussed in section \ref{mixture_include_prior}.
%\ref{...}

Using generative models as regression models is not used much in the literature. Using the
conditional of a Gaussian mixture model (or kernel estimator) for regression has been discussed
briefly in \cite{bishop1995neural} and using kernel estimator \cite{ALStatisticalModels} and
\cite{JordanPaper} for active learning. According to these sources, the good reasons for using the
mixtures for regression are that they can be used to represent any relations between the variables,
e.g. $p(y|x)$ or $p(x|y)$. They are both applicable in supervised and unsupervised machine learning.
And they are good at dealing with incomplete data, i.e. missing values in the data set <change
this>. It is our hyposesis that it will allow for an expressive surrogate model, which competently
can deal with complex BO tasks, as they do not assume continuity. In this thesis we will first look
at the most simple approach to a generative model, i.e. putting an equally weighted Gaussian mixture
component on each data point. This is also referred to as a kernel estimator (some might know this 
from kde-plots/estimating a distribution from data), but with a twist of including a prior
distribution to it. Next, we look at the more intelligent models, Gaussian mixture models, which
hopefully can capture some correlations between the variables. And finally, we look at the more
complicated sum-product networks, which introduce a generalization element and have a flavor of a
neural network. To summarize, the mixture regression models are:

\begin{itemize}
     \item Naive Gaussian mixture regression (Kernel estimator regression),
     \item Gaussian mixture regression,
     \item Sum-product networks.
 \end{itemize}

 \section{Conditional distribution in a Bayesian setting}\label{mixture_include_prior}
 \input{Chapters/05_a_prior_inclusion.tex}


%  \begin{figure}[H]
%     \centering
%     {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_10.pdf} }%
%     \qquad
%    {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_100.pdf} }%
%     \caption{Example of Naive GMR with prior. When data is observed $\alpha := \frac{S(x)}{S(x)+1}$ gets close
%     to 1 and the likehood is dominant.}
% \end{figure}


\section{Kernel estimator regression}
Maybe the most simple mixture model one could think about is to put a small variance Gaussian mixture
component around all data points and weight all the components equally. So for $n$ datapoints,
$\{(x_i,y_i)\}_{i=1}^n$, the generative model is given as, 

$$p(x,y) = \frac{1}{N} \sum_{i=1}^n \mathcal{N}\left(\begin{bmatrix}x\\y\end{bmatrix} |
\begin{bmatrix}x_i\\y_i\end{bmatrix}, \sigma^2 I \right) = \frac{1}{N} \sum_{i=1}^n 
\mathcal{N}(x|x_i, \sigma^2)\mathcal{N}(y|y_i, \sigma^2) $$
where $\sigma^2$ is refered as the bandwidth, when the literature refers to the above as a kernel estimator. 
Small $\sigma^2$ yields a complex model and large $\sigma^2$ yields a simple model. Therefore chosing $\sigma^2$
just rigth is crucial for a good model. 

\subsection{Conditional distribution}
The conditional distribution is very simple to calculate; first, we need the marginal distribution
$$p(x) = \int p(x,y) dy =  \frac{1}{N} \sum_{i=1}^n \mathcal{N}(x|x_i, \sigma^2)  \int
\mathcal{N}(y|y_i, \sigma^2) dy =  \frac{1}{N} \sum_{i=1}^n \mathcal{N}(x|x_i, \sigma^2)$$
and then we can calculate the conditional distribution,
\begin{align*}
    p(y|x) &= \frac{p(x,y)}{p(x)}\\
    &=  \frac{\frac{1}{N} \sum_{i=1}^n \mathcal{N}(x|x_i, \sigma^2)\mathcal{N}(y|y_i, \sigma^2)}{ \frac{1}{N} \sum_{j=1}^n \mathcal{N}(x|x_j, \sigma^2)}\\
    &= \sum_{i=1}^n \gamma_i \mathcal{N}(y|y_i, \sigma^2)
\end{align*}
where $\gamma_i = \frac{\mathcal{N}(x|x_i, \sigma^2)}{\sum_{j=1}^n \mathcal{N}(x|x_j, \sigma^2)}$. 


\section{Gaussian mixture regression}
Extending the kernel estimator regression with covariance between the variables, only $K \leq N$ components 
and different weighting on each component, we arrive at a Gaussian mixture model. This conditional of this
gives the Gaussian mixture regression model. 

We can model our data, as a generative model $p(x,y)$, 
$$p(x,y)= \sum_{k=1}^K \pi^{(k)} \mathcal{N}(x,y|\mu^{(k)},\Sigma^{(k)}), 
\hspace{1cm} \mu^{(k)}=\begin{bmatrix}
    \mu^{(k)}_x \\ \mu^{(k)}_y
\end{bmatrix},\hspace{0.1cm} \Sigma^{(k)} = \begin{bmatrix}
    \Sigma^{(k)}_{xx} & \Sigma^{(k)}_{xy}\\ \Sigma^{(k)}_{yx}& \Sigma^{(k)}_{yy}
\end{bmatrix}$$
where $\sum_{k=1}^K \pi^{(k)} = 1$. The parameters $\left(\Sigma^{(k)}, \pi^{(k)}, \mu^{(k)}\right)_{k=1}^K$
need to be trained, which is done using the EM algorithm. We will now show how the conditional is
calculated exactly. 
\subsection{Conditional distribution}
We will now formulate the conditial distribution in terms of conditional and marginals of
the individual mixture components. First of all the marginal distribution $p(x)$ 
of the mixture is given as,
\begin{align*}
    p(x) &= \int p(x,y) dy \\
    &=\sum_{k=1}^K \pi^{(k)} \int \mathcal{N}(x,y|\mu^{(k)},\Sigma^{(k)}) dy\\
    &=\sum_{k=1}^K \pi^{(k)} \mathcal{N}(x|\mu_{x}^{(k)},\Sigma_{xx}^{(k)})
\end{align*}
next the joint distribution can be decomposed with the probability chain rule,
\begin{align*}
    p(x,y) &= p(x)p(y|x)\\
    \implies \hspace{1cm} \mathcal{N}(x,y|\mu,\Sigma) &= 
    \mathcal{N}(x|\mu_{x},\Sigma_{xx}) \mathcal{N}(y|\mu_{y|x},\Sigma_{y|x})
\end{align*} 
And we can formulate the conditional in terms of individual multivariate Gaussians, 

\begin{align}
    p(y|x) &= \frac{p(y,x)}{p(x)}\\
    &= \sum_{k=1}^K \frac{\pi^{(k)}}{p(x)} \mathcal{N}(x,y|\mu^{(k)},\Sigma^{(k)})\\
    &=  \sum_{k=1}^K \frac{\pi^{(k)} \mathcal{N}(x|\mu_{x}^{(k)},\Sigma_{xx}^{(k)})}{p(x)}\mathcal{N}(y|\mu_{y|x}^{(k)},\Sigma_{y|x}^{(k)})\\
    &=  \sum_{k=1}^K \pi_{y|x}^{(k)} p(y|x,\mu_{y|x}^{(k)},\Sigma_{y|x}^{(k)})
\end{align}

where $\pi_{y|x}^{(k)} := \frac{\pi^{(k)} \mathcal{N}(x|\mu_{x}^{(k)},\Sigma_{xx}^{(k)})}
{\sum_{i=1}^K \mathcal{N}(x|\mu_{x}^{(i)},\Sigma_{xx}^{(i)})}$. So we see that 
the conditional of a Gaussian mixture model is again a Gaussian mixture model.


\begin{testexample2}[Conditional og multivariate Gaussian]
    The conditional distribution is defined as \cite{bishop} ..?
    $$p(y|x,\mu, \Sigma) = \mathcal{N}(y|\mu_{y|x},\Sigma_{y|x} )\hspace{1cm} \mu=\begin{bmatrix}
        \mu_x \\ \mu_y
    \end{bmatrix},\hspace{0.1cm} \Sigma = \begin{bmatrix}
        \Sigma_{xx} & \Sigma_{xy}\\ \Sigma_{yx}& \Sigma_{yy}
    \end{bmatrix}$$ 
    where 
    \begin{align}
        \mu_{y|x} &= \mu_y+\Sigma_{yx}\Sigma_{xx}^{-1}(x-\mu_x)\\
        \Sigma_{y|x} &= \Sigma_{yy}-\Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy} 
    \end{align}
\end{testexample2}


\section{Sum product networks}
%<What is SPN>
A sum-product network is a mixture model, which allows for exponentially many mixture components, 
but only with linearly many parameters. Figure \ref{SPN_fig} illustrates how
simple distributions from two different scopes $x$ and $y$ can be multiplied
together and define as many mixtures as the product of the numbers of 
distributions in each scope. In the SPN implementation used in this thesis, 
the number of distribution in each scope is called the number of channels.  

\begin{figure}[H]%
    \centering
    {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration1.pdf} }%
    \qquad
   {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration2.pdf} }%
    \caption{left: the data lies perfect for the SPN. Right: The data distribution
    is not suited for SPN}%
    \label{SPN_fig}%
\end{figure}

Figure \ref{WeightedSPN} shows that even though we are getting some
mixture components for free, then they can be turned off or given a small weight
if no data is present there. 

\begin{figure}[H]
    \centering
    {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration3.pdf} }%
    \qquad
   {\includegraphics[width=0.46\textwidth]{Pictures/SPN_illustration4.pdf} }%
    \caption{Example of how the weight of each mixture component is turned up and
    down according the amount of data observed.}
    \label{WeightedSPN}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/SPN_graph2.pdf}
    \caption{The SPN trained in Figure \ref{SPN_fig}. Number of channels are
    3, giving 9 product nodes, which are weighted in the final sum node.}
\end{figure}


\subsection{Conditional of SPN}

We will soon see how it is possible to write the conditional distribution as the mixture, 
$$p(y|x) = \sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)$$
where $ \Sigma(S)$ is the set of all sub-networks in the SPN, $S$ - \todo{IT IS EXPONENTIALLY LARGE}.  
And where $p_{z_y}(y)$ is defined through $p_z(x,y)$, 
\begin{align*}
    p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
            &=: p_{z_x}(x) p_{z_y}(y) 
\end{align*}
where $\phi_l$ is the density of the $l$'th leafs tractable distribution. Recall that we can interpret an SPN
as the mixture model, 
$$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$. First we calculate the marginal density,
$p(x)$, 
\begin{align*}
    p(x) &= \int p(x,y)dy\\
    &= \int \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)dy\\
    %&= \sum_{z \in \Sigma(S)} \lambda_z  \int p_z(x,y)dy\\
    &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)\int p_{z_y}(y)dy \\
    &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)
\end{align*}
Now we are ready to calculate the conditional density, 
\begin{align*}
    p(y|x) &=  \frac{p(x,y)}{p(x)}\\
            &= \frac{\sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)}{p(x)}\\
            &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{p(x)} p_{z_y}(y)\\
            &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)} p_{z_y}(y)\\
            &=\sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)
\end{align*}
So we defined $\gamma(x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)}$ 
and this is very convinient, as we will see soon is 
equavalent to the derivative of the log-likehood
of the SPN, which is easily obtained by automatic differentiation. 

\subsection{calculation of responsibility, $\gamma(x)$}
expectation maximization of a mixture model, is given by Bishop..
the responsibility of a datapoint to belong to one mixture component, is given by
$$\gamma(z_{nk}) = \frac{w_k p_j(x_n)}{\sum_i w_i p_i(x_n)}$$
We can prove that the responsibility is equal to the gradient of the log likehood, 
$$L:= \sum_n \log \sum_j w_j \exp \psi_j(x_n)$$
where we define $\psi_j(x_n) = \log p_j(x_n)$. Take the gradient 
$$\frac{\partial L}{\partial \psi_{j}(x_{n})} = \frac{w_k p_j(x_n)}{\sum_i w_i p_i(x_n)}$$
Note that the gradient easily can be found using autograd. 


\subsection{Mean and variance of conditional SPN}

The mean of the conditional is just
\begin{align*}
    E_{p(y|x)}[y] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y p_{z_y}(y) dy \\
    &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} E_{\phi_l}[y]
\end{align*}

and the variance is found using the second moment, 
\begin{align*}
    E_{p(y|x)}[y^2] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y^2 p_{z_y}(y) dy \\
    &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} (Var_{\phi_l}[y]+E_{\phi_l}[y]^2)
\end{align*}




\section{Mixture model training}
The following section presents the expectation-maximization algorithm, which is used to 
train the Gaussian mixture model and the SPN. 

\input{Chapters/05_b_EM.tex}