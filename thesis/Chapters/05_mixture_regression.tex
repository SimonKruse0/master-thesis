\chapter{Generative models as surrogate}
Generative models are statistical models of the joint distribution $p(x,y)$ We need, however, a
discriminative model for regression, i.e. a model of the conditional distribution of $y$ given $x$,
i.e. $p(y|x)$. All generative models we deal with in this thesis allow for exact inference of the
conditional distribution. So given a well-fitted generative model, one could immediately think they
would be feasible to use as surrogate models. However, in this project, we only look at Gaussian
mixture models as generative models - and they have a problem for $x$-values where the probability
of the observed input data, the marginal $p(x)$, is low. Recall the conditional distribution is 
$$p(y|x) = \frac{p(x,y)}{p(x)}$$ and can be interpreted as a slice of the joint distribution
$p(x,y)$ for a fixed value of $x$, but normalized with $p(x) = \int p(x,y) dy$. So even if there is
a very small probability of the data, the conditional probability $p(y|x)$ gets artificially certain
in the case of Gaussian mixtures. We, therefore, need to introduce a prior distribution for $y$,
which will take over in areas for no data, i.e. small $p(x)$. This is discussed in section \ref{mixture_include_prior}.
%\ref{...}

Using generative models as regression models is not used much in the literature. Using the
conditional of a Gaussian mixture model (or kernel estimator) for regression has been discussed
briefly in \cite{bishop1995neural} and using kernel estimator \cite{ALStatisticalModels} and
\cite{JordanPaper} for active learning. According to these sources, the good reasons for using the
mixtures for regression are that they can be used to represent any relations between the variables,
e.g., $p(y|x)$ or $p(x|y)$. They are both applicable in supervised and unsupervised machine learning.
And they are good at dealing with incomplete data, i.e. missing values in the data set <change
this>. We hypothesize that it will allow for an expressive surrogate model, which competently
can deal with complex BO tasks, as they do not assume continuity. In this thesis we will first look
at the most simple approach to a generative model, i.e. putting an equally weighted Gaussian mixture
component on each data point. This is also referred to as a kernel estimator (some might know this 
from kde-plots/estimating a distribution from data), but with a twist of including a prior
distribution to it. Next, we look at the more intelligent models, Gaussian mixture models, which
hopefully can capture some correlations between the variables. And finally, we look at the more
complicated sum-product networks, which introduce a generalization element and have a flavor of a
neural network. To summarize, the mixture regression models are:

\begin{itemize}[noitemsep]
     \item Kernel density estimator regression,
     \item Gaussian mixture regression,
     \item Sum-product networks.
 \end{itemize}

%  \begin{note2}[]
%     Initially, we wanted to investigate sum-product networks, until we realized that they are just
%     mixture models. Therefore, it made sense to develop regression theory for the more simple
%     Gaussian mixture models. However, as they are not so robust, we could deveolop the theory by
%     placing a Gaussian distribution around all data points i.e. the Kernel density estimator. 
%  \end{note2}



 \section{Conditional distribution in a Bayesian setting}\label{mixture_include_prior}
 \input{Chapters/05_a_prior_inclusion.tex}


%  \begin{figure}[H]
%     \centering
%     {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_10.pdf} }%
%     \qquad
%    {\includegraphics[width=0.46\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_100.pdf} }%
%     \caption{Example of Naive GMR with prior. When data is observed $\alpha := \frac{S(x)}{S(x)+1}$ gets close
%     to 1 and the likehood is dominant.}
% \end{figure}


\section{Conditional of mixture model}\label{Conditional_mixture}
To exploit a generative model as a surrogate model in Bayesian optimization, we need to calculate
the condtional distribution. Fortunately, all generative models used in this thesis are mixture
models, which simplifies the upcomming deveriations. We define a general mixture model with $Z$ mixture
components as, 
$$p(x,y) = \sum_{z=1}^Z \lambda_z p_z(x,y)$$ where $p_z(x,y)$ are mixture components, i.e. simpler
generative models with same support, $(x,y) \in \mathcal{X}\times \mathbb{R}$. 
The goal is to define
the conditional distribution exact for all the mixture models. As we will soon see, this is again a
mixture model, 
$$p(y|x) = \sum_z \gamma_z(x) p_z(y|x).$$ with $\sum_z \gamma_z(x) = 1$ and $\gamma_z(x) \in [0,1]$.
First, we calcalculate the marginal distribution $p(x)$ of the mixture, 
%assuming that it is possible to marginalize $p_z(x) = \int p_z(x,y) dy$
\begin{align*}
    p(x) &= \int p(x,y) dy =\sum_{z} \lambda_z \int p_z(x,y) dy =\sum_{z} \lambda_z p_z(x).
\end{align*}

Next, we can calculate the conditional in terms of the conditional of the individual mixture
components, 
\begin{align*}
    p(y|x) &= \frac{p(y,x)}{p(x)}\\
    &= \sum_{z} \frac{\lambda_z}{p(x)} p_z(x,y)\\
    &=  \sum_{z}  \frac{\lambda_z p_z(x)}{p(x)}p_z(y|x)\\
    &=  \sum_{z}  \underbrace{ \frac{\lambda_z p_z(x)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}}_{\gamma_z(x)} p_z(y|x).
\end{align*}

So we see that the conditional of a mixture model is again a mixture model. 
We also see that $\sum_z \gamma_z(x) = 1$ and hence we can interpret the above as the following, 
$$p(y|x) = p_z(y|x),  \hspace*{1cm} z \sim Cat(\gamma_1(x), \dots, \gamma_Z(x)).$$ And we name
$p(z|x) = \gamma_z(x) \in [0,1]$ the \textit{responsibility} of mixture component $z$ at a given
location $x \in \mathcal{X}$, (The probability that $y$ to belong to component $z$ at a given
location $x$). For implementation we notice that the denominator in $\gamma_z(x)$ can be reused for
all components. 

\begin{testexample}[Gaussian approximation of mixture conditional]
    As discussed in Section \ref{mean_variance_pred_mixture}, in order to obtain the closed-form
    solution in the expected improvement, we can approximate the  mixture with a gaussian distribution, 
    i.e. calculation of the conditional mean and variance. 

    The mean of the conditional is just
    \begin{align*}
        E_{p(y|x)}[y] &= \sum_{z} \gamma_z(x) \int y \cdot p_z(y|x)dy \\
        &= \sum_{z} \gamma_z(x) E_{ p_z(y|x)}[y].
    \end{align*}

    The variance is found using the variance definition $V[y] = E[y^2] - E[y]^2$, 
    \begin{align*}
        E_{p(y|x)}[y^2] &= \sum_{z} \gamma_z(x) \int  y^2 p_{z}(y|x) dy \\
        &= \sum_{z} \gamma_z(x) (Var_{ p_z(y|x)}[y]+E_{ p_z(y|x)}[y]^2).
    \end{align*}
\end{testexample}



We will now present all the models and show how their conditional distributions are calculated concretely.

\section{Kernel estimator regression}
Maybe the most simple mixture model one could think about is to put a small variance Gaussian mixture
component around all data points and weight all the components equally. So for $n$ datapoints,
$\{(x_i,y_i)\}_{i=1}^n$, the generative model is given as, 

$$p(x,y) = \frac{1}{N} \sum_{i=1}^n \mathcal{N}\left(\begin{bmatrix}x\\y\end{bmatrix} \middle|
\begin{bmatrix}x_i\\y_i\end{bmatrix}, \sigma^2 I \right) = \frac{1}{N} \sum_{i=1}^n 
\mathcal{N}(x|x_i, \sigma^2 I)\mathcal{N}(y|y_i, \sigma^2),$$
where $\sigma^2$ is refered as the bandwidth, when the literature refers to the above as a kernel estimator. 
Small $\sigma^2$ yields a complex model and large $\sigma^2$ yields a simple model. Therefore chosing $\sigma^2$
just rigth is crucial for a good model. 


\subsection{Conditional of Kernel density estimator}
Since the kernel density estimator is just a Gaussian mixture model, with no correlation between
any of the variables, yeilding i.e. $p_z(y|x) = p_z(y)$, therefor the conditional distribution is
given as, 
\begin{align}
    p(y|x) &= \sum_{z=1}^n \gamma_z(x) \mathcal{N}(y|\mu_{y}^{(z)},\Sigma_{yy}^{(z)} ),\\
    \gamma_z(x) &= \frac{\lambda_z \mathcal{N}(x|\mu_{x}^{(z)},\Sigma_{xx}^{(z)})}{\sum_{z^*}
\lambda_{z^*} \mathcal{N}(x|\mu_{x}^{(z^*)},\Sigma_{xx}^{(z^*)})}.
\end{align}
The computational complexity of calculating the conditional or the predictive distribution is
$O(n)$, since we reused the denominator of $\gamma_z(x)$ for all components $z$. 

\section{Gaussian mixture regression}
Extending the kernel estimator regression with covariance between the variables, only $K \leq N$ components 
and different weighting on each component, we arrive at a Gaussian mixture model. The conditional of GMM 
gives the Gaussian mixture regression model \cite{GMR}. 

We can model our data, as a generative model $p(x,y)$, 
$$p(x,y)= \sum_{z=1}^K \lambda_z \mathcal{N}(x,y|\mu^{(z)},\Sigma^{(z)}), \hspace{1cm}
\mu^{(z)}=\begin{bmatrix} \mu^{(z)}_x \\ \mu^{(z)}_y \end{bmatrix},\hspace{0.1cm} \Sigma^{(z)} =
\begin{bmatrix} \Sigma^{(z)}_{xx} & \Sigma^{(z)}_{xy}\\ \Sigma^{(z)}_{yx}& \Sigma^{(z)}_{yy}
\end{bmatrix},$$ where $\sum_{z=1}^K \lambda_z = 1$. The parameters $\left(\lambda_z,
\mu^{(z)}, \Sigma^{(z)} \right)_{z=1}^K$ need to be trained, which is done using the EM
algorithm. We will now show how the conditional is calculated exactly. 

\subsection{Conditional of Gaussian mixture model}
Since the components are multivariate Gaussian distributions, we use <REF> and can define the
conditional of a multivariate Gaussian as
\begin{align}
    p_z(y|x) &= \mathcal{N}(y|\mu^{(z)}_{y|x},\Sigma^{(z)}_{y|x} )\\
    \mu^{(z)}_{y|x} :&= \mu^{(z)}_y+\Sigma^{(z)}_{yx}(\Sigma^{(z)}_{xx})^{-1}(x-\mu^{(z)}_x)\\
    \Sigma^{(z)}_{y|x} :&= \Sigma^{(z)}_{yy}-\Sigma^{(z)}_{yx}(\Sigma^{(z)}_{xx})^{-1}\Sigma^{(z)}_{xy}.
\end{align}
Now, the conditional is defined straight forward from Section \eqref{Conditional_mixture},
\begin{align}
    p(y|x) &= \sum_{z=1}^K \gamma_z(x) \mathcal{N}(y|\mu_{y|x}^{(z)},\Sigma_{y|x}^{(z)} ) \\
    \gamma_z(x) :&=\frac{\lambda_z \mathcal{N}(x|\mu_{x}^{(z)},\Sigma_{xx}^{(z)})}{\sum_{z^*=1}^K \lambda_{z^*}
\mathcal{N}(x|\mu_{x}^{(z^*)},\Sigma_{xx}^{(z^*)})}
\end{align}
The computational complexity is $O(K\cdot d^3)$ ($d$ is the dimension of $x$), since the matrix inversion
of the covariance matrix $(\Sigma^{(z)}_{xx})^{-1}$ is the dominating cost and it happens
for all the $K$ components. 

\section{Sum product networks}
%<What is SPN>

A sum-product network (SPN) is a mixture model, which allows for exponentially many mixture
components, but with only <linearly many parameters> and, thereby, tractable inference (i.e.
conditionalization and marginalization queries). In short, the SPN consists of a computational
graph, with tractable leaf distributions, which are combined using products and sums nodes,
recursively. To keep the inference of the SPN tractable, we want to maintain certain properties when
designing the SPN graph. First we need to define a scope. 
\begin{testexample}[Scope of nodes in SPN]
    A scope (sc) of a leaf node is the set of random variables among each dimension of $x = \{x_1,
    \dots, x_{\text{dim}(x)} \}$ and $y$ of which the leaf distribution,
    $p_i(\cdot)$, defines a distribution function (in our implementation the leaf scopes are all
    singletons). The scope of a sum og product node, $i$, are defined recursively, $sc(i) = \cup_{j \in
    ch(i)} sc(j)$. 
\end{testexample}

\begin{itemize}[noitemsep]
    \item A sum nodes children must have the same scope (completeness). 
    \item A product nodes children must have distinct scopes (decomposability).
    \item Leaf nodes must have tractable inference.
\end{itemize}


The density of the mixture models is calculated in the following way, 
\begin{algorithm}
    \caption*{Calculation of $p(x,y)$}\label{SPN_1}
    \begin{algorithmic}
    \State \textbf{Input:} Fully trained SPN, with leaf distributions $p_i(\cdot)$ for $i\in \mathcal{L}eaf(S)$ and weights 
    $w_{i,j}$ for $(i,j) \in \{(i,j)|i \in \mathcal{S}um(S), j \in ch(i)\}$ 
    \Function{\text{Eval}}{node i}
    \If{$i \in \mathcal{L}eaf(S)$}
        \State  $\textbf{return: } p_i(x,y)$ \Comment{evaluate leaf distributions at point $(x,y)$}
    \EndIf
    %\For{$i \in I_{o}$}
    \If{$i\in \mathcal{S}um(S)$}
        \State $\textbf{return: } \sum_{j\in ch(i)} w_{i,j} \text{Eval}(j)$
    \EndIf
    \If{$i\in \mathcal{P}rod(S)$}
        \State $\textbf{return: } \prod_{j \in ch(i)} \text{Eval}(j)$
    \EndIf
    \EndFunction
    \State $p(x,y) =  \text{Eval(root node)}$
    \end{algorithmic}
\end{algorithm}

In this thesis, we implement the SPN similar to the RAT-SPN presented in \cite{RAT_SPN}, 
which ensures that we have a Complete and decomposable SPN. The following is the structure of 
the RAT-SPN,
\begin{enumerate}[noitemsep]
    \item Define $C$ leaf distributions per random variable\footnote{Note that we define the random
    variables as the dimensions in the joints distribution, i.e. $\{x_1, ..., x_{\text{Dim}(x)},
    y\}$.} ($C$ is called "channels").
    \item Pair up the elements in the set of random variables in a random way. If uneven set, then
    one element pairs with the empty set.
    \item For each pair: Define $C^2$ product nodes by combining each combination of leaf nodes. 
    \item For each pair: Give the $C^2$ product nodes the same $C$ sum node parents. 
    \item Now, pair up the pairs and repeat step 3 (with sum-nodes instead of leaf nodes) and 4.  
    \item At the final iterations: Give the $C^2$ product nodes 1 sum node parent.
\end{enumerate}
 This is a scalable and easy way to construct the SPN. For high-dimensional problems, we can limit the number
of pairs defined in step 2 and instead, combine more of the randomly defined SPNs in several tracks $T$. This allows the
model to be lucky in the case e.g. dimension $x_3$ and $x_5$ was a powerful combination. Figure \ref{SPN_graph_illu}
illustrates the concept of RAT-SPN for only one track $T=1$ (more tracks would shuffle the pairs).

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Pictures/SPN_illustration_graph3.pdf}
    \caption{Illustration of random constructed sum-product network (RAT-SPN) for the joint
    distribution $p(x_1, \dots, x_5, y)$. The product nodes, $\otimes$, always combine 2 nodes from different
    scopes, while sum nodes, $\oplus$, sum all of the product nodes for similar scopes. Note the drawing is not
    complete; This illustarates a RAT-SPN with 3 channels, so every cluster of product nodes has
    size 9 and every cluster of sum nodes (except the root) has size 3.}
     \label{SPN_graph_illu}
\end{figure}

\begin{wrapfigure}{r}{4cm}
    \includegraphics[width=4cm]{Pictures/SPN_illustration_graph1.pdf}
    \caption{Graphical representation of the SPN used in Figure \ref{SPN_fig}. The 9 green arrows
    are weighted and sums to 1}\label{SPN_fig2}
\end{wrapfigure} 
Figure \ref{SPN_fig} illustrates how 3 simple Gaussian distributions from two different scopes $x$ and $y$
can be multiplied together and defined as many mixtures as the product of the numbers of
distributions in each scope (i.e. 9). So by only training parameters for 6 distributions 
we obtain 9 distributions. In the middle figure, we see a data distribution with no need for all 9 
mixture components and the weighting ensures that the unnecessary components are turned off. Figure \ref{SPN_fig2}
illustrates the graphical representation of the SPN for 2 dimensions - and shows that the SPN for small dimensions 
is not deep or complicated. In fact, if we look at the right figure in Figure \ref{SPN_fig}, the SPN is just 
adding less flexibility.

\begin{figure}[H]%
    \centering
    \begin{minipage}[b]{0.32\textwidth}
      \begin{overpic}[trim=0.3cm 0cm 0.1cm 0.7cm,clip,width=\textwidth]{Pictures/SPN_illustration1.pdf}
        \put (-5,40) {\small y}
        \put (40,0) {\small x}
    \end{overpic}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \begin{overpic}[trim=0.3cm 0cm 0.1cm 0.7cm,clip,width=\textwidth]{Pictures/SPN_illustration3.pdf}
        \put (40,0) {\small x}
    \end{overpic}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=0.3cm 0cm 0.1cm 0.7cm,clip,width=\textwidth]{Pictures/SPN_illustration2.pdf}
        \put (40,0) {\small x}
    \end{overpic}
      \end{minipage}
    \caption{SPN on the joint probalility $p(x,y)$ with 3 leaf distributions in each scope (shown on
    the axes), trained on 3 different data sets. Left: The data lies perfect for the SPN. Middle:
    Numbers in the graph represent how the weight of each mixture component is weighted. Right: The
    data is distributed badly for the SPN.}%
    \label{SPN_fig}%
\end{figure}


\begin{note2}[SPN as neural network]
     <Lav om> The interpretation of an SPN as a neural network cite{vergari} (what motivated us to look at them in the first place)
    Here, imagine the weights of the sum nodes are parameters, leaf distributions are input neurons, root node is output and
    all other nodes correspond to hidden neurons.
    Note: the depth is at most $\log_2(\# variables)$, yielding not much depth in Bayesian optimization tasks, where the number
    of dimensions typically is small. 
\end{note2}

\subsection{SPN as a mixture model}
Each sum-node can be interpreted as a categorical variable \cite{??}, i.e. a weighted dice. So each
mixture component can be found by starting at the root sum-node and rolling the dice of which green
arrow to continue the path through the SPN. If the path meets a product-node all children are
included in the path. If the path meets a sum-node we roll a dice. Finally, if the path meets a
leaf-node it terminates. The defined path is referred to as a sub-network, $S_z$, in the SPN and is
equivalent to a mixture component. The total number mixture components equal the product of all
sum-nodes children, i.e. $Z = \prod_{i\in \mathcal{S}um(S)}|ch(i)|$. i.e. an exponentially large
amount. 

Denote the set of edges in the sub-network $\mathcal{E}(S_z)$.
Now the we define a mixture coeficient, $\lambda_z$ and component for each $S_z$ as 
$$\lambda_z := \prod_{(i,j)\in \mathcal{E}(S_z)} w_{i,j}, \hspace{1cm} p_z(x,y|\theta) := \prod_{i
\in \mathcal{L}(S_z)} \phi_i(x,y),$$ where $\phi_i(x,y)$ is the leaf distribution at leaf node $i$
paramitised with $\theta$. It can now be proven that the SPN can be interpreted as the following mixture
model, 
$$p(x,y|w,\theta) = \sum_{z=1}^Z \lambda_z(w)p_z(x,y|\theta)$$
i.e. by the weighted sum of all $Z$ sub-networks. 


\subsection{Conditional of SPN}
Conviniently we can define the mixture model as 
\begin{align*}
    p_z(x,y) &= \prod_{i \in x\mathcal{L}eaf(z)} \phi_i(x)\prod_{i \in y\mathcal{L}eaf(z)} \phi_i(y)\\
            &=: p_{z_x}(x) p_{z_y}(y) 
\end{align*}
<obs connect til forgående sections> giving the conditional of the mixture and the responsibility
\begin{align}
    p(y|x) &= \sum_{z=1}^n \gamma_z(x) p_{z_y}(y)\\
    \gamma_z(x) &= \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)}
\end{align}
\subsubsection{calculation of responsibility}
The responsibility of a datapoint to belong to one mixture component, is given by
$$\gamma_z(x) = \frac{\lambda_z p_z(x)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}$$
We can prove that the responsibility is equal to the gradient of the log likehood, 
$$L:= \sum_n \log \sum_z \lambda_z \exp \psi_z(x_n)$$
where we define $\psi_z(x_n) = \log p_z(x_n)$. Take the gradient 
$$\frac{\partial L}{\partial \psi_{z}(x_{n})} = \frac{\lambda_z p_z(x_n)}{\sum_{z^*} \lambda_{z^*} p_{z^*}(x)}$$
Note that the gradient easily can be found using automatic differentiation. 


% \begin{figure}
%     \includegraphics[width=0.5\textwidth]{Pictures/1D_mixture_regression_Naive GMR_N_10.pdf}
% \end{figure}

% \subsection{Conditional of SPN}

% We will soon see how it is possible to write the conditional distribution as the mixture, 
% $$p(y|x) = \sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)$$
% where $ \Sigma(S)$ is the set of all sub-networks in the SPN, $S$ - \todo{IT IS EXPONENTIALLY LARGE}.  
% And where $p_{z_y}(y)$ is defined through $p_z(x,y)$, 
% \begin{align*}
%     p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
%             &=: p_{z_x}(x) p_{z_y}(y) 
% \end{align*}
% where $\phi_l$ is the density of the $l$'th leafs tractable distribution. Recall that we can interpret an SPN
% as the mixture model, 
% $$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
% where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$. First we calculate the marginal density,
% $p(x)$, 
% \begin{align*}
%     p(x) &= \int p(x,y)dy\\
%     &= \int \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)dy\\
%     %&= \sum_{z \in \Sigma(S)} \lambda_z  \int p_z(x,y)dy\\
%     &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)\int p_{z_y}(y)dy \\
%     &= \sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)
% \end{align*}
% Now we are ready to calculate the conditional density, 
% \begin{align*}
%     p(y|x) &=  \frac{p(x,y)}{p(x)}\\
%             &= \frac{\sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)}{p(x)}\\
%             &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{p(x)} p_{z_y}(y)\\
%             &=\sum_{z \in \Sigma(S)}\frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)} p_{z_y}(y)\\
%             &=\sum_{z \in \Sigma(S)} \gamma(x) p_{z_y}(y)
% \end{align*}
% So we defined $\gamma(x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} \lambda_z p_{z_x}(x)}$ 
% and this is very convinient, as we will see soon is 
% equavalent to the derivative of the log-likehood
% of the SPN, which is easily obtained by automatic differentiation. 


% For convinience
% we define each sum component as $p(z,x,y|w,\theta) := \lambda_z(w)p_z(x,y|\theta)$.
% Evaluation of $p(x,y|w,\theta)$ will never be done as the sum over $Z$ components, 
% instead there is a proposition. 

% \begin{proposition}
%     Consider a SPN, S, a sum node $q \in \mathcal{S}um(S)$ and a child $i \in ch(q)$,
%     then the following relation holds, 
%     $$\sum_{z:(q,i)\in \mathcal{E}(S_z)} \lambda_z(w) p_z(x,y|\theta) = w_{i,q}
%     \frac{\partial S}{\partial v(q)} v(i)$$
% \end{proposition}



% \subsection{SPN as a mixture model}
% <source> we can interpret an SPN as the mixture model, 
% $$p(x,y) = \sum_{z \in \Sigma(S)} \lambda_z p_z(x,y)$$
% where $\lambda_z = \prod_{(q,j) \in \mathcal{E}(z)} w_{q,j}$. And where
% the mixture components are given as 
% \begin{align*}
%     p_z(x,y) &= \prod_{l \in \mathcal{L}eaf(z_x)} \phi_l(x)\prod_{l \in \mathcal{L}eaf(z_y)} \phi_l(y)\\
%             &=: p_{z}(x) p_{z}(y) 
% \end{align*}
% where $\phi_l$ is the density of the $l$'th leafs tractable distribution. The last equation
% is splitting the products up in the two marginals $p_z(x)$ and $p_z(y)$ since they are uncorrelated. 

% The responsibility 
% $$p(z|x) = \frac{ \lambda_z p_{z_x}(x)}{\sum_{z \in \Sigma(S)} }$$
% is calculated easily using autograd. 



%from [@desana]:
% \begin{definition} 
%     A sub-network $\bar S_z$ of $S$ is an SPN, which includes the root $S$ and then includes nodes
%     according to the following recursive scheme: 
% \end{definition}
% \begin{algorithm}[H]
%     \caption*{Collection of sub-network $S_z$ of $S$}\label{SPN4}
%     \begin{algorithmic}
%     %\State \textbf{Global:}  $S_z$ 
%     \Function{Process}{node i, $S_z$}
%     \If{$i \in \mathcal{L}eaf(S)$}
%         \State  $\textbf{return: }$ 
%     \EndIf
%     %\For{$i \in I_{o}$}
%     \If{$i\in \mathcal{S}um(S)$}
%        %\State $S_z =S_z \cup \{j \in ch(i)\}$ \Comment{include one child of node $i$}
%         \State $S_z =S_z.add(j \in ch(i))$ \Comment{include one child of node $i$}
%         \State $\textbf{return: } \text{Process}(j, S_z)$
%     \EndIf
%     \If{$i\in \mathcal{P}rod(S)$}
%         \State $S_z =S_z \cup \{j | j \in ch(i)\}$ \Comment{include all childen of node $i$}
%         \For{$j \in ch(i)$}
%             \State $\textbf{return: } \text{Process}(j,S_z)$
%         \EndFor
%     \EndIf
%     \State $\textbf{return: } S_z$
%     \EndFunction
%     \State $S_z =  \text{Process(root,Ø)}$
%     \end{algorithmic}
% \end{algorithm}






% \section{Gaussian approximation of mixture regression}



% \subsection{Mean and variance of conditional SPN}

% The mean of the conditional is just
% \begin{align*}
%     E_{p(y|x)}[y] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y p_{z_y}(y) dy \\
%     &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} E_{\phi_l}[y]
% \end{align*}

% and the variance is found using the second moment, 
% \begin{align*}
%     E_{p(y|x)}[y^2] &= \sum_{z \in \Sigma(S)} \gamma(x) \int  y^2 p_{z_y}(y) dy \\
%     &= \sum_{z \in \Sigma(S)} \gamma(x) \prod_{l \in \mathcal{L}eaf(z_y)} (Var_{\phi_l}[y]+E_{\phi_l}[y]^2)
% \end{align*}
\newpage
\section{Mixture model training}
The following section presents the expectation-maximization algorithm, which is used to 
train the Gaussian mixture model and the SPN. 

\input{Chapters/05_b_EM.tex}