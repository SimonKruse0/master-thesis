\chapter{Mixture models}
\section{Expectation-maximization for mixture models}
Mixture models can be seen as probibalistic graphical models, <fig> there one mixture component is
picked according to the realization of a catagorical variable $\textbf{Z}$ with parameters according
the the mixture weights, i.e we can reformelate, 
\begin{align*}
    p(x) &= \sum_{k=1}^K w_k p_k(x)\\
   \iff \hspace*{1cm} p(x) &= p_z(x), \hspace*{0.5cm} z \sim Cat(w_i, \dots, w_K).
\end{align*}
In fact $p_z(x)$ is a conditial distribution, $p(x|z)$, and combined with the distribution
of $Z$ we can define the joint 
$$p(x,z):=p_z(x)p(z)$$
The the case of a statistical model, data $\mathcal{D}$ is fitted by the mixture model 
by tuning the model parameters $\theta = \{w, \text{paramers for} p_i\}$. Then the joint
distribtuion $p(\mathcal{D},z| \theta)$ is refered as the \textit{complete-data} likehood in the EM algorithm. 
$$p(\mathcal{D},z|\theta):=p(\mathcal{D}|z,\theta)p(z|\theta)$$ 
When fitting model parameters we essentially want to find the parameters, that maximize the probability of
the parameters given the data, $p(\theta|\mathcal{D})$. Assuming an
unimformative/flat prior $p(\theta)$, 
\begin{align*}
p(\theta|\mathcal{D})&= \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})}\\
\Rightarrow  \arg\max_{\theta} p(\theta|\mathcal{D}) &= \arg\max_{\theta} p(\mathcal{D}|\theta)
\end{align*}
we arrive at the maximum likelihood estimate (MLE). The task of finding the MLE is conviniently
done using EM algorithm, since we can look at the likehood as the marginalized
\textit{complete-data} likehood, 
$$p(\mathcal{D}|\theta) = \sum_z p(\mathcal{D}, z|\theta)$$

%(Note that this statement true accoring to Theorem 2.1. in \cite{gupta2011theory}). 

\begin{testexample2}[Expectation-maximization EM <based on Bishops book>]
    Expectation maximization is a convinient method for finding ML (or MAP) estimate of a 
    latent variable model. We consider a probibalistic model paramitised with $\theta$, 
    $$p(\textbf{X}, \textbf{Z}|\theta)$$ where we denote all latent variables \textbf{Z}, and
    observed variables \textbf{X}. Our goal is to find the maximum of the likehood, 
    $$p(\textbf{X}|\theta) = \int p(\textbf{X}, \textbf{Z}| \theta) \mu(d\textbf{Z})$$
    maximizating the likehood itself $p(\textbf{X}|\theta)$ is assumed dificult 
    but maximizating of the \textit{complete-data} likehood $p(\textbf{X}, \textbf{Z}|\theta)$
    is much easier. The algorithm iterates over two steps: The expecation (E) step and the maximization (M) step, 
    defined in the following way for iteration $t$, 
    
    \textbf{E-step}

    Define the functional $Q(\theta,\theta^{(t)})$, to be the expected value of the complete-data 
    log likehood (log likehood function of $\theta$), with respect to the only random quantaty $\textbf{Z}$,
    which is assumed to follow a distribtuion with the density $p(\textbf{Z}|\textbf{X}, \theta^{(t)})$,
    i.e. the conditional distribution of \textbf{Z} given \textbf{X} and the current parameter point estimate
    $\theta^{(t)}$: 
    $$Q(\theta,\theta^{(t)}) := E_{p(\textbf{Z}|\textbf{X}, \theta^{(t)})}[\log p(\textbf{X}, \textbf{Z}|\theta)]$$

    \textbf{M-step}

    After the E-step we find the point estimate $\theta^{(t+1)}$ which maximizes $Q(\cdot|\theta^{(t)})$, i.e.
    $$\theta^{(t+1)} = \arg\max_{\theta} Q(\theta|\theta^{(t)})$$

    \begin{algorithm}[H]
        \caption*{(local) maximization of $p(\mathcal{D}|\theta)$}\label{EM}
        \begin{algorithmic}
        \State \textbf{Input:} dataset $\mathcal{D}$, joint model $p(\mathcal{D}, \textbf{Z}|\theta)$
        \While{Not converged}
            \State $Q(\cdot, \theta^{(t)}) \gets E_{p(\textbf{Z}|\mathcal{D}, \theta^{(t)})}[\log p(\mathcal{D}, \textbf{Z}|\theta)]$ \Comment{E-step}
            \State $\theta^{(t+1)} \gets \arg\max_{\theta} Q(\theta|\theta^{(t)})$ \Comment{M-step}
        \EndWhile
        \State $\textbf{return: } \theta^{(end)}$
    \end{algorithmic}
    \end{algorithm}

    \textbf{Proof of correctness} 
    
    We will now give a short proof that maximizing $Q(\cdot|\theta^{(t)})$ maximizes the likelihood
    $p(\textbf{X}|\theta)$, where we assume that $\textbf{Z}$ is a random vector with a discrete
    distribution. This allow us to use Gibbs inequality: 
    $$\sum_z p_1(z) \log p_1(z) \geq \sum_z p_1(z) \log p_2(z)$$ where $p_1(\cdot)$ and $p_2(\cdot)$
    are densities belonging to two discrete distributions of $Z$, equality if $p_1(\cdot) =
    p_2(\cdot)$. From now on we will alter the subscript on the expecations, just have in mind that
    $$E_{\theta^{(t)}}[g(Z)]:=E_{p(\textbf{Z}|\textbf{X}, \theta^{(t)})}[g(Z)] = \sum_z g(z)
    p(\textbf{z}|\textbf{X}, \theta^{(t)})$$ 
    Now to the proof: From bayes rule $p(\textbf{X}|\theta) =
    \frac{p(\textbf{X}, \textbf{Z})}{p(\textbf{X})}$ we can write
    $$\log p(\textbf{X}|\theta) = \log p(\textbf{X}, \textbf{Z}) - \log p(\textbf{Z}|\textbf{X},\theta)$$
    Now, taking the expecation of the above w.r.t. $p(\textbf{Z}|\textbf{X}, \theta^{(t)})$,
    yields,
    \begin{align*}
        \log p(\textbf{X}|\theta)  &= E_{\theta^{(t)}}[\log p(\textbf{X}, \textbf{Z}|\theta)]
        -  E_{\theta^{(t)}}[\log p(\textbf{Z}|\textbf{X},\theta)]\\
        &= Q(\theta,\theta^{(t)})+ E_{\theta^{(t)}}[\log p(\textbf{Z}|\textbf{X},\theta)]
    \end{align*} 
    Since the above equation holds for any $\theta$, it also holds for $\theta^{(t)}$
    now we have, 
    $$\log p(\textbf{X}|\theta^{(t)}) = Q(\theta^{(t)},\theta^{(t)})+ E_{\theta^{(t)}}[\log
    p(\textbf{Z}|\textbf{X},\theta^{(t)})]$$ 
    Subtracting the two equations, we get,  
    $$\log p(\textbf{X}|\theta) - \log p(\textbf{X}|\theta^{(t)}) = Q(\theta,\theta^{(t)})
    -Q(\theta^{(t)},\theta^{(t)})+ E_{\theta^{(t)}}[\log p(\textbf{Z}|\textbf{X},\theta)]-
    E_{\theta^{(t)}}[\log p(\textbf{Z}|\textbf{X},\theta^{(t)})]$$ From Gibb's inequality we have
    that $E_{\theta^{(t)}}[\log p(\textbf{Z}|\textbf{X},\theta^{(t)})]\leq E_{\theta^{(t)}}[\log
    p(\textbf{Z}|\textbf{X},\theta)]$ where equality only holds for $\theta^{(t)} = \theta$, giving
    \begin{align*}
        \log p(\textbf{X}|\theta) - \log p(\textbf{X}|\theta^{(t)}) \geq 
    Q(\theta,\theta^{(t)})
    -Q(\theta^{(t)},\theta^{(t)})
    \end{align*}
    so optimizing $Q(\theta,\theta^{(t)})$ will optimize
    $\log p(\textbf{X}|\theta) $ as least as much.
\end{testexample2}


% then we can marginalise over
%     $z$ in order to recover $p(x)$, 
%     $$p(\textbf{x}|\pi) = \sum_{z=1}^Z p(z,\textbf{x}|\pi) $$
%     assuming iid data, then the complete likehood can be decompose as the product 
%     $$ p(z,\textbf{x}|\pi) = \prod_{i=1}^n p(z,\textbf{x}_i|\pi)$$
%     We wil for convinience transform the likelihood using a log transform, 
%     as it will not influence the maximum. 
%     $$\log p(\textbf{x}|\pi) = \sum_{z=1}^Z \sum_{i=1}^n \log p(z,\textbf{x}_i|\pi)$$
\newpage
<EM for SPN >

<EM for GMM >