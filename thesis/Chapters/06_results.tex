\chapter{Results}
\todo{Lav systematiske resultater. Udvælg functionsklasser at kigge på}

<reproducer resultater fra Arayns thesis>
Egne forsøg med SPN og mixture regression


\section{implementation}
The Gaussian process regression is implemented using the Scikit-learn Gaussian process
implementation with the Matern kernel with $\nu=1.5$ and the y-values are normalized in order for
the prior distribution to be reasonable (mean 0 and variance 1). The lengthscale is optimized by
maximizing the marginalized likelihood using the limited memory quasi newton solver with bounds
l-bfgs-b with 200 restarts. 

The Bayesian neural network is implemented using Numpyro - which is a python library for
probabilistic machine learning, developed by some of the people behind Pyro, but instead of using
pyTorch as backend, Numpyro uses Jax. This allows for significantly large speedup when doing MCMC,
i.e. NUTS sampling. This was however still very slow and we therefore limited the network size to a
small 3 layer with 10 nodes on each layer network. The prior distribution for weights is a standard
normal distribution, and the bias normal priors are sat 
a bit less restrictive with a standard deviation of 2 instead. 
This is reasonable since the data is always standardized. 

\subsection{standardized data}
Before the data reach any of the models, it is standardized. Which is 
a scaling and translation such that the datas empirical mean and standard deviation are 0 and 1, respectively. 
In that way is much easier to control the parameters in the models, since the data it fits and predicts
is always the same scale. The transformation is as following, first time the models sees the data, 
the empirical mean and standard deviation is recorded both for $x$ and $y$, (note $x$ can be a vector),
giving $\mu_x$,$\mu_y$, $\sigma_x$and $\sigma_y$.
next all data is transformed using the transformation, 

$$T_x(\cdot) := \frac{\cdot-\mu_x}{\sigma_x}$$

When we predict the $x$ is transformed, the model output a prediction 
and then the inverse transform is mapping the prediction from the standardised
domain to the original domain, 
 $T^{-1}_y(\hat y) := \hat y \cdot \sigma_x+\mu_x$


\section{Model compartment on test problems}
% Experiment section. For each experiment
%  1. Introduction. What is the purpose of the experiment
%  2. Data, materials, methods. Exactly how was the experiment conducted.
%  3. Results
%  4. Discussion. Conclusions.
Bayesian optimization is typically performed on black-box functions, and thereby the Bayesian
regression model is trained on data from all kinds of underlying problems. Now we want to test the
performance of different regression models on different types of 1D problems, all in the interval $x
\in [-100,100]$:

\begin{itemize}
  \item Test 1: A well bahaved sine-function,
  \item Test 2: A highly discontinous function,
  \item Test 3: A multi-modal objective function,
  \item Test 4: An anisotropic objective function.
\end{itemize}

It is common to have many parameters to tune in Bayesian optimization (in 1D a human could
potentially assist the surrogate model since the regression can be plotted and judged by eye).
Nevertheless, we will keep the domain in 1D to establish  
a more informative judgment of the model performance.
This will establish some intuition about the models and 

\begin{table}[H]
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}[t]{p{.20\textwidth} | p{.40\textwidth}  | p{.40\textwidth}}
  \textbf{Model}       & \textbf{Specification}    &   \textbf{Training} \\ \hline
  BNN& 
  \begin{tabular}[t]{l}
    3 layers of 50 tanh nodes\\ 
    $p(\textbf{w}, b) \sim \mathcal{N}(0,1)$ \\
    $p(\sigma) \sim \text{InvGa}(1000,1)$ 
  \end{tabular} &
  \begin{tabular}[t]{l}
    NUTS(500 burn-in, 500 samples) 
  \end{tabular} \\\hline

  BOHamiANN& 
  \begin{tabular}[t]{l}
    3 layers of 50 tanh nodes\\ 
    $p(\textbf{w}, b) \sim \mathcal{N}(0, \sigma_{w})$ \\
    $p(\sigma) \sim \text{LogNormal}(??)$ \\
    $p(\sigma_{w}) \sim \text{Gamma}(??)$
  \end{tabular} &
  \begin{tabular}[t]{l}
    Adaptive stochatic HMC \\
    (1000 burn-in, every 5 of 2000 samples) 
  \end{tabular} \\\hline

  GP& 
  \begin{tabular}[t]{l}
    Mantern kernel 52
  \end{tabular} &
  \begin{tabular}[t]{l}
    Maximize marginalized likelihood \\
    20 restarts of BFGS optimization
  \end{tabular} \\\hline

  GMR& 
  \begin{tabular}[t]{l}
    Training sklearn GMM, \\
    pluggin into GMR. 
  \end{tabular} &
  \begin{tabular}[t]{l}
    BO Cross-validation 30 fold, \\
    testing number of components and prior weight\\
    3 restarts. 
  \end{tabular} \\\hline


  SPN& 
  \begin{tabular}[t]{l}
    Training sklearn GMM,\\ 
    pluggin into GMR. 
  \end{tabular} &
  \begin{tabular}[t]{l}
    BO Cross-validation 30 fold,\\
     testing number of components and prior weight\\
    3 restarts. 
  \end{tabular} \\\hline


  KDE& 
  \begin{tabular}[t]{l}
    Training sklearn GMM, \\
    pluggin into GMR. 
  \end{tabular} &
  \begin{tabular}[t]{l}
    BO Cross-validation 30 fold,\\
     testing number of components and prior weight\\
    3 restarts. 
  \end{tabular} \\\hline
  \end{tabular}}
  \caption{Overview of chosen models  
          }
\end{table}
%\subsection{Model definintions}
Whereas a true Bayesian would not choose a specific type of model, but using all models
, however, this would indeed be infeasible. We need to limit the scope, 
For the GP, we do 20 restarts in the emperical maximization. And the manteen kernel prior
with $\nu = 2.5$. 
BNN, is a 50x50x50 tanh layers with standard Gaussain priors on. And a inverse gamma (1000, 1)
prior on the noise. Trained with 100 burn in and 100 samples. 
BOHAMIANN is similar but uses a different noise prior, and trains using adaptive HMC method. 
The mixture models are trained using crossvalidation scored on the mean predictive log .. posterior
(formally speaking this is not a likelihood, but we will keep it as it is almost the same)
The tuning parameters 
We do 40 iterations 
SPN is trained using EM with maximum 1000 epochs (is terminated if the progress is stalling). 
it is trained 2 times. \todo{Lav den historie}
GMR is trained using EM but this is done by the sklearn software. 

\subsection*{Test1: Well-behaved problem}
The first problem will establish some benchmark since this is a simple function to fit. Its exact definition is,
$$f(x) = x\cdot \sin(\frac{x}{50} \pi) + 150 + \frac{x}{10} + \sin(x) \hspace{0.5cm} x \in [-100,100]$$
giving a smooth wave function, with the optimium around $-80$. Note that $\sin(x)\in [-1,1]$ contribute with
some small high frequency surface waves. This should be an easy task for the discriminative models, this is indeed 
also what we see in the learning curve left of Figure \ref{Test1_reg_plot} where the beginning is dominated by them. 

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=0.5cm 0.2cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1_dim_1_0.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=0.5cm 0.2cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1_dim_1_1.pdf}
   \end{minipage}
  \caption{Mean predictive performance MAE (left) and mean predictiv probalility (right) as
  functions of number of data points. All curves reprecent the average across 10 runs for each
  model. The Gaussian process has the bedst predictive power, however it scores very low in the }
  \label{Test1_reg_plot}
\end{figure}

To get a deeper understanding of Figure \ref{Test1_reg_plot}, we plot one of the 10 runs for 23 data points, 
in Figure \ref{Test1_reg_visual_1}, and for 56 data points in Figure \ref{Test1_reg_visual_2}.


\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/1.pdf}
      \put (10,65) {BNN}
      \put (-5,40) {\small y}
  \end{overpic}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/2.pdf}
      \put (10,65) {BOHamiANN}
    \end{overpic}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/5.pdf}
      \put (10,65) {GP}
    \end{overpic}
    \end{minipage}
     
   \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/4.pdf}
      \put (10,65) {GMR}
      \put (-5,40) {\small y}
      \put (51,5) {\small x}
    \end{overpic}
    \end{minipage}
  \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/7_W.pdf}
      \put (10,65) {SPN}
     \end{overpic}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1/6.pdf}
        \put (10,65) {KDE}
      \end{overpic}
      \end{minipage}

  \caption{Plots visulising the results in figure at $n_{data} = 23$}
  \label{Test1_reg_visual_1}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/1.pdf}
      \put (10,65) {BNN}
      \put (-5,40) {\small y}
  \end{overpic}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/2.pdf}
      \put (10,65) {BOHamiANN}
    \end{overpic}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/4.pdf}
      \put (10,65) {GP}
    \end{overpic}
    \end{minipage}
     
   \begin{minipage}[b]{0.32\textwidth}
    \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/3.pdf}
      \put (10,65) {GMR}
      \put (-5,40) {\small y}
      \put (51,5) {\small x}
    \end{overpic}
    \end{minipage}
  \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/6.pdf}
      \put (10,65) {SPN}
     \end{overpic}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \begin{overpic}[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test1b/5.pdf}
        \put (10,65) {KDE}
      \end{overpic}
      \end{minipage}

  \caption{Plots visulising the results in figure at $n_{data} = 133$}
  \label{Test1_reg_visual_2}
\end{figure}


The Gaussian process is however overconfident in its prediction and assigns
almost 0 probability to some of the 10000 test points, yielding very low mean log predictive
likelihood. The Bayesian neural networks learning curve stagnates around 100 data points, however,
it provides a decent mean log predictive likelihood. BOHamiANN performs well in the beginning 
but seems not expressive enough for this task. The architecture of the neural network is besides
they use a log-normal piror on the variance, and a hyper prior on the variance parameter of the weights.
But it seems like the algorithm does not converge?. The mixture models are crossvalidated to maximize
the predive likelihood, 

\subsection*{Test2: Discontinous problem}
This is a type of problem where we would expect the GP to not perform well, 
BNN should be the better 

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2_dim_1_0.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2_dim_1_1.pdf}
   \end{minipage}
  \caption{Plots visulising the results in above figure.}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/1.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/2.pdf}
   \end{minipage}
   
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/3.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/4.pdf}
    \end{minipage}
    
    
    \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/5.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test2/6.pdf}
     \end{minipage}

  \caption{Plots visulising the results in above figure.}
\end{figure}




\subsection{Test3: multi-modal problem}
Multimodal problems. 

Here is the more intersting problems for the mixture regression methods. 


\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3_dim_1_0.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3_dim_1_1.pdf}
   \end{minipage}
  \caption{Plots visulising the results in above figure.}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/1.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/2.pdf}
   \end{minipage}
   
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/3.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/4.pdf}
    \end{minipage}
    
    
    \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/5.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test3/6.pdf}
     \end{minipage}

  \caption{Plots visulising the results in above figure.}
\end{figure}


\subsection{Test4: anisotropic problem}
Finally this problem is designed to

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[width=\textwidth]{Figures/results_regression/Test4_dim_1_0.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Figures/results_regression/Test4_dim_1_1.pdf}
   \end{minipage}
  \caption{Plots visulising the results in above figure.}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/1.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/2.pdf}
   \end{minipage}
   
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/3.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/4.pdf}
    \end{minipage}
    
    
    \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/5.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1cm 0.7cm 1.5cm 0.5cm,clip,width=\textwidth]{Figures/results_regression/Test4/6.pdf}
     \end{minipage}

  \caption{Plots visulising the results in above figure.}
\end{figure}


\section{Regression analysis}
As described in the previous sections, an essential part of Bayesian optimization and the decision
theory built around Bayesian optimization is that the regression model is correct. We will therefore
look at how good the regression models are in terms of accurate prediction and 
correct uncertainty estimation. 

\subsection{uncertainty quantification}
The probibalistic model is given as 
$$p(y|x,\mathcal{D}) = \mathcal{N}(y|\mu_{\mathcal{D}}(x), \sigma_{\mathcal{D}}^2(x))$$
where the mean, and variance functions are different for all the models. 
Given a trained model, we quantify its ability to capture uncertainty with the average
predictive likelihood/ probability of the observation, given a test input $x_i$ the
the density of predictive distribution is evaluated in the coresponding test output $y_i$,

$$\overline{p(y_i|x_i,\mathcal{D})} := \frac{1}{n}\sum_{i=1}^n p(y_i|x_i,\mathcal{D})$$
note that the above might lead to that it is more important to classify one test point
really well, so what we actually would rather test is the test point evaluated in the predictive
posterior $$p(\textbf{y}|\textbf{x}, \mathcal{D}) = \prod p(y_i|x_i, \mathcal{D})$$
where it is much more convinient to deal with it in the log space, 
 $$\log p(\textbf{y}|\textbf{x}, \mathcal{D}) = \sum \log p(y_i|x_i, \mathcal{D})$$
which makes it convinient to get the mean log prediction. 

there the bigger the mean predictive likelihood is, the better. \todo{it is not called predictive
likelihood!!}

\subsection{prediction quantification}
Here we use the mean absolute error to quantify the prediction error. The i'th test point is
$(x_i,y_i)$ and the mean absolute error is given as, 
$MAE :=\frac{1}{n}\sum_{i=1}^n |\mu_{\mathcal{D}}(x_i) - y_i| $

\subsection{regression benchmark}
We will benchmark our different regression models againt a simple emperical mean and
emperical std. normal distribution benchmark, 
$$p(y|x\mathcal{D}) = \mathcal{N}(y| \bar{\textbf{y}} , \bar{\sigma}^2 (\textbf{y}))$$

where $\bar{\textbf{y}} = \frac{1}{n}\sum_{i=1}^n \textbf{y}_i $ and 
$\bar{\sigma}^2 (\textbf{y})) = \frac{1}{n-1}\sum_{i=1}^n (\textbf{y}_i-\bar{\textbf{y}})^2 $
this is however not a good model for Bayesian optimization, as it will not provide a
new candidate point, as all points are equally good. 


\section{Mixture regression}

\begin{figure}
  \caption{Regression of a probabilistic objective function, is not a smart choice
  using anything else than a mixture regression model. Top we see the performance
  of 5 different regression models fitted to an increasing amount of training data, 
  the underlying function is tricky, as it jumps between multiple objective functions
  yielding a violation to most of the generative models, where most are capable of
  handling gaussian noise this is not gaussian noise and hence a very difficult problem. 
  This kind of objective function could definitely be relevant in certain cases.}
\end{figure}

One could define a generative story, flip a coin, then the data will
be fitted with this Gaussian.! 

\begin{figure}
  \caption{Plots visulising the results in above figure.}
\end{figure}

<data generating process>
Model do well when they are the data generating process, but if they are not, then
they do bad. If you collect all model, 

What model to chose? the standard answer of a Bayesian should be: Why choose? If there 
is uncertainty about them, then we can just average them. Pic the maximum would be frquencialistic.

Could we combine GP and SPN?

ensample of models ..
Simulation studies. 


% We evaluate the optimization methods on the BBOB-2015 noiseless
% testbed, which consists of 24 functions divided into five groups in terms of
% difficulty (see Table 3.1). These functions are a part of Comparing Continu-
% ous Optimizers framework (COCO) [77], which is a popular benchmarking
% testbed for continuous black-box optimization algorithms and allows for
% comprehensive evaluation of optimization methods over a set of functions
% with a wide range of characteristics (please refer to [57] for more details on
% the benchmark and specifications of each f

\section{Regression analysis of GP, BOHAMIANN and NumpyNN 1D}

\section{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_BOHAMIANN2.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_BOHAMIANN3.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Gaussian Process - sklearn2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Gaussian Process - sklearn3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Naive Gaussian Mixture Regression optimized2.pdf}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Naive Gaussian Mixture Regression optimized3.pdf}
    \end{minipage}
  \label{f1_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_numpyro neural network200-2002.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_numpyro neural network200-2003.pdf}
   \end{minipage}
  
  %  \begin{minipage}[b]{0.49\textwidth}
  %   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_SPN regression optimized2.pdf}
  %  \end{minipage}
  %  \hfill
  %  \begin{minipage}[b]{0.49\textwidth}
  %    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_SPN regression optimized3.pdf}
  %   \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco/f1.pdf}
     \end{minipage}
     \hfill
  \label{f1_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_BOHAMIANN2.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_BOHAMIANN3.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Gaussian Process - sklearn2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Gaussian Process - sklearn3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Naive Gaussian Mixture Regression optimized-prior 12.pdf}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Naive Gaussian Mixture Regression optimized-prior 13.pdf}
    \end{minipage}
  \label{f23_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_numpyro neural network200-2002.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_numpyro neural network200-2003.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_SPN regression optimized2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_SPN regression optimized3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco/f23.png}
     \end{minipage}
     \hfill
  \label{f1_reg_2D}
\end{figure}



\begin{figure}[h]
  \caption{2D Regression plot for a few of the problems, with the correct }
\end{figure}

\section{Mixture regression on simple functions}
Choosing a good set of design parameters is crucial, 
the manipulation... 

\begin{figure}[h]
    \caption{Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction}
\end{figure}

\begin{figure}[h]
    \caption{Regression plot for anther problem}
\end{figure}

% \begin{figure}[h]
%     \centering
%         \begin{tabular}{c|ll|l|l}
%         \multicolumn{1}{l|}{Problem 2D} & Winner (rel error)                                              & Second                                                                                  & Third                              &  \\ \cline{1-4}
%         f1                                                      & \multicolumn{1}{c|}{GP (0.2)}                                                           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Neural Network\\ (0.00001)\end{tabular}} & \multicolumn{1}{c|}{Naive (0.002)} &  \\
%         f2                                                      & \multicolumn{1}{c|}{Naive (0.002)}                                                      &                                                                                         &                                    &  \\
%         f3                                                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Neural Network\\ (0.00001)\end{tabular}} &                                                                                         &                                    &  \\
%         f4                                                      & \multicolumn{1}{l|}{}                                                                   &                                                                                         &                                    &  \\
%                                                                 & \multicolumn{1}{l|}{}                                                                   &                                                                                         &                                    & 
%         \end{tabular}
%     \caption{Table summarizing for all problems}
% \end{figure}

\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_1.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 1: "5 separable functions"}
\end{figure}

\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_2.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 2: "4 functions with low or moderate conditioning"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_3.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 3: "5 functions with high conditioning, unimodal"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_4.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 4: "5 multi-modal functions with adequate global structure"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_5.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 5: "5 multi-modal functions with weak global structure"}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f2.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f3.png}
      %\caption{Rank}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f4.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f5.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f6.png}
    \end{minipage}
    

    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f7.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f8.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f9.png}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f10.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f11.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f12.png}
    \end{minipage}
    \caption{Test functions f1 to f12}
    \label{2DBlockcyclic}
  \end{figure}
  
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f13.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f14.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f15.png}
      %\caption{Rank}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f16.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f17.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f18.png}
    \end{minipage}
    

    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f19.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f20.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f21.png}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f22.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f23.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f24.png}
    \end{minipage}
    \caption{Test functions f13-24}
    \label{f13-24}
  \end{figure}
  