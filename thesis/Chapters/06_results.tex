\chapter{Results}
\todo{Lav systematiske resultater. Udvælg functionsklasser at kigge på}

<reproducer resultater fra Arayns thesis>
Egne forsøg med SPN og mixture regression


\section{implementation}
The Gaussian process regression is implemented using the Scikit-learn Gaussian process
implementation with the Matern kernel with $\nu=1.5$ and the y-values are normalized in order for
the prior distribution to be reasonable (mean 0 and variance 1). The lengthscale is optimized by
maximizing the marginalized likelihood using the limited memory quasi newton solver with bounds
l-bfgs-b with 200 restarts. 

The Bayesian neural network is implemented using Numpyro - which is a python library for
probabilistic machine learning, developed by some of the people behind Pyro, but instead of using
pyTorch as backend, Numpyro uses Jax. This allows for significantly large speedup when doing MCMC,
i.e. NUTS sampling. This was however still very slow and we therefore limited the network size to a
small 3 layer with 10 nodes on each layer network. The prior distribution for weights is a standard
normal distribution, and the bias normal priors are sat 
a bit less restrictive with a standard deviation of 2 instead. 
This is reasonable since the data is always standardized. 

\subsection{standardized data}
Before the data reach any of the models, it is standardized. Which is 
a scaling and translation such that the datas empirical mean and standard deviation are 0 and 1, respectively. 
In that way is much easier to control the parameters in the models, since the data it fits and predicts
is always the same scale. The transformation is as following, first time the models sees the data, 
the empirical mean and standard deviation is recorded both for $x$ and $y$, (note $x$ can be a vector),
giving $\mu_x$,$\mu_y$, $\sigma_x$and $\sigma_y$.
next all data is transformed using the transformation, 

$$T_x(\cdot) := \frac{\cdot-\mu_x}{\sigma_x}$$

When we predict the $x$ is transformed, the model output a prediction 
and then the inverse transform is mapping the prediction from the standardised
domain to the original domain, 
 $T^{-1}_y(\hat y) := \hat y \cdot \sigma_x+\mu_x$


\section{Model compartment on test problems}
% Experiment section. For each experiment
%  1. Introduction. What is the purpose of the experiment
%  2. Data, materials, methods. Exactly how was the experiment conducted.
%  3. Results
%  4. Discussion. Conclusions.
As Bayesian optimization is performed on black-box functions, we want to test the surrogates on
performance on different types of problems. 

> Well-behaved
> Discontinous
> Multi-modal 
> anisotropic problem. 

It is common to have many parameters to tune in Bayesian
optimization (in 1D a human could potentially assist the surrogate model since the regression can
be plotted and judged by eye). Nevertheless, we will keep the domain in 1D to establish  
a more informative judgment of the model performance.
This will establish some intuition about the models and 

\subsection{Model definintions}
Whereas a true Bayesian would not choose a specific type of model, but using all models
however, this would indeed be infeasible. We need to limit the scope, 

For the GP, we do 20 restarts in the emperical maximization. And the manteen kernel prior
with $\nu = 2.5$. 

BNN, is a 50x50x50 tanh layers with standard Gaussain priors on. And a inverse gamma (1000, 1)
prior on the noise. Trained with 100 burn in and 100 samples. 
BOHAMIANN is similar but uses a different noise prior, and trains using adaptive HMC method. 

The mixture models are trained using crossvalidation scored on the mean predictive log .. posterior
(formally speaking this is not a likelihood, but we will keep it as it is almost the same)
The tuning parameters 
We do 40 iterations 

SPN is trained using EM with maximum 1000 epochs (is terminated if the progress is stalling). 
it is trained 2 times. \todo{Lav den historie}

GMR is trained using EM but this is done by the sklearn software. 

\subsection{Test1: Well-behaved problem}
The first problem will establish some benchmark since this is a straightforward function
to fit. This should be an easy task for the discriminative models. But the mixture models
Should also have an easy time. 

The problem is a sine wave. 

<WHat did you do!> E.g. change in basic model?

Picture + regression of all. 

We see that GP is very good here. 

\begin{figure}
  \caption{Plots visulising the results in above figure.}
\end{figure}



\subsection{Test2: Discontinous problem}
This is a type of problem where we would expect the GP to not perform well, 
BNN should be the better 


\subsection{Test3: multi-modal problem}
Multimodal problems. 

Here is the more intersting problems for the mixture regression methods. 




\subsection{Test4: anisotropic problem}
Finally this problem is designed to



\section{Regression analysis}
As described in the previous sections, an essential part of Bayesian optimization and the decision
theory built around Bayesian optimization is that the regression model is correct. We will therefore
look at how good the regression models are in terms of accurate prediction and 
correct uncertainty estimation. 

\subsection{uncertainty quantification}
The probibalistic model is given as 
$$p(y|x,\mathcal{D}) = \mathcal{N}(y|\mu_{\mathcal{D}}(x), \sigma_{\mathcal{D}}^2(x))$$
where the mean, and variance functions are different for all the models. 
Given a trained model, we quantify its ability to capture uncertainty with the average
predictive likelihood/ probability of the observation, given a test input $x_i$ the
the density of predictive distribution is evaluated in the coresponding test output $y_i$,

$$\overline{p(y_i|x_i,\mathcal{D})} := \frac{1}{n}\sum_{i=1}^n p(y_i|x_i,\mathcal{D})$$
note that the above might lead to that it is more important to classify one test point
really well, so what we actually would rather test is the test point evaluated in the predictive
posterior $$p(\textbf{y}|\textbf{x}, \mathcal{D}) = \prod p(y_i|x_i, \mathcal{D})$$
where it is much more convinient to deal with it in the log space, 
 $$\log p(\textbf{y}|\textbf{x}, \mathcal{D}) = \sum \log p(y_i|x_i, \mathcal{D})$$
which makes it convinient to get the mean log prediction. 

there the bigger the mean predictive likelihood is, the better. \todo{it is not called predictive
likelihood!!}

\subsection{prediction quantification}
Here we use the mean absolute error to quantify the prediction error. The i'th test point is
$(x_i,y_i)$ and the mean absolute error is given as, 
$MAE :=\frac{1}{n}\sum_{i=1}^n |\mu_{\mathcal{D}}(x_i) - y_i| $

\subsection{regression benchmark}
We will benchmark our different regression models againt a simple emperical mean and
emperical std. normal distribution benchmark, 
$$p(y|x\mathcal{D}) = \mathcal{N}(y| \bar{\textbf{y}} , \bar{\sigma}^2 (\textbf{y}))$$

where $\bar{\textbf{y}} = \frac{1}{n}\sum_{i=1}^n \textbf{y}_i $ and 
$\bar{\sigma}^2 (\textbf{y})) = \frac{1}{n-1}\sum_{i=1}^n (\textbf{y}_i-\bar{\textbf{y}})^2 $
this is however not a good model for Bayesian optimization, as it will not provide a
new candidate point, as all points are equally good. 


\section{Mixture regression}

\begin{figure}
  \caption{Regression of a probabilistic objective function, is not a smart choice
  using anything else than a mixture regression model. Top we see the performance
  of 5 different regression models fitted to an increasing amount of training data, 
  the underlying function is tricky, as it jumps between multiple objective functions
  yielding a violation to most of the generative models, where most are capable of
  handling gaussian noise this is not gaussian noise and hence a very difficult problem. 
  This kind of objective function could definitely be relevant in certain cases.}
\end{figure}

One could define a generative story, flip a coin, then the data will
be fitted with this Gaussian.! 

\begin{figure}
  \caption{Plots visulising the results in above figure.}
\end{figure}

<data generating process>
Model do well when they are the data generating process, but if they are not, then
they do bad. If you collect all model, 

What model to chose? the standard answer of a Bayesian should be: Why choose? If there 
is uncertainty about them, then we can just average them. Pic the maximum would be frquencialistic.

Could we combine GP and SPN?

ensample of models ..
Simulation studies. 


% We evaluate the optimization methods on the BBOB-2015 noiseless
% testbed, which consists of 24 functions divided into five groups in terms of
% difficulty (see Table 3.1). These functions are a part of Comparing Continu-
% ous Optimizers framework (COCO) [77], which is a popular benchmarking
% testbed for continuous black-box optimization algorithms and allows for
% comprehensive evaluation of optimization methods over a set of functions
% with a wide range of characteristics (please refer to [57] for more details on
% the benchmark and specifications of each f

\section{Regression analysis of GP, BOHAMIANN and NumpyNN 1D}

\section{Regression analysis of GP, BOHAMIANN and NumpyNN 2D}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_BOHAMIANN2.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_BOHAMIANN3.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Gaussian Process - sklearn2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Gaussian Process - sklearn3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Naive Gaussian Mixture Regression optimized2.pdf}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_Naive Gaussian Mixture Regression optimized3.pdf}
    \end{minipage}
  \label{f1_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_numpyro neural network200-2002.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_numpyro neural network200-2003.pdf}
   \end{minipage}
  
  %  \begin{minipage}[b]{0.49\textwidth}
  %   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_SPN regression optimized2.pdf}
  %  \end{minipage}
  %  \hfill
  %  \begin{minipage}[b]{0.49\textwidth}
  %    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f1_SPN regression optimized3.pdf}
  %   \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco/f1.pdf}
     \end{minipage}
     \hfill
  \label{f1_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_BOHAMIANN2.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_BOHAMIANN3.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Gaussian Process - sklearn2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Gaussian Process - sklearn3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Naive Gaussian Mixture Regression optimized-prior 12.pdf}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.49\textwidth}
       \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_Naive Gaussian Mixture Regression optimized-prior 13.pdf}
    \end{minipage}
  \label{f23_reg_2D}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
   \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_numpyro neural network200-2002.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_numpyro neural network200-2003.pdf}
   \end{minipage}
  
   \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_SPN regression optimized2.pdf}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\textwidth}
     \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco_reg/f23_SPN regression optimized3.pdf}
    \end{minipage}

    \begin{minipage}[b]{0.49\textwidth}
      \includegraphics[trim=1.2cm 0.7cm 2cm 1cm,clip,width=\textwidth]{Figures/coco/f23.png}
     \end{minipage}
     \hfill
  \label{f1_reg_2D}
\end{figure}



\begin{figure}[h]
  \caption{2D Regression plot for a few of the problems, with the correct }
\end{figure}

\section{Mixture regression on simple functions}
Choosing a good set of design parameters is crucial, 
the manipulation... 

\begin{figure}[h]
    \caption{Regression plot for one problem with all dims: Number of data points / dims, mean squared error, with bayesline mean prediction}
\end{figure}

\begin{figure}[h]
    \caption{Regression plot for anther problem}
\end{figure}

% \begin{figure}[h]
%     \centering
%         \begin{tabular}{c|ll|l|l}
%         \multicolumn{1}{l|}{Problem 2D} & Winner (rel error)                                              & Second                                                                                  & Third                              &  \\ \cline{1-4}
%         f1                                                      & \multicolumn{1}{c|}{GP (0.2)}                                                           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Neural Network\\ (0.00001)\end{tabular}} & \multicolumn{1}{c|}{Naive (0.002)} &  \\
%         f2                                                      & \multicolumn{1}{c|}{Naive (0.002)}                                                      &                                                                                         &                                    &  \\
%         f3                                                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Neural Network\\ (0.00001)\end{tabular}} &                                                                                         &                                    &  \\
%         f4                                                      & \multicolumn{1}{l|}{}                                                                   &                                                                                         &                                    &  \\
%                                                                 & \multicolumn{1}{l|}{}                                                                   &                                                                                         &                                    & 
%         \end{tabular}
%     \caption{Table summarizing for all problems}
% \end{figure}

\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_1.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 1: "5 separable functions"}
\end{figure}

\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_2.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 2: "4 functions with low or moderate conditioning"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_3.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 3: "5 functions with high conditioning, unimodal"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_4.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 4: "5 multi-modal functions with adequate global structure"}
\end{figure}
\begin{figure}[h]
    \centering
    \input{Tables/best_minimizer_group_5.tex}
    \caption{Distance to optima $f(x^{best})-f^*$ using Bayesian optimization with different surrogates
    and using expected improvement with a budget of $40$ samples for group 5: "5 multi-modal functions with weak global structure"}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{BayesOpt plot: Number of iterations, distance to optima using EI, with bayesline random search}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f2.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f3.png}
      %\caption{Rank}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f4.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f5.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f6.png}
    \end{minipage}
    

    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f7.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f8.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f9.png}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f10.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f11.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f12.png}
    \end{minipage}
    \caption{Test functions f1 to f12}
    \label{2DBlockcyclic}
  \end{figure}
  
  \begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f13.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
     \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f14.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f15.png}
      %\caption{Rank}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f16.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f17.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f18.png}
    \end{minipage}
    

    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f19.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f20.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f21.png}
    \end{minipage}
    
    \begin{minipage}[b]{0.32\textwidth}
        \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f22.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f23.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
      \includegraphics[trim=2.5cm 1.3cm 2.5cm 1.3cm,clip,width=\textwidth]{Figures/coco/f24.png}
    \end{minipage}
    \caption{Test functions f13-24}
    \label{f13-24}
  \end{figure}
  