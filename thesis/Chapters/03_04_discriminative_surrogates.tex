\chapter{Discriminative surrogate models}
% \todo{Lav en historie, forklar hvorfor man skal læse om det næste! Hvad er formålet}
% \todo{Hvilke egenskaber har modellerne? Giv dem et eksempel med en fordel!}

When talking about a probabilistic surrogate model we are always implicitly talking about a
discriminative model: A statistical model of the conditional distribution of the observation, $y$, 
conditional on $x$ often parameterized by parameters $\theta$, i.e. $p(y|x, \theta)$ 
which, in a Bayesian context, is utilized in the \textit{predictive posterior
distribution}:
\begin{equation}\label{predictive_posterior_dist}
    p(y|x,\mathcal{D}) = \int p(y|x, \theta)p(\theta|\mathcal{D}) d\theta, 
\end{equation}
where we take all
possible models $\theta \in \text{Dom}(\theta)$ into account weighted accordingly to how probable
the model is $p(\theta|\mathcal{D})$ (the posterior distribution). Gaussian processes and Bayesian
neural networks are both discriminative models and they both assume that the observation is noisy in the
following way, 
$$y = f_{\textbf{w}}(x) + \epsilon, \hspace*{0.5cm} \epsilon \sim \mathcal{N}(0, \sigma^2).$$ They
are, however, using two very different approaches to define a discriminative model. A BNN define
$f_{\textbf{w}}(x)$ as the the neural network output for a specific realization of the network
weights and biases $\textbf{w}$. Given a realization of $\theta = (\textbf{w}, \sigma^2)$, the BNN
likelihood is defined as
$$p_{\text{\tiny BNN}} (y|x, \theta) = \mathcal{N}(y| f_{\textbf{w}}(x),\sigma^2).$$
A GP takes a different approach and directly model the noisefree prediction $f_* := f(x)$ as a random
variable. Given a realization of $\theta = (f_*,\sigma^2)$, the likelihood for a GP is given as
\begin{equation}\label{GP_likelihood}
    p_{\text{\tiny GP}}(y|x, \theta) = \mathcal{N}(y| f_*,\sigma^2).
\end{equation}
% It has been shown that a Bayesian neural network with Gaussian prior on infinitely amount of nodes
% is equavalent to a GP \cite{??}. So the similarities ... 


In the following chapter, we will provide more details on both models, how they are defined, can be trained
and used for predictions and discuss their properties. First, we dive into GPs.

% The predictive posterior of a GP is given as
% $$ p_{gp}(y|x,\mathcal{D}) = \int \mathcal{N}(y|f_*, \sigma^2) p(f_*|\mathcal{D}) d f_*$$
% where $p(\textbf{f}_*|\mathcal{D})$ defines a posterior of the
% predicition $f(x)$. We will now dive into Gaussian processes.



% Bayesian neural networks is
% defined as a typical probabilistic regression model with some additive gaussian noise
% $\mathcal{N}(0,\sigma^2)$,  
% \begin{equation}\label{typicalregmodel}
%     p(y|x, \theta) = \mathcal{N}(y| f_{\textbf{w}}(x),\sigma^2)
% \end{equation}
% where $\theta = (\textbf{w}, \sigma^2)$ and $f_{\textbf{w}}(x)$ is the neural network output for a
% specific realization of $\textbf{w}$. The formulation \eqref{typicalregmodel} is typical in the
% sense that $f_{\textbf{w}}$ could describe a linear model, $f_{\textbf{w}}(x) = \textbf{w}^Tx$ or a
% polynomial $f(x) = \sum_i^n \mathbf{w}_i\cdot x_i^2$ etc. 
% %The performance of the regression model is varies depending
% %on how expressive and prone to overfitting the model is. 
% Gaussian processes take a
% different approach and directly model the noisefree prediction $f_* := f(x)$ as a random
% variable. The likelihood for a GP is given as
% $$ p(y|x, \theta) = \mathcal{N}(y| f_*,\sigma^2)$$ where $\theta = f_*$. Note that $x$ is always
% given in all probabilistic quires, but for clearification we could add the subscript $\theta_x$, since $f_*$ is always depended
% on $x$. 




% The predictive posterior for Bayesian neural networks is given as:
% $$ p_{bnn}(y|x,\mathcal{D}) = \int \mathcal{N}(y|f_{\textbf{w}}(x),
% \sigma^2)p(\textbf{w},\sigma^2|\mathcal{D}) d \textbf{w} d\sigma^2$$ where 
% $p(\textbf{w},\sigma|\mathcal{D})$ is the posterior distribution.


% is infered using the
% following posterior on the unknown vector $\textbf{f} := [f(x_1),\dots,f(x_n)]$, where the
% distribution of each element is determined by the similarity between its $x$ and the other elements,  
% $$p(\textbf{f}_*|\mathcal{D}) = \int p(\textbf{f}_*|\textbf{f})p(\textbf{f}|\mathcal{D}) d \textbf{f}$$


% In Bayesian neural networks, we treat the model parameters as random quantities, and assign them a
% distribution before observing any data, this is the prior distribution. For the model neural network
% parameters, $w$, we typically assign a standard normal distribution and the observation variance
% parameter $\sigma$ is often assigned a lognormal or half-Cauchy, with support on the positive real
% domain, since a variance parameter can only be non-negative. We write the priors of the BNN model as
% \begin{align*}
%     p(w) &= \mathcal{N}(w;\textbf{0},I)\\
%     p(\sigma) &= \log\mathcal{N}(\sigma;\dots)
% \end{align*}

% Next, we look at the observation model of a BNN, this is essentially the same as described in
% section \ref{ObsModel}. We use the neural network output $f_w(x)$ to predict the mean value of the
% objective function and add some Gaussian noise $\sigma$. In order to simplify notation we collect
% all BNN parameters i.e. $\theta = (w,\sigma)$. And $\theta$ is given en Bayesian treatmeant.

% In Gaussian process regression we step up an abstraction level
% from modeling the objective function, to model the objective function "output" itself. 
% This is done by treating the objective function, $f$, as a random quantity, inducing
% a prior over it $p(f(\cdot))$ and a observation model given as, 
% $$p(y|x_i,\textbf{f}_i) = \mathcal{N}(y;\textbf{f}_i,\sigma)$$
% Where we define $\textbf{f}_i := f(x_i)$ to be the evaulation of the objective function 
% at point $x$.

% Now, given data, $\mathcal{D} =\{(x_i,y_i)\}_{i=1}^n$ we can find the posterior of
% the unknown quantaties $\textbf{f} = (f(x_1), \dots,f(x_n))$ i.e. the objective function
% value at the $n$ locations giving, 
% $$p(\textbf{f},\sigma|\mathcal{D}) = \frac{p(y_1,\dots,y_n|x_1,\dots,x_n,\textbf{f},\sigma)
% p(\textbf{f},\sigma|x)}{c} = \frac{p(\textbf{f},\sigma|x)\prod_{i=1}^n p(y_i|x_i,\textbf{f},\sigma)
% }{c} $$


\input{Chapters/03_gaussian_process_regression.tex}
\newpage
\input{Chapters/04_bayesian_nn_regression.tex}
\section{Summary}
This chapter introduced the discriminative models, which will be tested as surrogate models in a
Bayesian optimization setting. First, we introduced the difference between a Gaussian process and
the more common way of defining a Bayesian regression model, i.e. a deterministic regression model
which is made Bayesian by assigning its weights and regression output a joint uncertainty. We define
the Bayesian neural network in this common way. Next, we introduced the Gaussian processes and found
they, in practice, are multivariate Gaussian distributions, yielding nice exact inference. We
shortly discussed the hyperparameter tuning of GPs using empirical Bayes, and that we need to take
care of local optima. Finally, we introduced Bayesian Neural networks, which are trained with MCMC
sampling on joint probability factorized to the prior distribution and a likelihood function. 
\todo{a bit more?}



