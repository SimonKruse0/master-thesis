\chapter{Discussion}
%test results
After the result have been conducted it is evident that the Gaussian process is a preferred model
across the tested problems. however, we did find problems, where the GP was not the best. 
\begin{itemize}
    \item Test2
    \item Test3
\end{itemize}



%test results

% discussion of Test3 
Test3: One could define a generative story, flip a coin, then the data will
be fitted with this Gaussian.! However, if the generative story is not known..

%   \caption{Regression of a probabilistic objective function, is not a smart choice
%   using anything else than a mixture regression model. Top we see the performance
%   of 5 different regression models fitted to an increasing amount of training data, 
%   the underlying function is tricky, as it jumps between multiple objective functions
%   yielding a violation to most of the generative models, where most are capable of
%   handling gaussian noise this is not gaussian noise and hence a very difficult problem. 
%   This kind of objective function could definitely be relevant in certain cases.}

<data generating process>
Model do well when they are the data generating process, but if they are not, then
they do bad. If you collect all model, 
What model to chose? the standard answer of a Bayesian should be: Why choose? If there 
is uncertainty about them, then we can just average them. Pic the maximum would be frquencialistic.
Could we combine GP and SPN?
ensample of models ..
Simulation studies. 


The Bayesian neural network training is difficult to know when it has converged, in general the more samples the better, 
and the more weights the more complicated model there for more samples. We chose 500 warm-up and 500 samples, using 4
chains, since that we have 4 cpus. Also to make sure the fitting we sat a very informative prior on a very small observation
variance. 

BOHamiANN is should have been more in focus. More samples? Or Better parameters. 

Mixture regression for small amount of data does not seem smart. GMR is trained using MLE, has the most discriminative power,
however, is also failing a lot. Essentially it is just a bit smarter version of random search, which indeed migth be
better than an overconfident GP in certain problems. This is what we saw in ... Even for the MAP model SPN, the prior
matters a lot. 

Compartment between resluts should have used the mode instead for the mean. 

emperical expecpected improvement should be used..! Where the mode is not equal the mean and where we
us the predictive distribution. 

Even though SPN is an interesting model, its relevance is only for large dimensions, where the number for data points
or in certain cases where we want a more constraint model, i.e. that y can only come from 2 distributions thorughout the
data. 

The reason why SPN is working well sometimes is more due to it fact that it has a prior, 
and not the fact that it is -- Also it is hard to know exact when it has converged?!? Nah. 
Maybe 5 restarts are too little. Bayesian nice everything please. 


discussion: Using a generative model as a surrogates model is a novel idea - maybe it is not a good
idea.. Discretized regression

Maybe manipulation of variance could be done, or same mean prediction?!? Or 
mean prediction using the means of the close by x-values. 

The terminology deep is not really relevant for the SPN as they are constrained by the number of dimensions..!

crossvalidation for selecting the hyper paramters is not really smart for only few data. It would make more sence 
to just place a hyperprior on the priors instead!. 

\section{Mixture regression}
The prior variance! Meant a lot!



\section{Bayesopt procedure}
5 initial samples is maybe ok? Should only have done 2? 
And is 30 samples really scientific? 



\chapter{Conclusion and further work}
This thesis has motivated the relevance for Bayesian optimization, developed different alternative
surrogate models, i.e. a Bayesian NN and mixture regression models and tested those on some 
test functions. Among the 4 selected BBOB problems in higher dimensions it was not possible to show that
GP was not the prefered model. However, for the 4 small 1D test problems, we found problems, where the GP. 
is not preferable. To the story is that the developed generative surrogates all have been approximated by
a Gaussian. 

The SPN was understood in the context of regression, which to our knowledge is a novel approach. However, 
when diving into the definition of the SPN it became clear that it was nothing but a discretized regression model. 

The kernel density estimation is an uncorrelated GMM with a component on each datapoint, this made
it robust, but also very naive, without the prior this model would provide a local mean prediction
with mean equal the nearest data point and variance is selected. <Lav om>

The Gaussian mixture regression was included to make the more intelligent mixture components, which
would allow for better predictive power, however, without much data the model collapsed into very
spiky distributions, and the prior was often sat high to help out the uncertainty quantification. 

The BOHamiANN did a surprisingly good job in the Bayesian optimization (for the larger high-dim problems),
even though it had a 

We showed that there is a strong connection between being a good regression model and a good Bayesian optimization
algorithm. Unfortunately, even though the mixture regression models showed good uncertainty quantification,
they were approximated by gaussians to perform the fast closed form Expected improvement, therefore the 
connection is lost. 

Testing using empirical expected improvements. 

Maybe manipulation of variance could be done, or same mean prediction?!? Or 
mean prediction using the means of the close by x-values. 

\section{further work}
Many problems have dimensions of no relevance. Here This might be relevant to investigate

More dimensions of prior weighting?

<Using mixtures of more complicated components, i.e. hundebens distribution>
 
<Parrallel Bayesian OPtimization>

% In this paper, we have only considered function approximation problems. Problems
% requiring classication could be handled analogously with the appropriate models. For
% learning classication with a mixture model, one would select examples so as to maximize
% discriminability between Gaussians; for locally weighted regression, one would use a logistic
% regression instead of the linear one considered here (Weisberg, 1985).