\chapter{Discussion}
In the following, we will discuss the results has been conducted in the previous chapter and the
surrogate models in general. 

After the results have been conducted, it is evident that the Gaussian process is a preferred model
across the tested problems. We did, however, find problems, where the GP was not the best, those are
Test2 and Test3. Therefore, we indicate that the hypothesis is right. There might be more, and in
fact, we only looked at 4 self-defined 1D problems and 4 BBOB problems tested. An obvious next step
would be to test the surrogate models on all 24 BBOB problems (we test on 4) and for all dimensions
(2,3,5,10,20,40). But first of all, we need to discuss the validation of the BO test. A better test
would be to,
\begin{itemize}[noitemsep]
    \item Conduct a larger number of BO iterations (we use 30)
    \item Use more restarts (we use 20)
\end{itemize}
Especially more restarts is important, in Figure \ref{...} we have plotted the BO optimization
paths, which reveals a lot of variation depending on the restarts, it indicates that the BO result
might be noisy. .. The PhD thesis \cite{PhDthesis} uses only 15 restarts, however, here the tests are
conducted on 100 iterations and with only 2 initial points. We select to use 5 random samples .. no
argument. 

We established a connection between the performance of Bayesian optimization and the surrogate
model's performance as a regression model. Note that for very small $\sigma^2$ which was the case
for the Gaussian process (lower bounded with a observation variance of $10^{-3}$) the expected improvement
is $$EI(x) = (y_{\min}-\mu_x)\Phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right)+ \sigma_x
\phi\left(\frac{y_{\min}-\mu_x}{\sigma_x}\right) \approx y_{\min}-\mu_x,$$ since the fraction $\frac{y_{\min}-\mu_x}{\sigma_x}$
goes to infinity, the standard Gaussian pdf and cdf become 0 and 1, respectively. The connection 
between the mixture regression performance and as Bayesian optimization should not be as clear. Indeed 
Test3 shows an example where the mixtures ... this is due to the Gaussian approximation. Furthermore,
we chose the easily available mean value to do predictive power of the regression models. This is a shame, 
as the model's predictive distribution is richer than a simple mean and variance. This 
would, however, involve tests using the approximate expected improvement. The predictive distribution
of the Bayesian neural networks is also approximated with a Gaussian. This is however a bit more 
reasonable as the Bayesian neural networks in the limit are equal to a Gaussian process. Futhermore,
experiments using the approximated expected improvement with the Bayesian Neural networks were 
conducted in \cite{PhDthesis}, with no promising results. 

%Mixture vs generative models
In this thesis, we tried using mixture regression models as surrogate models. Especially, when breaking the 
GP and BNN's assumptions of a Gaussian observation noise they proved effective. This class of problems
might occur in real-life simulations where the objective function can have multiple values, which are not
just nearby values coursed by measurement noise. If one knew the "generative story" of the
objective function values (e.g. that Test3 jumps between 2 functions), it might be possible to think about combining more Gaussain proceses in a probalistic
graphical model, i.e. flipping a coin on what GP to use. But in the case where this generative story was not given 
mixture regression might indeed be a smart choice. 

Discriminative models are built with this one purpose of regression, whereas generative models
conduct different kinds of probabilistic queries, which are useful. Discriminative models are good
for finding the paterns in data. 

Variance of the prior was sat equal 10. I.e. when doing cross validation it was more keen on actually fitting a good model. 
crossvalidation on low amount of data is... Not a general smart thing? We always learn that we should use Bayesian methods 
on small amounts of data. 


\section{Models}
BOHamiANN is in general performing bad.. But got a comback for higher dims..?!.. It is designed to
have a fast inference, while second to have on pair performance with the GP but with only linearly
scaling inference.

Sum product networks are the motivation why we call this deep.. However, the seem to put unnecessary constraints on 
the model. 

GMR was overconfident.. Should have been Bayesian.

GPs we never discussed the kernel. But the Matern kernel is chosen as the default in ... BayesOpt book. 

%\section{Connection between a good regression model and surrogate model in BO}
Looking a regression plots and there is a clear connection between being a good regression
model and surrogate model.! However, we actually ran BO till more than 35 data points, i.e. 
all the figures shown above is from 36 points. 

The Bayesian neural network training is difficult to know when it has converged, in general the more samples the better, 
and the more weights the more complicated model there for more samples. We chose 500 warm-up and 500 samples, using 4
chains, since that we have 4 cpus. Also to make sure the fitting we sat a very informative prior on a very small observation
variance. 

BOHamiANN is should have been more in focus. More samples? Or Better parameters. 

Mixture regression for small amount of data does not seem smart. GMR is trained using MLE, has the most discriminative power,
however, is also failing a lot. Essentially it is just a bit smarter version of random search, which indeed migth be
better than an overconfident GP in certain problems. This is what we saw in ... Even for the MAP model SPN, the prior
matters a lot. 

Compartment between resluts should have used the mode instead for the mean. 

emperical expecpected improvement should be used..! Where the mode is not equal the mean and where we
us the predictive distribution. 

Even though SPN is an interesting model, its relevance is only for large dimensions, where the number for data points
or in certain cases where we want a more constraint model, i.e. that y can only come from 2 distributions thorughout the
data. The terminology deep is not really relevant for the SPN as they are constrained by the number of dimensions..!

crossvalidation for selecting the hyper paramters is not really smart for only few data. It would make more sence 
to just place a hyperprior on the priors instead!. 


\chapter{Conclusion and further work}
The following chapter concludes the thesis and my work over the last five months. 

First of the all, the hypothesis conducted in the beginning was, 
\begin{itemize}
    \item Mixture regression models and BNNs can be effective surrogate models
    performing better than GPs in some complex BO problems. 
\end{itemize}
In the results we did find two problems, where the GP is not a preferred model. This confirms that
we successfully implemented both BNNs and Mixture regression models in the Bayesian optimization setting.  
However, the tests were not exhaustive enough to conclude whether or not the new methods, in general, 
are a prefered model in these cases. 

This thesis has motivated the relevance for Bayesian optimization, developed different alternative
surrogate models, i.e. a Bayesian NN and mixture regression models and tested those on some 
test functions. Among the 4 selected BBOB problems in higher dimensions, it was not possible to show that
GP was not the preferred model. However, for the 4 small 1D test problems, we found problems, where the GP. 
is not preferable. To the story is that the developed generative surrogates all have been approximated by
a Gaussian. 

The SPN was understood in the context of regression, which to our knowledge is a novel approach. However, 
when diving into the definition of the SPN it became clear that it was nothing but a discretized regression model. 

The kernel density estimation is an uncorrelated GMM with a component on each datapoint, this made
it robust, but also very naive, without the prior this model would provide a local mean prediction
with mean equal the nearest data point and variance is selected.

The Gaussian mixture regression was included to make the more intelligent mixture components, which
would allow for better predictive power, however, without much data the model collapsed into very
spiky distributions, and the prior was often sat high to help out the uncertainty quantification. 

The BOHamiANN did a surprisingly good job in the Bayesian optimization (for the larger high-dim problems),
even though it had a 

We showed that there is a strong connection between being a good regression model and a good Bayesian optimization
algorithm. Unfortunately, even though the mixture regression models showed good uncertainty quantification,
they were approximated by gaussians to perform the fast closed form Expected improvement, therefore the 
connection is lost. 

\section{further work}
As mentioned, the mixture models were forcefully approximated by a Gaussian in the BO setting, but also when testing preditive
power with mean instead of mode. Therefore, an obvious next step in further work is to test the mixture models using approximate
expected improvement, which uses samples from their true predictive distributions. 

The investigation of this thesis was conducted to understand what model to choose. However, the standard answer
of a Bayesian should be: "Why choose?" If there is uncertainty about them, then we can just average
them. Picing the maximum would be frquencialistic. Could we combine GP and SPN? Ensemble of models.

Avoiding cross-validation and instead include the hyperpriors and do it the Bayesian way. 

Many problems have dimensions of no relevance. Here This might be relevant to investigate

More dimensions of prior weighting?

<Using mixtures of more complicated components, i.e. hundebens distribution>
 
<Parrallel Bayesian OPtimization>

% In this paper, we have only considered function approximation problems. Problems
% requiring classication could be handled analogously with the appropriate models. For
% learning classication with a mixture model, one would select examples so as to maximize
% discriminability between Gaussians; for locally weighted regression, one would use a logistic
% regression instead of the linear one considered here (Weisberg, 1985).